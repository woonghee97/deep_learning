{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woonghee97/deep_learning/blob/DeepLearning/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OA0f9oxYGYb"
      },
      "source": [
        "# HW#6 Regularization\n",
        "\n",
        "안녕하세요, 광운대학교 로봇학부의 오정현 교수입니다. 본 자료는 딥러닝 실습 수업을 위해 제작된 것입니다.\n",
        "\n",
        "파이썬 문법\n",
        "- 점프투파이썬(https://wikidocs.net/book/1) 참고\n",
        "\n",
        "이번 과제는 딥러닝의 일반화 성능을 높이기 위한 Regularization을 해보는 것입니다.이미지 분류에 여러 가지 Regularization 기법을 적용해 보도록 하겠습니다. 대표적인 Regularization 기법으로 Dropout, Data augmentation, Batch Normalization 등이 있습니다.\n",
        "\n",
        "이번 과제는 (https://www.tensorflow.org/tutorials/keras/classification?hl=ko)를 참고하면 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9KBzwzd7Bub"
      },
      "source": [
        "#1. Data Generation\n",
        "Data는 mnist dataset을 이용하도록 하겠습니다. mnist dataset은 원래 60000개의 training set이 주어져 있지만 overfitting을 유도하기 위하여 1000개의 data만 이용하려고 합니다. 1000개의 data로 이루어진 x_train과 y_train을 만들어 보세요. 그리고 2000개로 이루어진 large_x_train, large_y_train을 만들어보세요. 그리고 training data의 다른 범위에서 200개로 이루어진 x_validation과 y_validation도 만들어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3K3NqD_wTyh",
        "outputId": "08e462a6-2024-454f-fd92-706de9cc231d"
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot\n",
        "from keras.utils import np_utils\n",
        "\n",
        "batch_size = 28\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, Y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "### START CODE HERE ###\n",
        "x_validation = X_train[3000:3200]\n",
        "y_validation = Y_train[3000:3200]\n",
        "large_x_train = X_train[0:2000]\n",
        "large_y_train = Y_train[0:2000]\n",
        "x_train = X_train[2000:3000]\n",
        "y_train = Y_train[2000:3000]\n",
        "### END CODE HERE ###\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "large_x_train = large_x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "large_x_train /= 255\n",
        "x_validation /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "large_y_train = keras.utils.np_utils.to_categorical(large_y_train, num_classes)\n",
        "y_validation = keras.utils.np_utils.to_categorical(y_validation, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "assert large_x_train.shape[0]==2000\n",
        "assert large_y_train.shape[0]==2000\n",
        "assert x_train.shape[0]==1000\n",
        "assert y_train.shape[0]==1000\n",
        "assert x_validation.shape[0]==200\n",
        "assert y_validation.shape[0]==200\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"large_x_train shape:\", large_x_train.shape)\n",
        "print(\"large_y_train shape:\", large_y_train.shape)\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x_validation shape: (200, 28, 28)\n",
            "y_validation shape: (200,)\n",
            "x_train shape: (1000, 28, 28, 1)\n",
            "y_train shape: (1000, 10)\n",
            "large_x_train shape: (2000, 28, 28, 1)\n",
            "large_y_train shape: (2000, 10)\n",
            "x_validation shape: (200, 28, 28, 1)\n",
            "y_validation shape: (200, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlNTST2K_IRE"
      },
      "source": [
        "#2. 모델 생성\n",
        "복습 차원에서 MLP 분류모델을 만들어 보도록 하겠습니다. 모델의 마지막 레이어에는 활성화 함수로 10개의 출력을 가지는 softmax를 달겠습니다. 이를 통해서 모델은 이미지안의 숫자가 0부터 9까지의 숫자중에 어디에 가까운지를 확률적으로 나타냅니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNI8KWo8PyC"
      },
      "source": [
        "다음과 같은 MLP 모델을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560L-utPx8z1",
        "outputId": "24c13e0d-e559-4a88-ec0b-524703ce2662"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "### START CODE HERE ###\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_14 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhIfmZeBZm8"
      },
      "source": [
        "#3. Learning MLP\n",
        "기본 MLP 분류모델을 학습해 보겠습니다. Overfitting은 Training data에 맞추어 과도하게 학습이 이루어져 Test data에서 높은 성능이 나지 않는 현상, 즉 Generalization 성능이 높지 않게 나타나는 현상을 의미합니다. 따라서 Overfitting이 발생하면 Training accuracy는 높지만 Test accuracy는 높지 않게 나타납니다. \n",
        "\n",
        "현재 모델이 overfitting이 발생하는지 체크해 보세요. 일부러 overfitting이 발생하도록 유도하였기 때문에 overfitting 현상이 나타나야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1VoI2bCyBy-",
        "outputId": "e3b67203-3a1f-46a2-e905-58239ab1f839"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "weights = model.get_weights()\n",
        "\n",
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 2s 40ms/step - loss: 2.3980 - accuracy: 0.0700 - val_loss: 2.3267 - val_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.3045 - accuracy: 0.1020 - val_loss: 2.2566 - val_accuracy: 0.1450\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.2180 - accuracy: 0.1590 - val_loss: 2.1913 - val_accuracy: 0.2100\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1368 - accuracy: 0.2330 - val_loss: 2.1288 - val_accuracy: 0.2400\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0602 - accuracy: 0.3240 - val_loss: 2.0695 - val_accuracy: 0.3000\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.9877 - accuracy: 0.4080 - val_loss: 2.0125 - val_accuracy: 0.3650\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.9183 - accuracy: 0.4620 - val_loss: 1.9579 - val_accuracy: 0.4300\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.8526 - accuracy: 0.5260 - val_loss: 1.9049 - val_accuracy: 0.4600\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.7896 - accuracy: 0.5710 - val_loss: 1.8545 - val_accuracy: 0.5100\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.7293 - accuracy: 0.6080 - val_loss: 1.8059 - val_accuracy: 0.5650\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6720 - accuracy: 0.6390 - val_loss: 1.7590 - val_accuracy: 0.5950\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6171 - accuracy: 0.6640 - val_loss: 1.7138 - val_accuracy: 0.6100\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5650 - accuracy: 0.6890 - val_loss: 1.6706 - val_accuracy: 0.6600\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.5153 - accuracy: 0.7050 - val_loss: 1.6294 - val_accuracy: 0.6750\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.4679 - accuracy: 0.7190 - val_loss: 1.5898 - val_accuracy: 0.6850\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4227 - accuracy: 0.7290 - val_loss: 1.5513 - val_accuracy: 0.7100\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3800 - accuracy: 0.7410 - val_loss: 1.5150 - val_accuracy: 0.7100\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3387 - accuracy: 0.7550 - val_loss: 1.4798 - val_accuracy: 0.7150\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2998 - accuracy: 0.7560 - val_loss: 1.4455 - val_accuracy: 0.7300\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2626 - accuracy: 0.7670 - val_loss: 1.4132 - val_accuracy: 0.7300\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2270 - accuracy: 0.7770 - val_loss: 1.3823 - val_accuracy: 0.7400\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.1932 - accuracy: 0.7810 - val_loss: 1.3530 - val_accuracy: 0.7450\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.1612 - accuracy: 0.7850 - val_loss: 1.3245 - val_accuracy: 0.7500\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.1306 - accuracy: 0.7870 - val_loss: 1.2973 - val_accuracy: 0.7600\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.1015 - accuracy: 0.7930 - val_loss: 1.2710 - val_accuracy: 0.7700\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0736 - accuracy: 0.8000 - val_loss: 1.2456 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0470 - accuracy: 0.8010 - val_loss: 1.2214 - val_accuracy: 0.7800\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.0216 - accuracy: 0.8030 - val_loss: 1.1977 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.9973 - accuracy: 0.8140 - val_loss: 1.1757 - val_accuracy: 0.7850\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.9741 - accuracy: 0.8210 - val_loss: 1.1547 - val_accuracy: 0.7850\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.9519 - accuracy: 0.8270 - val_loss: 1.1340 - val_accuracy: 0.7850\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.9307 - accuracy: 0.8290 - val_loss: 1.1141 - val_accuracy: 0.7900\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.9105 - accuracy: 0.8330 - val_loss: 1.0951 - val_accuracy: 0.7900\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8910 - accuracy: 0.8350 - val_loss: 1.0765 - val_accuracy: 0.7900\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8724 - accuracy: 0.8370 - val_loss: 1.0589 - val_accuracy: 0.7950\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8546 - accuracy: 0.8420 - val_loss: 1.0419 - val_accuracy: 0.8000\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8375 - accuracy: 0.8420 - val_loss: 1.0256 - val_accuracy: 0.8000\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8211 - accuracy: 0.8480 - val_loss: 1.0101 - val_accuracy: 0.8000\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8056 - accuracy: 0.8480 - val_loss: 0.9949 - val_accuracy: 0.8050\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7906 - accuracy: 0.8530 - val_loss: 0.9803 - val_accuracy: 0.8050\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7763 - accuracy: 0.8540 - val_loss: 0.9662 - val_accuracy: 0.8100\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.7625 - accuracy: 0.8580 - val_loss: 0.9529 - val_accuracy: 0.8100\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7492 - accuracy: 0.8590 - val_loss: 0.9402 - val_accuracy: 0.8100\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7365 - accuracy: 0.8610 - val_loss: 0.9277 - val_accuracy: 0.8100\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 2s 46ms/step - loss: 0.7244 - accuracy: 0.8620 - val_loss: 0.9157 - val_accuracy: 0.8100\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7127 - accuracy: 0.8640 - val_loss: 0.9041 - val_accuracy: 0.8100\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.7014 - accuracy: 0.8650 - val_loss: 0.8928 - val_accuracy: 0.8150\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6906 - accuracy: 0.8670 - val_loss: 0.8822 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6801 - accuracy: 0.8700 - val_loss: 0.8720 - val_accuracy: 0.8250\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6699 - accuracy: 0.8720 - val_loss: 0.8620 - val_accuracy: 0.8300\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6602 - accuracy: 0.8730 - val_loss: 0.8519 - val_accuracy: 0.8250\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6507 - accuracy: 0.8750 - val_loss: 0.8427 - val_accuracy: 0.8250\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6416 - accuracy: 0.8760 - val_loss: 0.8336 - val_accuracy: 0.8250\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6327 - accuracy: 0.8770 - val_loss: 0.8246 - val_accuracy: 0.8250\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6241 - accuracy: 0.8800 - val_loss: 0.8161 - val_accuracy: 0.8300\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6158 - accuracy: 0.8810 - val_loss: 0.8078 - val_accuracy: 0.8300\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.6079 - accuracy: 0.8820 - val_loss: 0.7996 - val_accuracy: 0.8300\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6001 - accuracy: 0.8850 - val_loss: 0.7918 - val_accuracy: 0.8300\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5926 - accuracy: 0.8860 - val_loss: 0.7844 - val_accuracy: 0.8300\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5853 - accuracy: 0.8890 - val_loss: 0.7773 - val_accuracy: 0.8300\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5782 - accuracy: 0.8900 - val_loss: 0.7698 - val_accuracy: 0.8300\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5714 - accuracy: 0.8890 - val_loss: 0.7629 - val_accuracy: 0.8400\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5647 - accuracy: 0.8900 - val_loss: 0.7561 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5583 - accuracy: 0.8900 - val_loss: 0.7497 - val_accuracy: 0.8400\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5520 - accuracy: 0.8900 - val_loss: 0.7435 - val_accuracy: 0.8400\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5460 - accuracy: 0.8910 - val_loss: 0.7372 - val_accuracy: 0.8450\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5399 - accuracy: 0.8920 - val_loss: 0.7316 - val_accuracy: 0.8450\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5343 - accuracy: 0.8930 - val_loss: 0.7260 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5286 - accuracy: 0.8920 - val_loss: 0.7203 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.5231 - accuracy: 0.8920 - val_loss: 0.7149 - val_accuracy: 0.8500\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5177 - accuracy: 0.8930 - val_loss: 0.7095 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5126 - accuracy: 0.8940 - val_loss: 0.7039 - val_accuracy: 0.8500\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5075 - accuracy: 0.8950 - val_loss: 0.6990 - val_accuracy: 0.8500\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.5025 - accuracy: 0.8980 - val_loss: 0.6940 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4977 - accuracy: 0.8970 - val_loss: 0.6893 - val_accuracy: 0.8500\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4930 - accuracy: 0.8990 - val_loss: 0.6845 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4884 - accuracy: 0.8980 - val_loss: 0.6798 - val_accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4839 - accuracy: 0.8990 - val_loss: 0.6754 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4796 - accuracy: 0.9000 - val_loss: 0.6710 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4752 - accuracy: 0.9000 - val_loss: 0.6665 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4710 - accuracy: 0.9000 - val_loss: 0.6622 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4669 - accuracy: 0.9010 - val_loss: 0.6580 - val_accuracy: 0.8550\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4629 - accuracy: 0.9020 - val_loss: 0.6539 - val_accuracy: 0.8550\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4590 - accuracy: 0.9020 - val_loss: 0.6499 - val_accuracy: 0.8550\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4552 - accuracy: 0.9020 - val_loss: 0.6462 - val_accuracy: 0.8600\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4513 - accuracy: 0.9020 - val_loss: 0.6427 - val_accuracy: 0.8600\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4477 - accuracy: 0.9020 - val_loss: 0.6388 - val_accuracy: 0.8600\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4441 - accuracy: 0.9020 - val_loss: 0.6349 - val_accuracy: 0.8600\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4405 - accuracy: 0.9020 - val_loss: 0.6315 - val_accuracy: 0.8600\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4370 - accuracy: 0.9020 - val_loss: 0.6278 - val_accuracy: 0.8600\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4336 - accuracy: 0.9040 - val_loss: 0.6242 - val_accuracy: 0.8600\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4303 - accuracy: 0.9030 - val_loss: 0.6211 - val_accuracy: 0.8600\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4270 - accuracy: 0.9040 - val_loss: 0.6173 - val_accuracy: 0.8600\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4237 - accuracy: 0.9050 - val_loss: 0.6143 - val_accuracy: 0.8600\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4206 - accuracy: 0.9050 - val_loss: 0.6111 - val_accuracy: 0.8600\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4175 - accuracy: 0.9050 - val_loss: 0.6080 - val_accuracy: 0.8600\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4145 - accuracy: 0.9050 - val_loss: 0.6052 - val_accuracy: 0.8600\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.4115 - accuracy: 0.9060 - val_loss: 0.6023 - val_accuracy: 0.8600\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4085 - accuracy: 0.9060 - val_loss: 0.5993 - val_accuracy: 0.8600\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4056 - accuracy: 0.9050 - val_loss: 0.5967 - val_accuracy: 0.8600\n",
            "Test loss: 0.5708253383636475\n",
            "Test accuracy: 0.8521000146865845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jebNKzFBJGE"
      },
      "source": [
        "#4. Regularization\n",
        "Overfitting이 발생한 모델에 다양한 Regularization 기법을 이용해 보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxZKsjCFvhT"
      },
      "source": [
        "4.1 Large Dataset\n",
        "\n",
        "Training data가 충분하다면 overfitting 현상이 발생할 가능성이 줄어듭니다. 기본 MLP 분류 모델에서 large_x_train과 large_y_train을 이용하면 성능이 올라갈 것입니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5b1fp96FuEy",
        "outputId": "85e7fcd8-50da-4dfd-9020-110bbd1e7089"
      },
      "source": [
        "#initialize weights\n",
        "model.set_weights(weights)\n",
        "\n",
        "large_model_history=model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Largemodel Test loss:', score[0])\n",
        "print('Largemodel Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.2830 - accuracy: 0.1390 - val_loss: 2.2006 - val_accuracy: 0.1950\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.0781 - accuracy: 0.3110 - val_loss: 2.0436 - val_accuracy: 0.3350\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9137 - accuracy: 0.4705 - val_loss: 1.9128 - val_accuracy: 0.4600\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.7768 - accuracy: 0.5645 - val_loss: 1.8005 - val_accuracy: 0.5600\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6603 - accuracy: 0.6265 - val_loss: 1.7023 - val_accuracy: 0.6150\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.5592 - accuracy: 0.6650 - val_loss: 1.6148 - val_accuracy: 0.6550\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4696 - accuracy: 0.7005 - val_loss: 1.5358 - val_accuracy: 0.7050\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3896 - accuracy: 0.7235 - val_loss: 1.4645 - val_accuracy: 0.7400\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3182 - accuracy: 0.7355 - val_loss: 1.4003 - val_accuracy: 0.7600\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2539 - accuracy: 0.7570 - val_loss: 1.3422 - val_accuracy: 0.7650\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1956 - accuracy: 0.7715 - val_loss: 1.2891 - val_accuracy: 0.7700\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1427 - accuracy: 0.7800 - val_loss: 1.2393 - val_accuracy: 0.7700\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.0944 - accuracy: 0.7915 - val_loss: 1.1931 - val_accuracy: 0.7750\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0505 - accuracy: 0.7995 - val_loss: 1.1513 - val_accuracy: 0.7800\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0103 - accuracy: 0.8075 - val_loss: 1.1134 - val_accuracy: 0.7800\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9736 - accuracy: 0.8140 - val_loss: 1.0783 - val_accuracy: 0.7850\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9398 - accuracy: 0.8225 - val_loss: 1.0462 - val_accuracy: 0.7950\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9087 - accuracy: 0.8310 - val_loss: 1.0168 - val_accuracy: 0.7950\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.8799 - accuracy: 0.8335 - val_loss: 0.9875 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8532 - accuracy: 0.8355 - val_loss: 0.9614 - val_accuracy: 0.8050\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8284 - accuracy: 0.8410 - val_loss: 0.9368 - val_accuracy: 0.8050\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8055 - accuracy: 0.8460 - val_loss: 0.9142 - val_accuracy: 0.8050\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7841 - accuracy: 0.8465 - val_loss: 0.8935 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7642 - accuracy: 0.8520 - val_loss: 0.8737 - val_accuracy: 0.8250\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7454 - accuracy: 0.8525 - val_loss: 0.8549 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7280 - accuracy: 0.8565 - val_loss: 0.8367 - val_accuracy: 0.8250\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7114 - accuracy: 0.8570 - val_loss: 0.8197 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6961 - accuracy: 0.8585 - val_loss: 0.8044 - val_accuracy: 0.8250\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6816 - accuracy: 0.8600 - val_loss: 0.7898 - val_accuracy: 0.8250\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6680 - accuracy: 0.8620 - val_loss: 0.7762 - val_accuracy: 0.8350\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6553 - accuracy: 0.8610 - val_loss: 0.7639 - val_accuracy: 0.8350\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6432 - accuracy: 0.8635 - val_loss: 0.7515 - val_accuracy: 0.8350\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6318 - accuracy: 0.8630 - val_loss: 0.7398 - val_accuracy: 0.8400\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6210 - accuracy: 0.8655 - val_loss: 0.7285 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6108 - accuracy: 0.8665 - val_loss: 0.7187 - val_accuracy: 0.8400\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6010 - accuracy: 0.8665 - val_loss: 0.7082 - val_accuracy: 0.8400\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5917 - accuracy: 0.8670 - val_loss: 0.6991 - val_accuracy: 0.8450\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5829 - accuracy: 0.8700 - val_loss: 0.6898 - val_accuracy: 0.8450\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5745 - accuracy: 0.8715 - val_loss: 0.6814 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5665 - accuracy: 0.8715 - val_loss: 0.6731 - val_accuracy: 0.8500\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5587 - accuracy: 0.8745 - val_loss: 0.6663 - val_accuracy: 0.8550\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5514 - accuracy: 0.8735 - val_loss: 0.6582 - val_accuracy: 0.8550\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5444 - accuracy: 0.8760 - val_loss: 0.6510 - val_accuracy: 0.8550\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5376 - accuracy: 0.8755 - val_loss: 0.6438 - val_accuracy: 0.8550\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5311 - accuracy: 0.8780 - val_loss: 0.6367 - val_accuracy: 0.8550\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5249 - accuracy: 0.8790 - val_loss: 0.6305 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5188 - accuracy: 0.8810 - val_loss: 0.6243 - val_accuracy: 0.8600\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5131 - accuracy: 0.8810 - val_loss: 0.6186 - val_accuracy: 0.8600\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5076 - accuracy: 0.8810 - val_loss: 0.6125 - val_accuracy: 0.8600\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5022 - accuracy: 0.8815 - val_loss: 0.6074 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4970 - accuracy: 0.8820 - val_loss: 0.6020 - val_accuracy: 0.8600\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4920 - accuracy: 0.8825 - val_loss: 0.5975 - val_accuracy: 0.8600\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4872 - accuracy: 0.8820 - val_loss: 0.5921 - val_accuracy: 0.8600\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4826 - accuracy: 0.8835 - val_loss: 0.5877 - val_accuracy: 0.8600\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4780 - accuracy: 0.8840 - val_loss: 0.5828 - val_accuracy: 0.8600\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4737 - accuracy: 0.8865 - val_loss: 0.5783 - val_accuracy: 0.8600\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 4s 60ms/step - loss: 0.4694 - accuracy: 0.8865 - val_loss: 0.5747 - val_accuracy: 0.8600\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 5s 63ms/step - loss: 0.4653 - accuracy: 0.8875 - val_loss: 0.5703 - val_accuracy: 0.8650\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4613 - accuracy: 0.8890 - val_loss: 0.5663 - val_accuracy: 0.8650\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4574 - accuracy: 0.8895 - val_loss: 0.5628 - val_accuracy: 0.8650\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4536 - accuracy: 0.8890 - val_loss: 0.5582 - val_accuracy: 0.8650\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4499 - accuracy: 0.8895 - val_loss: 0.5553 - val_accuracy: 0.8650\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4464 - accuracy: 0.8910 - val_loss: 0.5515 - val_accuracy: 0.8650\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4429 - accuracy: 0.8925 - val_loss: 0.5484 - val_accuracy: 0.8650\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4395 - accuracy: 0.8935 - val_loss: 0.5453 - val_accuracy: 0.8650\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4362 - accuracy: 0.8950 - val_loss: 0.5415 - val_accuracy: 0.8650\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4329 - accuracy: 0.8940 - val_loss: 0.5383 - val_accuracy: 0.8650\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4298 - accuracy: 0.8950 - val_loss: 0.5356 - val_accuracy: 0.8650\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4267 - accuracy: 0.8940 - val_loss: 0.5321 - val_accuracy: 0.8650\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4238 - accuracy: 0.8965 - val_loss: 0.5290 - val_accuracy: 0.8650\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4208 - accuracy: 0.8965 - val_loss: 0.5265 - val_accuracy: 0.8650\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4181 - accuracy: 0.8970 - val_loss: 0.5241 - val_accuracy: 0.8650\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4153 - accuracy: 0.8975 - val_loss: 0.5210 - val_accuracy: 0.8650\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4125 - accuracy: 0.8985 - val_loss: 0.5180 - val_accuracy: 0.8650\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4100 - accuracy: 0.9000 - val_loss: 0.5151 - val_accuracy: 0.8650\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4074 - accuracy: 0.8995 - val_loss: 0.5127 - val_accuracy: 0.8650\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4048 - accuracy: 0.9000 - val_loss: 0.5109 - val_accuracy: 0.8650\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4024 - accuracy: 0.9010 - val_loss: 0.5083 - val_accuracy: 0.8650\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3999 - accuracy: 0.9010 - val_loss: 0.5063 - val_accuracy: 0.8650\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3975 - accuracy: 0.9010 - val_loss: 0.5041 - val_accuracy: 0.8650\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3951 - accuracy: 0.9010 - val_loss: 0.5021 - val_accuracy: 0.8650\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3929 - accuracy: 0.9020 - val_loss: 0.5003 - val_accuracy: 0.8650\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3906 - accuracy: 0.9030 - val_loss: 0.4972 - val_accuracy: 0.8650\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3884 - accuracy: 0.9025 - val_loss: 0.4952 - val_accuracy: 0.8650\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3862 - accuracy: 0.9045 - val_loss: 0.4930 - val_accuracy: 0.8700\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3841 - accuracy: 0.9045 - val_loss: 0.4914 - val_accuracy: 0.8700\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3820 - accuracy: 0.9045 - val_loss: 0.4893 - val_accuracy: 0.8700\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3799 - accuracy: 0.9050 - val_loss: 0.4874 - val_accuracy: 0.8700\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 43ms/step - loss: 0.3780 - accuracy: 0.9050 - val_loss: 0.4862 - val_accuracy: 0.8750\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3761 - accuracy: 0.9055 - val_loss: 0.4845 - val_accuracy: 0.8800\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3741 - accuracy: 0.9045 - val_loss: 0.4826 - val_accuracy: 0.8750\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3721 - accuracy: 0.9060 - val_loss: 0.4811 - val_accuracy: 0.8850\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3703 - accuracy: 0.9060 - val_loss: 0.4792 - val_accuracy: 0.8850\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3685 - accuracy: 0.9070 - val_loss: 0.4769 - val_accuracy: 0.8850\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3666 - accuracy: 0.9070 - val_loss: 0.4757 - val_accuracy: 0.8850\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3649 - accuracy: 0.9070 - val_loss: 0.4741 - val_accuracy: 0.8850\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3631 - accuracy: 0.9070 - val_loss: 0.4726 - val_accuracy: 0.8800\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3614 - accuracy: 0.9065 - val_loss: 0.4707 - val_accuracy: 0.8850\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3598 - accuracy: 0.9075 - val_loss: 0.4693 - val_accuracy: 0.8850\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3580 - accuracy: 0.9085 - val_loss: 0.4690 - val_accuracy: 0.8900\n",
            "Largemodel Test loss: 0.4469853639602661\n",
            "Largemodel Test accuracy: 0.8748999834060669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j_-85YFDA6y"
      },
      "source": [
        "##4.2 Dropout\n",
        "Dropout은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 Dropout layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 Dropout model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYHpRcVDX22",
        "outputId": "f39d7416-a282-4e2b-c00b-193b765aa2ad"
      },
      "source": [
        "dropout_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dropout(0.2))\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "dropout_model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "\n",
        "dropout_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_18 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuxjOGWEKo9"
      },
      "source": [
        "Dropout Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?\n",
        "\n",
        "※Dropout을 적용한 Model은 일정확률로 신경망의 뉴런을 비활성화시키기 때문에, 오버피팅을 방지하는 효과가 있습니다. 하지만 비활성화로 인해서  학습속도가 떨어진다는 단점이 존재합니다. 그 때문에 test accuracy는 같은 학습파라미터 조건에서 기존모델보다 더 낮을 수 있습니다. \n",
        "\n",
        "기존모델과 dropout을 적용한 모델에 대해서, train 데이터와 test데이터에 대한 accuracy차이를 주목해보시면 좋을 것같습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqMK-jsyED_h",
        "outputId": "156d1359-cce5-4a0c-8b31-b1a354938b40"
      },
      "source": [
        "dropout_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dropout_model_history=dropout_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = dropout_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Dropout model Test loss:', score[0])\n",
        "print('Dropout model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 44ms/step - loss: 2.3057 - accuracy: 0.1580 - val_loss: 2.1836 - val_accuracy: 0.2500\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1475 - accuracy: 0.2475 - val_loss: 2.0590 - val_accuracy: 0.3850\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0180 - accuracy: 0.3570 - val_loss: 1.9510 - val_accuracy: 0.4700\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9001 - accuracy: 0.4460 - val_loss: 1.8541 - val_accuracy: 0.5200\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.7958 - accuracy: 0.5225 - val_loss: 1.7663 - val_accuracy: 0.5650\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7150 - accuracy: 0.5760 - val_loss: 1.6859 - val_accuracy: 0.6000\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6185 - accuracy: 0.6210 - val_loss: 1.6119 - val_accuracy: 0.6450\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5382 - accuracy: 0.6530 - val_loss: 1.5437 - val_accuracy: 0.6550\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4724 - accuracy: 0.6710 - val_loss: 1.4794 - val_accuracy: 0.6750\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4030 - accuracy: 0.6945 - val_loss: 1.4206 - val_accuracy: 0.6850\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3410 - accuracy: 0.7030 - val_loss: 1.3672 - val_accuracy: 0.6850\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2847 - accuracy: 0.7210 - val_loss: 1.3165 - val_accuracy: 0.7150\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.2296 - accuracy: 0.7380 - val_loss: 1.2698 - val_accuracy: 0.7350\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1874 - accuracy: 0.7485 - val_loss: 1.2270 - val_accuracy: 0.7400\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1426 - accuracy: 0.7525 - val_loss: 1.1863 - val_accuracy: 0.7450\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1037 - accuracy: 0.7585 - val_loss: 1.1491 - val_accuracy: 0.7550\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.0586 - accuracy: 0.7740 - val_loss: 1.1138 - val_accuracy: 0.7600\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.0268 - accuracy: 0.7890 - val_loss: 1.0816 - val_accuracy: 0.7700\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9928 - accuracy: 0.7860 - val_loss: 1.0516 - val_accuracy: 0.7750\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9636 - accuracy: 0.7925 - val_loss: 1.0227 - val_accuracy: 0.7800\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9387 - accuracy: 0.7940 - val_loss: 0.9959 - val_accuracy: 0.7800\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9081 - accuracy: 0.8025 - val_loss: 0.9714 - val_accuracy: 0.7750\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8852 - accuracy: 0.8115 - val_loss: 0.9482 - val_accuracy: 0.7750\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8678 - accuracy: 0.8035 - val_loss: 0.9258 - val_accuracy: 0.7900\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8384 - accuracy: 0.8180 - val_loss: 0.9064 - val_accuracy: 0.7900\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8215 - accuracy: 0.8265 - val_loss: 0.8866 - val_accuracy: 0.7900\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8004 - accuracy: 0.8180 - val_loss: 0.8692 - val_accuracy: 0.7900\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7810 - accuracy: 0.8285 - val_loss: 0.8512 - val_accuracy: 0.7900\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7692 - accuracy: 0.8260 - val_loss: 0.8349 - val_accuracy: 0.7950\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7513 - accuracy: 0.8315 - val_loss: 0.8197 - val_accuracy: 0.7950\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7391 - accuracy: 0.8275 - val_loss: 0.8060 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7260 - accuracy: 0.8320 - val_loss: 0.7924 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7089 - accuracy: 0.8395 - val_loss: 0.7790 - val_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6983 - accuracy: 0.8400 - val_loss: 0.7668 - val_accuracy: 0.8100\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6855 - accuracy: 0.8410 - val_loss: 0.7549 - val_accuracy: 0.8200\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6703 - accuracy: 0.8425 - val_loss: 0.7431 - val_accuracy: 0.8200\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6626 - accuracy: 0.8400 - val_loss: 0.7321 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6488 - accuracy: 0.8485 - val_loss: 0.7226 - val_accuracy: 0.8200\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6394 - accuracy: 0.8570 - val_loss: 0.7132 - val_accuracy: 0.8200\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6264 - accuracy: 0.8535 - val_loss: 0.7042 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6248 - accuracy: 0.8505 - val_loss: 0.6956 - val_accuracy: 0.8200\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6157 - accuracy: 0.8480 - val_loss: 0.6872 - val_accuracy: 0.8200\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6084 - accuracy: 0.8585 - val_loss: 0.6793 - val_accuracy: 0.8200\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6005 - accuracy: 0.8540 - val_loss: 0.6722 - val_accuracy: 0.8250\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5885 - accuracy: 0.8615 - val_loss: 0.6639 - val_accuracy: 0.8250\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5873 - accuracy: 0.8570 - val_loss: 0.6569 - val_accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5759 - accuracy: 0.8650 - val_loss: 0.6489 - val_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5761 - accuracy: 0.8640 - val_loss: 0.6425 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5650 - accuracy: 0.8625 - val_loss: 0.6360 - val_accuracy: 0.8350\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5585 - accuracy: 0.8660 - val_loss: 0.6300 - val_accuracy: 0.8350\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5538 - accuracy: 0.8675 - val_loss: 0.6252 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5509 - accuracy: 0.8635 - val_loss: 0.6190 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5428 - accuracy: 0.8685 - val_loss: 0.6129 - val_accuracy: 0.8350\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5348 - accuracy: 0.8660 - val_loss: 0.6079 - val_accuracy: 0.8400\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5304 - accuracy: 0.8735 - val_loss: 0.6024 - val_accuracy: 0.8350\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5273 - accuracy: 0.8710 - val_loss: 0.5976 - val_accuracy: 0.8350\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5208 - accuracy: 0.8720 - val_loss: 0.5931 - val_accuracy: 0.8350\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5150 - accuracy: 0.8685 - val_loss: 0.5876 - val_accuracy: 0.8400\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5120 - accuracy: 0.8665 - val_loss: 0.5822 - val_accuracy: 0.8400\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5074 - accuracy: 0.8780 - val_loss: 0.5785 - val_accuracy: 0.8400\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4983 - accuracy: 0.8765 - val_loss: 0.5735 - val_accuracy: 0.8400\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4959 - accuracy: 0.8790 - val_loss: 0.5693 - val_accuracy: 0.8400\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4938 - accuracy: 0.8780 - val_loss: 0.5645 - val_accuracy: 0.8400\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4877 - accuracy: 0.8760 - val_loss: 0.5612 - val_accuracy: 0.8500\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4874 - accuracy: 0.8805 - val_loss: 0.5568 - val_accuracy: 0.8500\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4817 - accuracy: 0.8760 - val_loss: 0.5532 - val_accuracy: 0.8500\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4751 - accuracy: 0.8745 - val_loss: 0.5494 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4725 - accuracy: 0.8865 - val_loss: 0.5461 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4710 - accuracy: 0.8775 - val_loss: 0.5432 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4710 - accuracy: 0.8785 - val_loss: 0.5402 - val_accuracy: 0.8500\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4604 - accuracy: 0.8840 - val_loss: 0.5371 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4597 - accuracy: 0.8820 - val_loss: 0.5340 - val_accuracy: 0.8500\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4570 - accuracy: 0.8850 - val_loss: 0.5304 - val_accuracy: 0.8500\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4510 - accuracy: 0.8845 - val_loss: 0.5276 - val_accuracy: 0.8500\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4531 - accuracy: 0.8835 - val_loss: 0.5245 - val_accuracy: 0.8500\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4464 - accuracy: 0.8845 - val_loss: 0.5217 - val_accuracy: 0.8500\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4453 - accuracy: 0.8875 - val_loss: 0.5196 - val_accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4455 - accuracy: 0.8825 - val_loss: 0.5172 - val_accuracy: 0.8500\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4392 - accuracy: 0.8870 - val_loss: 0.5146 - val_accuracy: 0.8500\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4385 - accuracy: 0.8870 - val_loss: 0.5134 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4349 - accuracy: 0.8910 - val_loss: 0.5111 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4331 - accuracy: 0.8940 - val_loss: 0.5088 - val_accuracy: 0.8500\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4260 - accuracy: 0.8880 - val_loss: 0.5065 - val_accuracy: 0.8500\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4257 - accuracy: 0.8915 - val_loss: 0.5047 - val_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4283 - accuracy: 0.8900 - val_loss: 0.5017 - val_accuracy: 0.8500\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4193 - accuracy: 0.8930 - val_loss: 0.4995 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4191 - accuracy: 0.8935 - val_loss: 0.4966 - val_accuracy: 0.8500\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4138 - accuracy: 0.8965 - val_loss: 0.4940 - val_accuracy: 0.8500\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4165 - accuracy: 0.8930 - val_loss: 0.4920 - val_accuracy: 0.8500\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4127 - accuracy: 0.8915 - val_loss: 0.4902 - val_accuracy: 0.8500\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4071 - accuracy: 0.8955 - val_loss: 0.4886 - val_accuracy: 0.8500\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4079 - accuracy: 0.8980 - val_loss: 0.4866 - val_accuracy: 0.8500\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4068 - accuracy: 0.8995 - val_loss: 0.4846 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4047 - accuracy: 0.8950 - val_loss: 0.4829 - val_accuracy: 0.8500\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4001 - accuracy: 0.8975 - val_loss: 0.4810 - val_accuracy: 0.8500\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4027 - accuracy: 0.8970 - val_loss: 0.4798 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3995 - accuracy: 0.9015 - val_loss: 0.4778 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3930 - accuracy: 0.9045 - val_loss: 0.4754 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3936 - accuracy: 0.8950 - val_loss: 0.4739 - val_accuracy: 0.8450\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3899 - accuracy: 0.8985 - val_loss: 0.4718 - val_accuracy: 0.8450\n",
            "Dropout model Test loss: 0.4589352607727051\n",
            "Dropout model Test accuracy: 0.8702999949455261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEyhMagnGq8F"
      },
      "source": [
        "##4.3 BatchNormalization\n",
        "BatchNormalization(BN)은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 BN layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 BN model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIBMssiBJaB",
        "outputId": "e6139a1c-cf02-4941-f2e4-daca576ba22c"
      },
      "source": [
        "bn_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('relu'))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('relu'))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('relu'))\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "bn_model.build(input_shape=(None, 28, 28, 1))\n",
        "\n",
        "### END CODE HERE ###\n",
        "bn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_28 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_29 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQBrVW6KYr3"
      },
      "source": [
        "BN Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2oakRaKZ4y",
        "outputId": "7b0e052d-2aa3-4b22-d75a-5862189f398e"
      },
      "source": [
        "bn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "bn_model_history=bn_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = bn_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('BN model Test loss:', score[0])\n",
        "print('BN model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 38ms/step - loss: 2.4799 - accuracy: 0.1285 - val_loss: 2.5764 - val_accuracy: 0.1150\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.3379 - accuracy: 0.1455 - val_loss: 2.5272 - val_accuracy: 0.1150\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.2116 - accuracy: 0.1665 - val_loss: 2.4465 - val_accuracy: 0.1450\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.0877 - accuracy: 0.2120 - val_loss: 2.3192 - val_accuracy: 0.1500\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9730 - accuracy: 0.2780 - val_loss: 2.1610 - val_accuracy: 0.1650\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.8687 - accuracy: 0.3530 - val_loss: 2.0071 - val_accuracy: 0.2450\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.7698 - accuracy: 0.4425 - val_loss: 1.8756 - val_accuracy: 0.3600\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.6812 - accuracy: 0.4960 - val_loss: 1.7669 - val_accuracy: 0.4400\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.5991 - accuracy: 0.5490 - val_loss: 1.6757 - val_accuracy: 0.5150\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.5228 - accuracy: 0.6130 - val_loss: 1.5968 - val_accuracy: 0.5600\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4598 - accuracy: 0.6455 - val_loss: 1.5276 - val_accuracy: 0.6150\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.3977 - accuracy: 0.6850 - val_loss: 1.4641 - val_accuracy: 0.6550\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3399 - accuracy: 0.7155 - val_loss: 1.4069 - val_accuracy: 0.6750\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.2854 - accuracy: 0.7295 - val_loss: 1.3552 - val_accuracy: 0.6850\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2453 - accuracy: 0.7390 - val_loss: 1.3085 - val_accuracy: 0.7100\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2018 - accuracy: 0.7550 - val_loss: 1.2658 - val_accuracy: 0.7250\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.1566 - accuracy: 0.7630 - val_loss: 1.2249 - val_accuracy: 0.7250\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.1186 - accuracy: 0.7795 - val_loss: 1.1884 - val_accuracy: 0.7350\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.0894 - accuracy: 0.7805 - val_loss: 1.1536 - val_accuracy: 0.7650\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.0568 - accuracy: 0.7940 - val_loss: 1.1222 - val_accuracy: 0.7650\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.0311 - accuracy: 0.7845 - val_loss: 1.0928 - val_accuracy: 0.7700\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.9990 - accuracy: 0.8025 - val_loss: 1.0658 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.9750 - accuracy: 0.8015 - val_loss: 1.0402 - val_accuracy: 0.7700\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9606 - accuracy: 0.8060 - val_loss: 1.0168 - val_accuracy: 0.7750\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 3s 44ms/step - loss: 0.9382 - accuracy: 0.8115 - val_loss: 0.9950 - val_accuracy: 0.7750\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9094 - accuracy: 0.8210 - val_loss: 0.9735 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.8949 - accuracy: 0.8125 - val_loss: 0.9537 - val_accuracy: 0.7800\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8823 - accuracy: 0.8145 - val_loss: 0.9359 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8595 - accuracy: 0.8185 - val_loss: 0.9191 - val_accuracy: 0.7850\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8481 - accuracy: 0.8315 - val_loss: 0.9029 - val_accuracy: 0.7850\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8316 - accuracy: 0.8305 - val_loss: 0.8872 - val_accuracy: 0.7900\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8207 - accuracy: 0.8365 - val_loss: 0.8726 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.8041 - accuracy: 0.8245 - val_loss: 0.8583 - val_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7913 - accuracy: 0.8400 - val_loss: 0.8454 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7772 - accuracy: 0.8400 - val_loss: 0.8329 - val_accuracy: 0.8100\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7740 - accuracy: 0.8375 - val_loss: 0.8217 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7588 - accuracy: 0.8355 - val_loss: 0.8099 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7468 - accuracy: 0.8390 - val_loss: 0.7993 - val_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7354 - accuracy: 0.8390 - val_loss: 0.7894 - val_accuracy: 0.8100\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7240 - accuracy: 0.8505 - val_loss: 0.7797 - val_accuracy: 0.8100\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7206 - accuracy: 0.8410 - val_loss: 0.7692 - val_accuracy: 0.8150\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.7057 - accuracy: 0.8465 - val_loss: 0.7603 - val_accuracy: 0.8150\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6985 - accuracy: 0.8500 - val_loss: 0.7513 - val_accuracy: 0.8150\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6931 - accuracy: 0.8505 - val_loss: 0.7433 - val_accuracy: 0.8150\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6822 - accuracy: 0.8535 - val_loss: 0.7353 - val_accuracy: 0.8150\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6791 - accuracy: 0.8495 - val_loss: 0.7278 - val_accuracy: 0.8200\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6664 - accuracy: 0.8545 - val_loss: 0.7207 - val_accuracy: 0.8200\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6580 - accuracy: 0.8590 - val_loss: 0.7126 - val_accuracy: 0.8250\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6569 - accuracy: 0.8550 - val_loss: 0.7063 - val_accuracy: 0.8250\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6440 - accuracy: 0.8605 - val_loss: 0.7005 - val_accuracy: 0.8250\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6474 - accuracy: 0.8565 - val_loss: 0.6938 - val_accuracy: 0.8250\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.6391 - accuracy: 0.8520 - val_loss: 0.6875 - val_accuracy: 0.8300\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6287 - accuracy: 0.8650 - val_loss: 0.6816 - val_accuracy: 0.8300\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6260 - accuracy: 0.8595 - val_loss: 0.6761 - val_accuracy: 0.8300\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6128 - accuracy: 0.8655 - val_loss: 0.6706 - val_accuracy: 0.8400\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6111 - accuracy: 0.8660 - val_loss: 0.6655 - val_accuracy: 0.8450\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6127 - accuracy: 0.8660 - val_loss: 0.6599 - val_accuracy: 0.8450\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5985 - accuracy: 0.8710 - val_loss: 0.6548 - val_accuracy: 0.8450\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6026 - accuracy: 0.8660 - val_loss: 0.6497 - val_accuracy: 0.8450\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5921 - accuracy: 0.8695 - val_loss: 0.6454 - val_accuracy: 0.8450\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5910 - accuracy: 0.8695 - val_loss: 0.6403 - val_accuracy: 0.8450\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5780 - accuracy: 0.8795 - val_loss: 0.6362 - val_accuracy: 0.8500\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5810 - accuracy: 0.8735 - val_loss: 0.6319 - val_accuracy: 0.8500\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5705 - accuracy: 0.8715 - val_loss: 0.6283 - val_accuracy: 0.8500\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5728 - accuracy: 0.8755 - val_loss: 0.6237 - val_accuracy: 0.8500\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5656 - accuracy: 0.8740 - val_loss: 0.6200 - val_accuracy: 0.8500\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5609 - accuracy: 0.8790 - val_loss: 0.6156 - val_accuracy: 0.8550\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5730 - accuracy: 0.8680 - val_loss: 0.6123 - val_accuracy: 0.8550\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5601 - accuracy: 0.8770 - val_loss: 0.6084 - val_accuracy: 0.8550\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5554 - accuracy: 0.8720 - val_loss: 0.6055 - val_accuracy: 0.8550\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5401 - accuracy: 0.8785 - val_loss: 0.6020 - val_accuracy: 0.8550\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5416 - accuracy: 0.8815 - val_loss: 0.5991 - val_accuracy: 0.8550\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5385 - accuracy: 0.8825 - val_loss: 0.5961 - val_accuracy: 0.8550\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5366 - accuracy: 0.8845 - val_loss: 0.5928 - val_accuracy: 0.8550\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5321 - accuracy: 0.8750 - val_loss: 0.5900 - val_accuracy: 0.8550\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5235 - accuracy: 0.8860 - val_loss: 0.5867 - val_accuracy: 0.8550\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5241 - accuracy: 0.8830 - val_loss: 0.5834 - val_accuracy: 0.8550\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5231 - accuracy: 0.8820 - val_loss: 0.5810 - val_accuracy: 0.8550\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5140 - accuracy: 0.8885 - val_loss: 0.5779 - val_accuracy: 0.8550\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5192 - accuracy: 0.8775 - val_loss: 0.5754 - val_accuracy: 0.8550\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5106 - accuracy: 0.8890 - val_loss: 0.5729 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5050 - accuracy: 0.8890 - val_loss: 0.5711 - val_accuracy: 0.8500\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5089 - accuracy: 0.8890 - val_loss: 0.5684 - val_accuracy: 0.8500\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5017 - accuracy: 0.8920 - val_loss: 0.5658 - val_accuracy: 0.8500\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5138 - accuracy: 0.8825 - val_loss: 0.5629 - val_accuracy: 0.8500\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5008 - accuracy: 0.8860 - val_loss: 0.5601 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4969 - accuracy: 0.8895 - val_loss: 0.5584 - val_accuracy: 0.8500\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4978 - accuracy: 0.8865 - val_loss: 0.5560 - val_accuracy: 0.8450\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4964 - accuracy: 0.8840 - val_loss: 0.5540 - val_accuracy: 0.8400\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4972 - accuracy: 0.8795 - val_loss: 0.5520 - val_accuracy: 0.8400\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4949 - accuracy: 0.8860 - val_loss: 0.5499 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4853 - accuracy: 0.8915 - val_loss: 0.5475 - val_accuracy: 0.8450\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4877 - accuracy: 0.8920 - val_loss: 0.5458 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4753 - accuracy: 0.8945 - val_loss: 0.5441 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4755 - accuracy: 0.8940 - val_loss: 0.5431 - val_accuracy: 0.8500\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4724 - accuracy: 0.8935 - val_loss: 0.5407 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4745 - accuracy: 0.8935 - val_loss: 0.5387 - val_accuracy: 0.8500\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4739 - accuracy: 0.8950 - val_loss: 0.5366 - val_accuracy: 0.8550\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4673 - accuracy: 0.8925 - val_loss: 0.5358 - val_accuracy: 0.8550\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4657 - accuracy: 0.8935 - val_loss: 0.5335 - val_accuracy: 0.8500\n",
            "BN model Test loss: 0.5068138241767883\n",
            "BN model Test accuracy: 0.8705000281333923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fDPwaFsJ47I"
      },
      "source": [
        "##4.4 Final Model\n",
        "지금까지 썼던 Regularization 기법들을 종합선물세트로 적용해 봅시다. 다음과 같은 Final model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ppnPcELADo",
        "outputId": "dd250703-9aa8-4f6f-ef01-9b88e18296d6"
      },
      "source": [
        "final_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "final_model.add(Flatten())\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation('relu'))\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation('relu'))\n",
        "final_model.add(Dense(1024))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Activation('relu'))\n",
        "final_model.add(Dropout(0.2))\n",
        "final_model.add(Flatten())\n",
        "final_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "final_model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "final_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_26 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_27 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocok8VnoLcb4"
      },
      "source": [
        "Final Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bFRiHtLdVe",
        "outputId": "f15ddb78-7080-4273-fa98-138fe93c1348"
      },
      "source": [
        "final_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "final_model_history=final_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = final_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Final model Test loss:', score[0])\n",
        "print('Final model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 40ms/step - loss: 2.6263 - accuracy: 0.1125 - val_loss: 2.2437 - val_accuracy: 0.1600\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.4233 - accuracy: 0.1400 - val_loss: 2.1743 - val_accuracy: 0.1750\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.2772 - accuracy: 0.2050 - val_loss: 2.0663 - val_accuracy: 0.2050\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.1225 - accuracy: 0.2570 - val_loss: 1.9451 - val_accuracy: 0.2950\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 1.9986 - accuracy: 0.2970 - val_loss: 1.8251 - val_accuracy: 0.3700\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.8963 - accuracy: 0.3450 - val_loss: 1.7153 - val_accuracy: 0.4300\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 1.7524 - accuracy: 0.4025 - val_loss: 1.6209 - val_accuracy: 0.5050\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 1.7004 - accuracy: 0.4365 - val_loss: 1.5361 - val_accuracy: 0.5200\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.6034 - accuracy: 0.4765 - val_loss: 1.4606 - val_accuracy: 0.5600\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.5179 - accuracy: 0.5235 - val_loss: 1.3952 - val_accuracy: 0.5800\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.4422 - accuracy: 0.5555 - val_loss: 1.3351 - val_accuracy: 0.6050\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.3753 - accuracy: 0.5705 - val_loss: 1.2781 - val_accuracy: 0.6350\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.3101 - accuracy: 0.6005 - val_loss: 1.2281 - val_accuracy: 0.6450\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.2628 - accuracy: 0.6165 - val_loss: 1.1806 - val_accuracy: 0.6650\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 1.2113 - accuracy: 0.6345 - val_loss: 1.1402 - val_accuracy: 0.6800\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.1619 - accuracy: 0.6535 - val_loss: 1.0988 - val_accuracy: 0.6900\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.1160 - accuracy: 0.6680 - val_loss: 1.0618 - val_accuracy: 0.7050\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.0691 - accuracy: 0.6795 - val_loss: 1.0297 - val_accuracy: 0.7100\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.0367 - accuracy: 0.7080 - val_loss: 0.9982 - val_accuracy: 0.7100\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9942 - accuracy: 0.7090 - val_loss: 0.9684 - val_accuracy: 0.7250\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9812 - accuracy: 0.7090 - val_loss: 0.9391 - val_accuracy: 0.7350\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9365 - accuracy: 0.7395 - val_loss: 0.9128 - val_accuracy: 0.7350\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9044 - accuracy: 0.7505 - val_loss: 0.8888 - val_accuracy: 0.7450\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.8824 - accuracy: 0.7505 - val_loss: 0.8666 - val_accuracy: 0.7550\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.8519 - accuracy: 0.7695 - val_loss: 0.8440 - val_accuracy: 0.7700\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.8179 - accuracy: 0.7685 - val_loss: 0.8234 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.7926 - accuracy: 0.7855 - val_loss: 0.8042 - val_accuracy: 0.7750\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.7867 - accuracy: 0.7725 - val_loss: 0.7867 - val_accuracy: 0.7800\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.7726 - accuracy: 0.7835 - val_loss: 0.7691 - val_accuracy: 0.7900\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.7465 - accuracy: 0.8025 - val_loss: 0.7531 - val_accuracy: 0.7950\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 3s 40ms/step - loss: 0.7459 - accuracy: 0.8005 - val_loss: 0.7369 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 0.7167 - accuracy: 0.8065 - val_loss: 0.7222 - val_accuracy: 0.7950\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.7073 - accuracy: 0.8130 - val_loss: 0.7075 - val_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.6639 - accuracy: 0.8300 - val_loss: 0.6954 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.6558 - accuracy: 0.8300 - val_loss: 0.6831 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.6389 - accuracy: 0.8230 - val_loss: 0.6719 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 0.6473 - accuracy: 0.8305 - val_loss: 0.6596 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 0.6124 - accuracy: 0.8460 - val_loss: 0.6489 - val_accuracy: 0.8100\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.6130 - accuracy: 0.8370 - val_loss: 0.6386 - val_accuracy: 0.8200\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.5988 - accuracy: 0.8445 - val_loss: 0.6279 - val_accuracy: 0.8350\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 0.5828 - accuracy: 0.8535 - val_loss: 0.6173 - val_accuracy: 0.8400\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5816 - accuracy: 0.8445 - val_loss: 0.6086 - val_accuracy: 0.8400\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.5635 - accuracy: 0.8530 - val_loss: 0.5992 - val_accuracy: 0.8400\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5467 - accuracy: 0.8650 - val_loss: 0.5924 - val_accuracy: 0.8450\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5273 - accuracy: 0.8710 - val_loss: 0.5829 - val_accuracy: 0.8500\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5326 - accuracy: 0.8715 - val_loss: 0.5764 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5299 - accuracy: 0.8695 - val_loss: 0.5681 - val_accuracy: 0.8600\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5091 - accuracy: 0.8615 - val_loss: 0.5597 - val_accuracy: 0.8650\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.5115 - accuracy: 0.8695 - val_loss: 0.5541 - val_accuracy: 0.8700\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5010 - accuracy: 0.8720 - val_loss: 0.5474 - val_accuracy: 0.8700\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.5024 - accuracy: 0.8730 - val_loss: 0.5405 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4775 - accuracy: 0.8785 - val_loss: 0.5341 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.4713 - accuracy: 0.8865 - val_loss: 0.5290 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.4759 - accuracy: 0.8825 - val_loss: 0.5216 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4593 - accuracy: 0.8800 - val_loss: 0.5158 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4533 - accuracy: 0.8795 - val_loss: 0.5109 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4351 - accuracy: 0.8985 - val_loss: 0.5058 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4379 - accuracy: 0.8905 - val_loss: 0.5012 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4215 - accuracy: 0.9025 - val_loss: 0.4954 - val_accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4250 - accuracy: 0.8895 - val_loss: 0.4910 - val_accuracy: 0.8750\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.4105 - accuracy: 0.8945 - val_loss: 0.4864 - val_accuracy: 0.8750\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4150 - accuracy: 0.8925 - val_loss: 0.4830 - val_accuracy: 0.8800\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4077 - accuracy: 0.9020 - val_loss: 0.4775 - val_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3960 - accuracy: 0.9060 - val_loss: 0.4724 - val_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.4021 - accuracy: 0.9090 - val_loss: 0.4682 - val_accuracy: 0.8800\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3957 - accuracy: 0.8930 - val_loss: 0.4646 - val_accuracy: 0.8800\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3890 - accuracy: 0.9100 - val_loss: 0.4607 - val_accuracy: 0.8800\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3903 - accuracy: 0.9075 - val_loss: 0.4563 - val_accuracy: 0.8800\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3914 - accuracy: 0.8985 - val_loss: 0.4531 - val_accuracy: 0.8750\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.3863 - accuracy: 0.9085 - val_loss: 0.4488 - val_accuracy: 0.8750\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3760 - accuracy: 0.9100 - val_loss: 0.4442 - val_accuracy: 0.8800\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3669 - accuracy: 0.9075 - val_loss: 0.4411 - val_accuracy: 0.8800\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3718 - accuracy: 0.9170 - val_loss: 0.4371 - val_accuracy: 0.8800\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3614 - accuracy: 0.9125 - val_loss: 0.4340 - val_accuracy: 0.8850\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3405 - accuracy: 0.9225 - val_loss: 0.4313 - val_accuracy: 0.8850\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3528 - accuracy: 0.9170 - val_loss: 0.4287 - val_accuracy: 0.8850\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3425 - accuracy: 0.9175 - val_loss: 0.4255 - val_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3413 - accuracy: 0.9210 - val_loss: 0.4229 - val_accuracy: 0.8900\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3411 - accuracy: 0.9150 - val_loss: 0.4202 - val_accuracy: 0.8950\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3310 - accuracy: 0.9175 - val_loss: 0.4164 - val_accuracy: 0.8950\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3325 - accuracy: 0.9215 - val_loss: 0.4132 - val_accuracy: 0.8950\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3184 - accuracy: 0.9270 - val_loss: 0.4115 - val_accuracy: 0.8950\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3131 - accuracy: 0.9275 - val_loss: 0.4082 - val_accuracy: 0.8950\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3127 - accuracy: 0.9250 - val_loss: 0.4063 - val_accuracy: 0.8950\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3091 - accuracy: 0.9245 - val_loss: 0.4040 - val_accuracy: 0.8950\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.3228 - accuracy: 0.9255 - val_loss: 0.4012 - val_accuracy: 0.8950\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2944 - accuracy: 0.9300 - val_loss: 0.3981 - val_accuracy: 0.9000\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2951 - accuracy: 0.9295 - val_loss: 0.3957 - val_accuracy: 0.9000\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3083 - accuracy: 0.9260 - val_loss: 0.3941 - val_accuracy: 0.9000\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2912 - accuracy: 0.9330 - val_loss: 0.3916 - val_accuracy: 0.9050\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2874 - accuracy: 0.9310 - val_loss: 0.3888 - val_accuracy: 0.9050\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2962 - accuracy: 0.9375 - val_loss: 0.3870 - val_accuracy: 0.9000\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2776 - accuracy: 0.9345 - val_loss: 0.3842 - val_accuracy: 0.9050\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2842 - accuracy: 0.9355 - val_loss: 0.3813 - val_accuracy: 0.9050\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2838 - accuracy: 0.9350 - val_loss: 0.3798 - val_accuracy: 0.9100\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2741 - accuracy: 0.9370 - val_loss: 0.3780 - val_accuracy: 0.9100\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2668 - accuracy: 0.9375 - val_loss: 0.3757 - val_accuracy: 0.9100\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2629 - accuracy: 0.9445 - val_loss: 0.3734 - val_accuracy: 0.9100\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2697 - accuracy: 0.9340 - val_loss: 0.3724 - val_accuracy: 0.9100\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2592 - accuracy: 0.9425 - val_loss: 0.3705 - val_accuracy: 0.9100\n",
            "Final model Test loss: 0.3903407156467438\n",
            "Final model Test accuracy: 0.8842999935150146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPR3nEMAL4Yd"
      },
      "source": [
        "(optional) Training data가 늘어나면 regularization 효과가 나는 것을 보였습니다. 쉽게 training data를 늘릴 수 있는 방법은 data augmentation 입니다. 이 방법은 기존 training data를 적절히 rotating, flipping, scaling, shifting 하여 training data 수를 늘리는 것입니다. data augmentation의 regularization 효과를 테스트해 보세요. 또한 뉴럴 네트워크의 노드 개수나 층수를 바꿔서 성능을 올려보는 것도 테스트해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn11ZtM7GmS-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "flipped_image_updown = []\n",
        "flipped_image_leftright = []\n",
        "augmentated_data = []\n",
        "\n",
        "\n",
        "for i in large_x_train:\n",
        "  flipped_image_updown.append(tf.image.flip_up_down(i))\n",
        "  flipped_image_leftright.append(tf.image.flip_left_right(i))\n",
        "\n",
        "flipped_image = flipped_image_updown + flipped_image_leftright\n",
        "\n",
        "augmentated_label = large_y_train * 3\n",
        "  \n",
        "augmentated_data.extend(large_x_train)\n",
        "augmentated_data.extend(flipped_image)\n",
        "augmentated_data[0]\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('original')\n",
        "plt.imshow(np.squeeze(flipped_image[2]))\n"
      ],
      "metadata": {
        "id": "PjxByJBzBKar",
        "outputId": "6e9de823-fdc5-4796-9cdd-d007e72c4ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4d2e632a50>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ],
      "metadata": {
        "id": "VHgaOIBIb_Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_final_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "real_final_model.add(Flatten())\n",
        "real_final_model.add(Dense(1024))\n",
        "real_final_model.add(BatchNormalization())\n",
        "real_final_model.add(Activation('relu'))\n",
        "real_final_model.add(Dense(1024))\n",
        "real_final_model.add(Dropout(0.2))\n",
        "real_final_model.add(BatchNormalization())\n",
        "real_final_model.add(Activation('relu'))\n",
        "real_final_model.add(Dense(1024))\n",
        "real_final_model.add(BatchNormalization())\n",
        "real_final_model.add(Activation('relu'))\n",
        "real_final_model.add(Dropout(0.2))\n",
        "real_final_model.add(Flatten())\n",
        "real_final_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "real_final_model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "real_final_model.summary()"
      ],
      "metadata": {
        "id": "M9EmLK8STf1e",
        "outputId": "deafe068-dbb3-4d0a-aa8f-196f48a4f344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_final_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "real_final_model_history=real_final_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=200,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = real_final_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Real Final model Test loss:', score[0])\n",
        "print('Real Final model Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "HWNJ7gbRRUcT",
        "outputId": "7b70e35b-a4e8-4868-d9d1-3747a6785066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "72/72 [==============================] - 5s 52ms/step - loss: 2.8310 - accuracy: 0.1240 - val_loss: 2.3072 - val_accuracy: 0.1450\n",
            "Epoch 2/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 2.6922 - accuracy: 0.1320 - val_loss: 2.3089 - val_accuracy: 0.1800\n",
            "Epoch 3/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 2.5512 - accuracy: 0.1640 - val_loss: 2.2761 - val_accuracy: 0.2000\n",
            "Epoch 4/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 2.4147 - accuracy: 0.2305 - val_loss: 2.2069 - val_accuracy: 0.2400\n",
            "Epoch 5/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 2.3236 - accuracy: 0.2400 - val_loss: 2.1234 - val_accuracy: 0.3150\n",
            "Epoch 6/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 2.2221 - accuracy: 0.2665 - val_loss: 2.0418 - val_accuracy: 0.3400\n",
            "Epoch 7/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 2.1316 - accuracy: 0.3075 - val_loss: 1.9629 - val_accuracy: 0.3550\n",
            "Epoch 8/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 2.0907 - accuracy: 0.3150 - val_loss: 1.8897 - val_accuracy: 0.3750\n",
            "Epoch 9/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.9820 - accuracy: 0.3625 - val_loss: 1.8173 - val_accuracy: 0.3900\n",
            "Epoch 10/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 1.8925 - accuracy: 0.3795 - val_loss: 1.7494 - val_accuracy: 0.4250\n",
            "Epoch 11/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 1.8335 - accuracy: 0.4085 - val_loss: 1.6893 - val_accuracy: 0.4350\n",
            "Epoch 12/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 1.7680 - accuracy: 0.4360 - val_loss: 1.6307 - val_accuracy: 0.4350\n",
            "Epoch 13/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.7074 - accuracy: 0.4555 - val_loss: 1.5791 - val_accuracy: 0.4500\n",
            "Epoch 14/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.6403 - accuracy: 0.4830 - val_loss: 1.5251 - val_accuracy: 0.4600\n",
            "Epoch 15/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 1.5742 - accuracy: 0.4865 - val_loss: 1.4752 - val_accuracy: 0.4800\n",
            "Epoch 16/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.5040 - accuracy: 0.5315 - val_loss: 1.4285 - val_accuracy: 0.4950\n",
            "Epoch 17/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 1.5092 - accuracy: 0.5230 - val_loss: 1.3848 - val_accuracy: 0.5250\n",
            "Epoch 18/200\n",
            "72/72 [==============================] - 3s 49ms/step - loss: 1.4613 - accuracy: 0.5350 - val_loss: 1.3440 - val_accuracy: 0.5650\n",
            "Epoch 19/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 1.4006 - accuracy: 0.5495 - val_loss: 1.3058 - val_accuracy: 0.5950\n",
            "Epoch 20/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.3587 - accuracy: 0.5795 - val_loss: 1.2666 - val_accuracy: 0.6050\n",
            "Epoch 21/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 1.3238 - accuracy: 0.5855 - val_loss: 1.2306 - val_accuracy: 0.6200\n",
            "Epoch 22/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 1.2839 - accuracy: 0.5940 - val_loss: 1.1979 - val_accuracy: 0.6300\n",
            "Epoch 23/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 1.2386 - accuracy: 0.6140 - val_loss: 1.1653 - val_accuracy: 0.6500\n",
            "Epoch 24/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 1.2031 - accuracy: 0.6315 - val_loss: 1.1364 - val_accuracy: 0.6550\n",
            "Epoch 25/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 1.1839 - accuracy: 0.6385 - val_loss: 1.1042 - val_accuracy: 0.6550\n",
            "Epoch 26/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 1.1576 - accuracy: 0.6420 - val_loss: 1.0760 - val_accuracy: 0.6700\n",
            "Epoch 27/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 1.1087 - accuracy: 0.6720 - val_loss: 1.0499 - val_accuracy: 0.6800\n",
            "Epoch 28/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 1.1066 - accuracy: 0.6675 - val_loss: 1.0247 - val_accuracy: 0.6950\n",
            "Epoch 29/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 1.0495 - accuracy: 0.6790 - val_loss: 0.9999 - val_accuracy: 0.7000\n",
            "Epoch 30/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 1.0224 - accuracy: 0.6850 - val_loss: 0.9791 - val_accuracy: 0.7100\n",
            "Epoch 31/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 1.0234 - accuracy: 0.6865 - val_loss: 0.9544 - val_accuracy: 0.7200\n",
            "Epoch 32/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.9757 - accuracy: 0.7115 - val_loss: 0.9356 - val_accuracy: 0.7300\n",
            "Epoch 33/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.9759 - accuracy: 0.7075 - val_loss: 0.9146 - val_accuracy: 0.7400\n",
            "Epoch 34/200\n",
            "72/72 [==============================] - 4s 54ms/step - loss: 0.9485 - accuracy: 0.7320 - val_loss: 0.8975 - val_accuracy: 0.7500\n",
            "Epoch 35/200\n",
            "72/72 [==============================] - 4s 55ms/step - loss: 0.9283 - accuracy: 0.7290 - val_loss: 0.8805 - val_accuracy: 0.7500\n",
            "Epoch 36/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.9269 - accuracy: 0.7235 - val_loss: 0.8603 - val_accuracy: 0.7550\n",
            "Epoch 37/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.8882 - accuracy: 0.7485 - val_loss: 0.8455 - val_accuracy: 0.7700\n",
            "Epoch 38/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.8719 - accuracy: 0.7395 - val_loss: 0.8287 - val_accuracy: 0.7900\n",
            "Epoch 39/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.8673 - accuracy: 0.7550 - val_loss: 0.8116 - val_accuracy: 0.7900\n",
            "Epoch 40/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.8184 - accuracy: 0.7710 - val_loss: 0.7988 - val_accuracy: 0.7900\n",
            "Epoch 41/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.8276 - accuracy: 0.7605 - val_loss: 0.7858 - val_accuracy: 0.8000\n",
            "Epoch 42/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.7883 - accuracy: 0.7715 - val_loss: 0.7729 - val_accuracy: 0.8000\n",
            "Epoch 43/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.7981 - accuracy: 0.7735 - val_loss: 0.7607 - val_accuracy: 0.8050\n",
            "Epoch 44/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.7750 - accuracy: 0.7755 - val_loss: 0.7476 - val_accuracy: 0.8100\n",
            "Epoch 45/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.7574 - accuracy: 0.7880 - val_loss: 0.7366 - val_accuracy: 0.8200\n",
            "Epoch 46/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.7515 - accuracy: 0.7800 - val_loss: 0.7250 - val_accuracy: 0.8200\n",
            "Epoch 47/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.7528 - accuracy: 0.7820 - val_loss: 0.7153 - val_accuracy: 0.8250\n",
            "Epoch 48/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.7381 - accuracy: 0.7870 - val_loss: 0.7057 - val_accuracy: 0.8300\n",
            "Epoch 49/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.7094 - accuracy: 0.8005 - val_loss: 0.6961 - val_accuracy: 0.8250\n",
            "Epoch 50/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.7176 - accuracy: 0.7995 - val_loss: 0.6857 - val_accuracy: 0.8350\n",
            "Epoch 51/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.6981 - accuracy: 0.8070 - val_loss: 0.6764 - val_accuracy: 0.8350\n",
            "Epoch 52/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.6871 - accuracy: 0.8095 - val_loss: 0.6686 - val_accuracy: 0.8350\n",
            "Epoch 53/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.6647 - accuracy: 0.8225 - val_loss: 0.6619 - val_accuracy: 0.8350\n",
            "Epoch 54/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.6597 - accuracy: 0.8160 - val_loss: 0.6524 - val_accuracy: 0.8350\n",
            "Epoch 55/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.6471 - accuracy: 0.8155 - val_loss: 0.6442 - val_accuracy: 0.8350\n",
            "Epoch 56/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.6400 - accuracy: 0.8255 - val_loss: 0.6366 - val_accuracy: 0.8350\n",
            "Epoch 57/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.6531 - accuracy: 0.8200 - val_loss: 0.6279 - val_accuracy: 0.8350\n",
            "Epoch 58/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.6180 - accuracy: 0.8320 - val_loss: 0.6232 - val_accuracy: 0.8350\n",
            "Epoch 59/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.6069 - accuracy: 0.8355 - val_loss: 0.6143 - val_accuracy: 0.8350\n",
            "Epoch 60/200\n",
            "72/72 [==============================] - 3s 49ms/step - loss: 0.6264 - accuracy: 0.8290 - val_loss: 0.6083 - val_accuracy: 0.8400\n",
            "Epoch 61/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.6041 - accuracy: 0.8340 - val_loss: 0.6019 - val_accuracy: 0.8350\n",
            "Epoch 62/200\n",
            "72/72 [==============================] - 4s 55ms/step - loss: 0.5899 - accuracy: 0.8455 - val_loss: 0.5953 - val_accuracy: 0.8300\n",
            "Epoch 63/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.6034 - accuracy: 0.8285 - val_loss: 0.5901 - val_accuracy: 0.8350\n",
            "Epoch 64/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.5702 - accuracy: 0.8430 - val_loss: 0.5845 - val_accuracy: 0.8400\n",
            "Epoch 65/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.5655 - accuracy: 0.8485 - val_loss: 0.5778 - val_accuracy: 0.8550\n",
            "Epoch 66/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.5669 - accuracy: 0.8510 - val_loss: 0.5715 - val_accuracy: 0.8550\n",
            "Epoch 67/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.5679 - accuracy: 0.8480 - val_loss: 0.5657 - val_accuracy: 0.8650\n",
            "Epoch 68/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.5563 - accuracy: 0.8540 - val_loss: 0.5615 - val_accuracy: 0.8650\n",
            "Epoch 69/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.5575 - accuracy: 0.8445 - val_loss: 0.5579 - val_accuracy: 0.8650\n",
            "Epoch 70/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.5582 - accuracy: 0.8500 - val_loss: 0.5520 - val_accuracy: 0.8650\n",
            "Epoch 71/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.5415 - accuracy: 0.8540 - val_loss: 0.5476 - val_accuracy: 0.8650\n",
            "Epoch 72/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.5508 - accuracy: 0.8425 - val_loss: 0.5433 - val_accuracy: 0.8600\n",
            "Epoch 73/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.5270 - accuracy: 0.8615 - val_loss: 0.5386 - val_accuracy: 0.8600\n",
            "Epoch 74/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.5201 - accuracy: 0.8615 - val_loss: 0.5337 - val_accuracy: 0.8600\n",
            "Epoch 75/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.5188 - accuracy: 0.8600 - val_loss: 0.5276 - val_accuracy: 0.8600\n",
            "Epoch 76/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 0.5125 - accuracy: 0.8590 - val_loss: 0.5244 - val_accuracy: 0.8600\n",
            "Epoch 77/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4882 - accuracy: 0.8665 - val_loss: 0.5187 - val_accuracy: 0.8600\n",
            "Epoch 78/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 0.5009 - accuracy: 0.8655 - val_loss: 0.5162 - val_accuracy: 0.8600\n",
            "Epoch 79/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4949 - accuracy: 0.8625 - val_loss: 0.5121 - val_accuracy: 0.8650\n",
            "Epoch 80/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.4964 - accuracy: 0.8645 - val_loss: 0.5096 - val_accuracy: 0.8650\n",
            "Epoch 81/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.4862 - accuracy: 0.8705 - val_loss: 0.5055 - val_accuracy: 0.8650\n",
            "Epoch 82/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.4774 - accuracy: 0.8665 - val_loss: 0.5014 - val_accuracy: 0.8650\n",
            "Epoch 83/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.4848 - accuracy: 0.8730 - val_loss: 0.4951 - val_accuracy: 0.8750\n",
            "Epoch 84/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4769 - accuracy: 0.8715 - val_loss: 0.4934 - val_accuracy: 0.8700\n",
            "Epoch 85/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.4644 - accuracy: 0.8695 - val_loss: 0.4897 - val_accuracy: 0.8700\n",
            "Epoch 86/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.4794 - accuracy: 0.8700 - val_loss: 0.4864 - val_accuracy: 0.8750\n",
            "Epoch 87/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.4575 - accuracy: 0.8740 - val_loss: 0.4830 - val_accuracy: 0.8800\n",
            "Epoch 88/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.4709 - accuracy: 0.8715 - val_loss: 0.4795 - val_accuracy: 0.8750\n",
            "Epoch 89/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.4462 - accuracy: 0.8760 - val_loss: 0.4777 - val_accuracy: 0.8750\n",
            "Epoch 90/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4472 - accuracy: 0.8835 - val_loss: 0.4745 - val_accuracy: 0.8750\n",
            "Epoch 91/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.4404 - accuracy: 0.8815 - val_loss: 0.4724 - val_accuracy: 0.8750\n",
            "Epoch 92/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.4494 - accuracy: 0.8770 - val_loss: 0.4682 - val_accuracy: 0.8750\n",
            "Epoch 93/200\n",
            "72/72 [==============================] - 3s 45ms/step - loss: 0.4447 - accuracy: 0.8885 - val_loss: 0.4654 - val_accuracy: 0.8750\n",
            "Epoch 94/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.4151 - accuracy: 0.8915 - val_loss: 0.4633 - val_accuracy: 0.8750\n",
            "Epoch 95/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.4313 - accuracy: 0.8875 - val_loss: 0.4619 - val_accuracy: 0.8750\n",
            "Epoch 96/200\n",
            "72/72 [==============================] - 3s 49ms/step - loss: 0.4253 - accuracy: 0.8890 - val_loss: 0.4582 - val_accuracy: 0.8750\n",
            "Epoch 97/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.4243 - accuracy: 0.8860 - val_loss: 0.4548 - val_accuracy: 0.8800\n",
            "Epoch 98/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.4237 - accuracy: 0.8880 - val_loss: 0.4516 - val_accuracy: 0.8850\n",
            "Epoch 99/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.4172 - accuracy: 0.8895 - val_loss: 0.4479 - val_accuracy: 0.8850\n",
            "Epoch 100/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.4164 - accuracy: 0.8880 - val_loss: 0.4450 - val_accuracy: 0.8900\n",
            "Epoch 101/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.4010 - accuracy: 0.8910 - val_loss: 0.4420 - val_accuracy: 0.8950\n",
            "Epoch 102/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.3910 - accuracy: 0.9045 - val_loss: 0.4401 - val_accuracy: 0.8900\n",
            "Epoch 103/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.4173 - accuracy: 0.8965 - val_loss: 0.4391 - val_accuracy: 0.8900\n",
            "Epoch 104/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.3964 - accuracy: 0.8895 - val_loss: 0.4374 - val_accuracy: 0.8900\n",
            "Epoch 105/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.3968 - accuracy: 0.8955 - val_loss: 0.4347 - val_accuracy: 0.8900\n",
            "Epoch 106/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.3960 - accuracy: 0.8965 - val_loss: 0.4337 - val_accuracy: 0.8850\n",
            "Epoch 107/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.3874 - accuracy: 0.8930 - val_loss: 0.4309 - val_accuracy: 0.8850\n",
            "Epoch 108/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.3855 - accuracy: 0.8980 - val_loss: 0.4293 - val_accuracy: 0.8850\n",
            "Epoch 109/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.3829 - accuracy: 0.9045 - val_loss: 0.4270 - val_accuracy: 0.8850\n",
            "Epoch 110/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.3831 - accuracy: 0.8895 - val_loss: 0.4243 - val_accuracy: 0.8950\n",
            "Epoch 111/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.3839 - accuracy: 0.8995 - val_loss: 0.4224 - val_accuracy: 0.8950\n",
            "Epoch 112/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.3755 - accuracy: 0.8960 - val_loss: 0.4221 - val_accuracy: 0.8950\n",
            "Epoch 113/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.3681 - accuracy: 0.9020 - val_loss: 0.4187 - val_accuracy: 0.8950\n",
            "Epoch 114/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.3556 - accuracy: 0.9065 - val_loss: 0.4170 - val_accuracy: 0.8950\n",
            "Epoch 115/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3653 - accuracy: 0.9070 - val_loss: 0.4129 - val_accuracy: 0.8950\n",
            "Epoch 116/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.3594 - accuracy: 0.9060 - val_loss: 0.4121 - val_accuracy: 0.8950\n",
            "Epoch 117/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.3423 - accuracy: 0.9090 - val_loss: 0.4108 - val_accuracy: 0.8950\n",
            "Epoch 118/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.3586 - accuracy: 0.9105 - val_loss: 0.4078 - val_accuracy: 0.8950\n",
            "Epoch 119/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3481 - accuracy: 0.9160 - val_loss: 0.4071 - val_accuracy: 0.8950\n",
            "Epoch 120/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3468 - accuracy: 0.9110 - val_loss: 0.4053 - val_accuracy: 0.8950\n",
            "Epoch 121/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3599 - accuracy: 0.9060 - val_loss: 0.4031 - val_accuracy: 0.8950\n",
            "Epoch 122/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3347 - accuracy: 0.9140 - val_loss: 0.4011 - val_accuracy: 0.8950\n",
            "Epoch 123/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3425 - accuracy: 0.9120 - val_loss: 0.4001 - val_accuracy: 0.8950\n",
            "Epoch 124/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3356 - accuracy: 0.9145 - val_loss: 0.3987 - val_accuracy: 0.8950\n",
            "Epoch 125/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3318 - accuracy: 0.9155 - val_loss: 0.3969 - val_accuracy: 0.8950\n",
            "Epoch 126/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3395 - accuracy: 0.9095 - val_loss: 0.3936 - val_accuracy: 0.8950\n",
            "Epoch 127/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3228 - accuracy: 0.9200 - val_loss: 0.3927 - val_accuracy: 0.8950\n",
            "Epoch 128/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3212 - accuracy: 0.9215 - val_loss: 0.3921 - val_accuracy: 0.8950\n",
            "Epoch 129/200\n",
            "72/72 [==============================] - 4s 56ms/step - loss: 0.3283 - accuracy: 0.9195 - val_loss: 0.3897 - val_accuracy: 0.8950\n",
            "Epoch 130/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3212 - accuracy: 0.9210 - val_loss: 0.3879 - val_accuracy: 0.8950\n",
            "Epoch 131/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3184 - accuracy: 0.9185 - val_loss: 0.3872 - val_accuracy: 0.8950\n",
            "Epoch 132/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3256 - accuracy: 0.9175 - val_loss: 0.3867 - val_accuracy: 0.8950\n",
            "Epoch 133/200\n",
            "72/72 [==============================] - 4s 56ms/step - loss: 0.3245 - accuracy: 0.9225 - val_loss: 0.3856 - val_accuracy: 0.8950\n",
            "Epoch 134/200\n",
            "72/72 [==============================] - 4s 56ms/step - loss: 0.3231 - accuracy: 0.9180 - val_loss: 0.3827 - val_accuracy: 0.8950\n",
            "Epoch 135/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3132 - accuracy: 0.9225 - val_loss: 0.3810 - val_accuracy: 0.8950\n",
            "Epoch 136/200\n",
            "72/72 [==============================] - 4s 56ms/step - loss: 0.3246 - accuracy: 0.9235 - val_loss: 0.3798 - val_accuracy: 0.8950\n",
            "Epoch 137/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3104 - accuracy: 0.9160 - val_loss: 0.3788 - val_accuracy: 0.8950\n",
            "Epoch 138/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.3021 - accuracy: 0.9200 - val_loss: 0.3776 - val_accuracy: 0.8950\n",
            "Epoch 139/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.3180 - accuracy: 0.9250 - val_loss: 0.3759 - val_accuracy: 0.8950\n",
            "Epoch 140/200\n",
            "72/72 [==============================] - 4s 54ms/step - loss: 0.3109 - accuracy: 0.9190 - val_loss: 0.3750 - val_accuracy: 0.8950\n",
            "Epoch 141/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.3009 - accuracy: 0.9220 - val_loss: 0.3735 - val_accuracy: 0.8950\n",
            "Epoch 142/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2915 - accuracy: 0.9225 - val_loss: 0.3729 - val_accuracy: 0.8950\n",
            "Epoch 143/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.3142 - accuracy: 0.9105 - val_loss: 0.3695 - val_accuracy: 0.8950\n",
            "Epoch 144/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.3029 - accuracy: 0.9255 - val_loss: 0.3691 - val_accuracy: 0.8950\n",
            "Epoch 145/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2910 - accuracy: 0.9245 - val_loss: 0.3679 - val_accuracy: 0.8950\n",
            "Epoch 146/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2966 - accuracy: 0.9225 - val_loss: 0.3667 - val_accuracy: 0.8950\n",
            "Epoch 147/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2818 - accuracy: 0.9350 - val_loss: 0.3652 - val_accuracy: 0.8950\n",
            "Epoch 148/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2913 - accuracy: 0.9285 - val_loss: 0.3661 - val_accuracy: 0.8950\n",
            "Epoch 149/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2927 - accuracy: 0.9225 - val_loss: 0.3656 - val_accuracy: 0.8950\n",
            "Epoch 150/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2892 - accuracy: 0.9280 - val_loss: 0.3634 - val_accuracy: 0.8950\n",
            "Epoch 151/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2995 - accuracy: 0.9230 - val_loss: 0.3621 - val_accuracy: 0.8950\n",
            "Epoch 152/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2862 - accuracy: 0.9290 - val_loss: 0.3601 - val_accuracy: 0.9050\n",
            "Epoch 153/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2693 - accuracy: 0.9420 - val_loss: 0.3588 - val_accuracy: 0.9050\n",
            "Epoch 154/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2959 - accuracy: 0.9180 - val_loss: 0.3575 - val_accuracy: 0.9050\n",
            "Epoch 155/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.2806 - accuracy: 0.9280 - val_loss: 0.3554 - val_accuracy: 0.9050\n",
            "Epoch 156/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2758 - accuracy: 0.9295 - val_loss: 0.3546 - val_accuracy: 0.9050\n",
            "Epoch 157/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2675 - accuracy: 0.9370 - val_loss: 0.3530 - val_accuracy: 0.9050\n",
            "Epoch 158/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2920 - accuracy: 0.9225 - val_loss: 0.3528 - val_accuracy: 0.9050\n",
            "Epoch 159/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2690 - accuracy: 0.9330 - val_loss: 0.3521 - val_accuracy: 0.9050\n",
            "Epoch 160/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2616 - accuracy: 0.9365 - val_loss: 0.3511 - val_accuracy: 0.9050\n",
            "Epoch 161/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2682 - accuracy: 0.9295 - val_loss: 0.3494 - val_accuracy: 0.9050\n",
            "Epoch 162/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2730 - accuracy: 0.9310 - val_loss: 0.3481 - val_accuracy: 0.9050\n",
            "Epoch 163/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2543 - accuracy: 0.9425 - val_loss: 0.3466 - val_accuracy: 0.9150\n",
            "Epoch 164/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2567 - accuracy: 0.9410 - val_loss: 0.3462 - val_accuracy: 0.9100\n",
            "Epoch 165/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2565 - accuracy: 0.9315 - val_loss: 0.3460 - val_accuracy: 0.9100\n",
            "Epoch 166/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2662 - accuracy: 0.9285 - val_loss: 0.3458 - val_accuracy: 0.9100\n",
            "Epoch 167/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2623 - accuracy: 0.9355 - val_loss: 0.3444 - val_accuracy: 0.9100\n",
            "Epoch 168/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2554 - accuracy: 0.9360 - val_loss: 0.3429 - val_accuracy: 0.9100\n",
            "Epoch 169/200\n",
            "72/72 [==============================] - 4s 54ms/step - loss: 0.2576 - accuracy: 0.9375 - val_loss: 0.3416 - val_accuracy: 0.9150\n",
            "Epoch 170/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2649 - accuracy: 0.9305 - val_loss: 0.3397 - val_accuracy: 0.9150\n",
            "Epoch 171/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2586 - accuracy: 0.9395 - val_loss: 0.3401 - val_accuracy: 0.9150\n",
            "Epoch 172/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2429 - accuracy: 0.9375 - val_loss: 0.3388 - val_accuracy: 0.9150\n",
            "Epoch 173/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2502 - accuracy: 0.9365 - val_loss: 0.3384 - val_accuracy: 0.9150\n",
            "Epoch 174/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2502 - accuracy: 0.9345 - val_loss: 0.3386 - val_accuracy: 0.9150\n",
            "Epoch 175/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2556 - accuracy: 0.9385 - val_loss: 0.3367 - val_accuracy: 0.9150\n",
            "Epoch 176/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2433 - accuracy: 0.9415 - val_loss: 0.3351 - val_accuracy: 0.9150\n",
            "Epoch 177/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2474 - accuracy: 0.9370 - val_loss: 0.3339 - val_accuracy: 0.9200\n",
            "Epoch 178/200\n",
            "72/72 [==============================] - 4s 54ms/step - loss: 0.2523 - accuracy: 0.9370 - val_loss: 0.3326 - val_accuracy: 0.9200\n",
            "Epoch 179/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2376 - accuracy: 0.9400 - val_loss: 0.3322 - val_accuracy: 0.9200\n",
            "Epoch 180/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2354 - accuracy: 0.9505 - val_loss: 0.3316 - val_accuracy: 0.9200\n",
            "Epoch 181/200\n",
            "72/72 [==============================] - 4s 52ms/step - loss: 0.2362 - accuracy: 0.9410 - val_loss: 0.3308 - val_accuracy: 0.9150\n",
            "Epoch 182/200\n",
            "72/72 [==============================] - 4s 53ms/step - loss: 0.2405 - accuracy: 0.9415 - val_loss: 0.3302 - val_accuracy: 0.9200\n",
            "Epoch 183/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2365 - accuracy: 0.9430 - val_loss: 0.3293 - val_accuracy: 0.9200\n",
            "Epoch 184/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.2348 - accuracy: 0.9450 - val_loss: 0.3289 - val_accuracy: 0.9200\n",
            "Epoch 185/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2237 - accuracy: 0.9530 - val_loss: 0.3288 - val_accuracy: 0.9200\n",
            "Epoch 186/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2324 - accuracy: 0.9440 - val_loss: 0.3286 - val_accuracy: 0.9200\n",
            "Epoch 187/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2259 - accuracy: 0.9465 - val_loss: 0.3274 - val_accuracy: 0.9200\n",
            "Epoch 188/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2304 - accuracy: 0.9385 - val_loss: 0.3255 - val_accuracy: 0.9200\n",
            "Epoch 189/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2307 - accuracy: 0.9470 - val_loss: 0.3261 - val_accuracy: 0.9200\n",
            "Epoch 190/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2293 - accuracy: 0.9445 - val_loss: 0.3259 - val_accuracy: 0.9200\n",
            "Epoch 191/200\n",
            "72/72 [==============================] - 3s 47ms/step - loss: 0.2259 - accuracy: 0.9425 - val_loss: 0.3255 - val_accuracy: 0.9200\n",
            "Epoch 192/200\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 0.2304 - accuracy: 0.9435 - val_loss: 0.3238 - val_accuracy: 0.9200\n",
            "Epoch 193/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2244 - accuracy: 0.9470 - val_loss: 0.3230 - val_accuracy: 0.9200\n",
            "Epoch 194/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2133 - accuracy: 0.9470 - val_loss: 0.3223 - val_accuracy: 0.9200\n",
            "Epoch 195/200\n",
            "72/72 [==============================] - 4s 49ms/step - loss: 0.2174 - accuracy: 0.9435 - val_loss: 0.3216 - val_accuracy: 0.9200\n",
            "Epoch 196/200\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.2285 - accuracy: 0.9420 - val_loss: 0.3215 - val_accuracy: 0.9200\n",
            "Epoch 197/200\n",
            "72/72 [==============================] - 3s 48ms/step - loss: 0.2113 - accuracy: 0.9570 - val_loss: 0.3210 - val_accuracy: 0.9200\n",
            "Epoch 198/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2116 - accuracy: 0.9500 - val_loss: 0.3200 - val_accuracy: 0.9200\n",
            "Epoch 199/200\n",
            "72/72 [==============================] - 4s 54ms/step - loss: 0.2197 - accuracy: 0.9430 - val_loss: 0.3190 - val_accuracy: 0.9200\n",
            "Epoch 200/200\n",
            "72/72 [==============================] - 4s 51ms/step - loss: 0.2125 - accuracy: 0.9515 - val_loss: 0.3185 - val_accuracy: 0.9200\n",
            "Real Final model Test loss: 0.3146114647388458\n",
            "Real Final model Test accuracy: 0.9093000292778015\n"
          ]
        }
      ]
    }
  ]
}