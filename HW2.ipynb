{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woonghee97/deep_learning/blob/DeepLearning/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OA0f9oxYGYb"
      },
      "source": [
        "# HW#6 Regularization\n",
        "\n",
        "안녕하세요, 광운대학교 로봇학부의 오정현 교수입니다. 본 자료는 딥러닝 실습 수업을 위해 제작된 것입니다.\n",
        "\n",
        "파이썬 문법\n",
        "- 점프투파이썬(https://wikidocs.net/book/1) 참고\n",
        "\n",
        "이번 과제는 딥러닝의 일반화 성능을 높이기 위한 Regularization을 해보는 것입니다.이미지 분류에 여러 가지 Regularization 기법을 적용해 보도록 하겠습니다. 대표적인 Regularization 기법으로 Dropout, Data augmentation, Batch Normalization 등이 있습니다.\n",
        "\n",
        "이번 과제는 (https://www.tensorflow.org/tutorials/keras/classification?hl=ko)를 참고하면 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9KBzwzd7Bub"
      },
      "source": [
        "#1. Data Generation\n",
        "Data는 mnist dataset을 이용하도록 하겠습니다. mnist dataset은 원래 60000개의 training set이 주어져 있지만 overfitting을 유도하기 위하여 1000개의 data만 이용하려고 합니다. 1000개의 data로 이루어진 x_train과 y_train을 만들어 보세요. 그리고 2000개로 이루어진 large_x_train, large_y_train을 만들어보세요. 그리고 training data의 다른 범위에서 200개로 이루어진 x_validation과 y_validation도 만들어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3K3NqD_wTyh",
        "outputId": "9e173b22-c62f-4e54-e727-ddd4f3850204"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot\n",
        "\n",
        "batch_size = 28\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, Y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "### START CODE HERE ###\n",
        "x_validation = X_train\n",
        "y_validation =\n",
        "large_x_train =\n",
        "large_y_train = \n",
        "x_train = \n",
        "y_train =\n",
        "### END CODE HERE ###\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "large_x_train = large_x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "large_x_train /= 255\n",
        "x_validation /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "large_y_train = keras.utils.to_categorical(large_y_train, num_classes)\n",
        "y_validation = keras.utils.to_categorical(y_validation, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "assert large_x_train.shape[0]==2000\n",
        "assert large_y_train.shape[0]==2000\n",
        "assert x_train.shape[0]==1000\n",
        "assert y_train.shape[0]==1000\n",
        "assert x_validation.shape[0]==200\n",
        "assert y_validation.shape[0]==200\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"large_x_train shape:\", large_x_train.shape)\n",
        "print(\"large_y_train shape:\", large_y_train.shape)\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_validation shape: (200, 28, 28)\n",
            "y_validation shape: (200,)\n",
            "x_train shape: (1000, 28, 28, 1)\n",
            "y_train shape: (1000, 10)\n",
            "large_x_train shape: (2000, 28, 28, 1)\n",
            "large_y_train shape: (2000, 10)\n",
            "x_validation shape: (200, 28, 28, 1)\n",
            "y_validation shape: (200, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlNTST2K_IRE"
      },
      "source": [
        "#2. 모델 생성\n",
        "복습 차원에서 MLP 분류모델을 만들어 보도록 하겠습니다. 모델의 마지막 레이어에는 활성화 함수로 10개의 출력을 가지는 softmax를 달겠습니다. 이를 통해서 모델은 이미지안의 숫자가 0부터 9까지의 숫자중에 어디에 가까운지를 확률적으로 나타냅니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNI8KWo8PyC"
      },
      "source": [
        "다음과 같은 MLP 모델을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560L-utPx8z1",
        "outputId": "f410ea64-1dd6-4a05-df0c-23ee66cd0fa0"
      },
      "source": [
        "model = Sequential()\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhIfmZeBZm8"
      },
      "source": [
        "#3. Learning MLP\n",
        "기본 MLP 분류모델을 학습해 보겠습니다. Overfitting은 Training data에 맞추어 과도하게 학습이 이루어져 Test data에서 높은 성능이 나지 않는 현상, 즉 Generalization 성능이 높지 않게 나타나는 현상을 의미합니다. 따라서 Overfitting이 발생하면 Training accuracy는 높지만 Test accuracy는 높지 않게 나타납니다. \n",
        "\n",
        "현재 모델이 overfitting이 발생하는지 체크해 보세요. 일부러 overfitting이 발생하도록 유도하였기 때문에 overfitting 현상이 나타나야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1VoI2bCyBy-",
        "outputId": "64273e28-87f4-4040-b153-a3cc76ec1203"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "weights = model.get_weights()\n",
        "\n",
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 2s 48ms/step - loss: 2.3006 - accuracy: 0.1181 - val_loss: 2.2940 - val_accuracy: 0.1050\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.2914 - accuracy: 0.1191 - val_loss: 2.2843 - val_accuracy: 0.1150\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2749 - accuracy: 0.1417 - val_loss: 2.2748 - val_accuracy: 0.1200\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2659 - accuracy: 0.1397 - val_loss: 2.2653 - val_accuracy: 0.1300\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2594 - accuracy: 0.1440 - val_loss: 2.2558 - val_accuracy: 0.1300\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2498 - accuracy: 0.1482 - val_loss: 2.2463 - val_accuracy: 0.1450\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2413 - accuracy: 0.1581 - val_loss: 2.2368 - val_accuracy: 0.1600\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 2.2258 - accuracy: 0.1926 - val_loss: 2.2275 - val_accuracy: 0.1850\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 2.2166 - accuracy: 0.2369 - val_loss: 2.2180 - val_accuracy: 0.2450\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.2084 - accuracy: 0.2507 - val_loss: 2.2087 - val_accuracy: 0.2800\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 2.1924 - accuracy: 0.2946 - val_loss: 2.1993 - val_accuracy: 0.3050\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 2.1923 - accuracy: 0.3183 - val_loss: 2.1898 - val_accuracy: 0.3350\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1822 - accuracy: 0.3534 - val_loss: 2.1805 - val_accuracy: 0.3700\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1705 - accuracy: 0.3807 - val_loss: 2.1710 - val_accuracy: 0.3950\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1591 - accuracy: 0.3861 - val_loss: 2.1616 - val_accuracy: 0.4300\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1525 - accuracy: 0.4155 - val_loss: 2.1521 - val_accuracy: 0.4750\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1392 - accuracy: 0.4598 - val_loss: 2.1425 - val_accuracy: 0.4950\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1297 - accuracy: 0.4890 - val_loss: 2.1329 - val_accuracy: 0.5050\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.1208 - accuracy: 0.5006 - val_loss: 2.1232 - val_accuracy: 0.5350\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.1150 - accuracy: 0.4814 - val_loss: 2.1133 - val_accuracy: 0.5600\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1018 - accuracy: 0.5194 - val_loss: 2.1034 - val_accuracy: 0.5600\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0944 - accuracy: 0.5305 - val_loss: 2.0934 - val_accuracy: 0.5700\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0723 - accuracy: 0.5815 - val_loss: 2.0832 - val_accuracy: 0.5700\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.0693 - accuracy: 0.5705 - val_loss: 2.0730 - val_accuracy: 0.5700\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0562 - accuracy: 0.6025 - val_loss: 2.0628 - val_accuracy: 0.5750\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0466 - accuracy: 0.6162 - val_loss: 2.0525 - val_accuracy: 0.5850\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0462 - accuracy: 0.6130 - val_loss: 2.0421 - val_accuracy: 0.6000\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.0387 - accuracy: 0.6253 - val_loss: 2.0317 - val_accuracy: 0.6150\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.0113 - accuracy: 0.6435 - val_loss: 2.0213 - val_accuracy: 0.6150\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 2.0045 - accuracy: 0.6536 - val_loss: 2.0108 - val_accuracy: 0.6200\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9886 - accuracy: 0.6886 - val_loss: 2.0002 - val_accuracy: 0.6200\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9810 - accuracy: 0.6842 - val_loss: 1.9895 - val_accuracy: 0.6400\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.9792 - accuracy: 0.6754 - val_loss: 1.9787 - val_accuracy: 0.6450\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9639 - accuracy: 0.6972 - val_loss: 1.9679 - val_accuracy: 0.6550\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9523 - accuracy: 0.6701 - val_loss: 1.9570 - val_accuracy: 0.6700\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9502 - accuracy: 0.6847 - val_loss: 1.9460 - val_accuracy: 0.6750\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9292 - accuracy: 0.7078 - val_loss: 1.9350 - val_accuracy: 0.6800\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.9205 - accuracy: 0.6836 - val_loss: 1.9238 - val_accuracy: 0.6800\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.8972 - accuracy: 0.7319 - val_loss: 1.9125 - val_accuracy: 0.6800\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8854 - accuracy: 0.7229 - val_loss: 1.9011 - val_accuracy: 0.6850\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8903 - accuracy: 0.7198 - val_loss: 1.8898 - val_accuracy: 0.6900\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8791 - accuracy: 0.7172 - val_loss: 1.8783 - val_accuracy: 0.6950\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.8560 - accuracy: 0.7225 - val_loss: 1.8666 - val_accuracy: 0.6950\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8473 - accuracy: 0.7435 - val_loss: 1.8549 - val_accuracy: 0.6950\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.8199 - accuracy: 0.7524 - val_loss: 1.8431 - val_accuracy: 0.7050\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8295 - accuracy: 0.7388 - val_loss: 1.8313 - val_accuracy: 0.7050\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.8005 - accuracy: 0.7672 - val_loss: 1.8195 - val_accuracy: 0.7050\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.8001 - accuracy: 0.7260 - val_loss: 1.8075 - val_accuracy: 0.7050\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7891 - accuracy: 0.7425 - val_loss: 1.7955 - val_accuracy: 0.7100\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7685 - accuracy: 0.7508 - val_loss: 1.7834 - val_accuracy: 0.7100\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7624 - accuracy: 0.7657 - val_loss: 1.7712 - val_accuracy: 0.7200\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7427 - accuracy: 0.7721 - val_loss: 1.7591 - val_accuracy: 0.7200\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.7390 - accuracy: 0.7628 - val_loss: 1.7468 - val_accuracy: 0.7200\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7258 - accuracy: 0.7787 - val_loss: 1.7344 - val_accuracy: 0.7200\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.7130 - accuracy: 0.7788 - val_loss: 1.7220 - val_accuracy: 0.7250\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6863 - accuracy: 0.7917 - val_loss: 1.7096 - val_accuracy: 0.7250\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6880 - accuracy: 0.7661 - val_loss: 1.6970 - val_accuracy: 0.7250\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.6612 - accuracy: 0.7745 - val_loss: 1.6844 - val_accuracy: 0.7250\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6545 - accuracy: 0.7886 - val_loss: 1.6716 - val_accuracy: 0.7350\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.6435 - accuracy: 0.7871 - val_loss: 1.6589 - val_accuracy: 0.7350\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6376 - accuracy: 0.7969 - val_loss: 1.6463 - val_accuracy: 0.7400\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.6315 - accuracy: 0.7856 - val_loss: 1.6335 - val_accuracy: 0.7400\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.6182 - accuracy: 0.7868 - val_loss: 1.6207 - val_accuracy: 0.7450\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5903 - accuracy: 0.8122 - val_loss: 1.6078 - val_accuracy: 0.7500\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5790 - accuracy: 0.7885 - val_loss: 1.5950 - val_accuracy: 0.7550\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5665 - accuracy: 0.7772 - val_loss: 1.5822 - val_accuracy: 0.7550\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5460 - accuracy: 0.8131 - val_loss: 1.5694 - val_accuracy: 0.7550\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5277 - accuracy: 0.8061 - val_loss: 1.5565 - val_accuracy: 0.7550\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5398 - accuracy: 0.7886 - val_loss: 1.5437 - val_accuracy: 0.7600\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5120 - accuracy: 0.8128 - val_loss: 1.5309 - val_accuracy: 0.7550\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.5156 - accuracy: 0.7882 - val_loss: 1.5181 - val_accuracy: 0.7600\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4864 - accuracy: 0.7939 - val_loss: 1.5053 - val_accuracy: 0.7650\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.4509 - accuracy: 0.8038 - val_loss: 1.4925 - val_accuracy: 0.7700\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4550 - accuracy: 0.8035 - val_loss: 1.4797 - val_accuracy: 0.7700\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4392 - accuracy: 0.8147 - val_loss: 1.4670 - val_accuracy: 0.7700\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4620 - accuracy: 0.7921 - val_loss: 1.4543 - val_accuracy: 0.7700\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4375 - accuracy: 0.7739 - val_loss: 1.4416 - val_accuracy: 0.7650\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.4159 - accuracy: 0.7957 - val_loss: 1.4289 - val_accuracy: 0.7650\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.4020 - accuracy: 0.8068 - val_loss: 1.4164 - val_accuracy: 0.7700\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3837 - accuracy: 0.8038 - val_loss: 1.4039 - val_accuracy: 0.7700\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3705 - accuracy: 0.8157 - val_loss: 1.3915 - val_accuracy: 0.7650\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3735 - accuracy: 0.8009 - val_loss: 1.3791 - val_accuracy: 0.7750\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.3484 - accuracy: 0.8155 - val_loss: 1.3668 - val_accuracy: 0.7800\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3481 - accuracy: 0.8130 - val_loss: 1.3546 - val_accuracy: 0.7900\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 1.3386 - accuracy: 0.8113 - val_loss: 1.3424 - val_accuracy: 0.7850\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2930 - accuracy: 0.8109 - val_loss: 1.3304 - val_accuracy: 0.7850\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3002 - accuracy: 0.8180 - val_loss: 1.3184 - val_accuracy: 0.7850\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2966 - accuracy: 0.8134 - val_loss: 1.3065 - val_accuracy: 0.7850\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2699 - accuracy: 0.8287 - val_loss: 1.2947 - val_accuracy: 0.7850\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2535 - accuracy: 0.8340 - val_loss: 1.2830 - val_accuracy: 0.7900\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2731 - accuracy: 0.8205 - val_loss: 1.2714 - val_accuracy: 0.7950\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2670 - accuracy: 0.8144 - val_loss: 1.2598 - val_accuracy: 0.7950\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2262 - accuracy: 0.8184 - val_loss: 1.2485 - val_accuracy: 0.7950\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2237 - accuracy: 0.8281 - val_loss: 1.2373 - val_accuracy: 0.7950\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 1s 39ms/step - loss: 1.1940 - accuracy: 0.8321 - val_loss: 1.2262 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.2041 - accuracy: 0.8211 - val_loss: 1.2151 - val_accuracy: 0.8000\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2085 - accuracy: 0.8160 - val_loss: 1.2040 - val_accuracy: 0.8000\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.1631 - accuracy: 0.8269 - val_loss: 1.1931 - val_accuracy: 0.8000\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.1496 - accuracy: 0.8262 - val_loss: 1.1824 - val_accuracy: 0.8000\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.1592 - accuracy: 0.8243 - val_loss: 1.1718 - val_accuracy: 0.8000\n",
            "Test loss: 1.2494491338729858\n",
            "Test accuracy: 0.7807999849319458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jebNKzFBJGE"
      },
      "source": [
        "#4. Regularization\n",
        "Overfitting이 발생한 모델에 다양한 Regularization 기법을 이용해 보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxZKsjCFvhT"
      },
      "source": [
        "4.1 Large Dataset\n",
        "\n",
        "Training data가 충분하다면 overfitting 현상이 발생할 가능성이 줄어듭니다. 기본 MLP 분류 모델에서 large_x_train과 large_y_train을 이용하면 성능이 올라갈 것입니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5b1fp96FuEy",
        "outputId": "07cd3440-71d8-42fa-88f6-a5a091ac0fdd"
      },
      "source": [
        "#initialize weights\n",
        "model.set_weights(weights)\n",
        "\n",
        "large_model_history=model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Largemodel Test loss:', score[0])\n",
        "print('Largemodel Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.2890 - accuracy: 0.1280 - val_loss: 2.2843 - val_accuracy: 0.1150\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.2706 - accuracy: 0.1445 - val_loss: 2.2652 - val_accuracy: 0.1200\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.2524 - accuracy: 0.1615 - val_loss: 2.2462 - val_accuracy: 0.1400\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.2342 - accuracy: 0.1940 - val_loss: 2.2272 - val_accuracy: 0.2000\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.2161 - accuracy: 0.2335 - val_loss: 2.2084 - val_accuracy: 0.2800\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1979 - accuracy: 0.2935 - val_loss: 2.1895 - val_accuracy: 0.3450\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 2.1796 - accuracy: 0.3460 - val_loss: 2.1706 - val_accuracy: 0.4100\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1613 - accuracy: 0.3990 - val_loss: 2.1515 - val_accuracy: 0.4500\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1430 - accuracy: 0.4435 - val_loss: 2.1323 - val_accuracy: 0.5050\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1244 - accuracy: 0.4880 - val_loss: 2.1129 - val_accuracy: 0.5600\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1056 - accuracy: 0.5280 - val_loss: 2.0932 - val_accuracy: 0.5700\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0866 - accuracy: 0.5575 - val_loss: 2.0733 - val_accuracy: 0.5850\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0673 - accuracy: 0.5905 - val_loss: 2.0531 - val_accuracy: 0.5900\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.0478 - accuracy: 0.6215 - val_loss: 2.0327 - val_accuracy: 0.5950\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0279 - accuracy: 0.6400 - val_loss: 2.0120 - val_accuracy: 0.6200\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0079 - accuracy: 0.6640 - val_loss: 1.9910 - val_accuracy: 0.6300\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9875 - accuracy: 0.6760 - val_loss: 1.9696 - val_accuracy: 0.6550\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9668 - accuracy: 0.6920 - val_loss: 1.9481 - val_accuracy: 0.6650\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9458 - accuracy: 0.6975 - val_loss: 1.9261 - val_accuracy: 0.6750\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9245 - accuracy: 0.7090 - val_loss: 1.9037 - val_accuracy: 0.6800\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9027 - accuracy: 0.7200 - val_loss: 1.8808 - val_accuracy: 0.6950\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8806 - accuracy: 0.7275 - val_loss: 1.8578 - val_accuracy: 0.7000\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8581 - accuracy: 0.7330 - val_loss: 1.8344 - val_accuracy: 0.7000\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8354 - accuracy: 0.7405 - val_loss: 1.8108 - val_accuracy: 0.7000\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8125 - accuracy: 0.7450 - val_loss: 1.7869 - val_accuracy: 0.7100\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7894 - accuracy: 0.7500 - val_loss: 1.7627 - val_accuracy: 0.7100\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7660 - accuracy: 0.7540 - val_loss: 1.7385 - val_accuracy: 0.7150\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7424 - accuracy: 0.7560 - val_loss: 1.7139 - val_accuracy: 0.7150\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.7184 - accuracy: 0.7605 - val_loss: 1.6890 - val_accuracy: 0.7200\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6941 - accuracy: 0.7650 - val_loss: 1.6637 - val_accuracy: 0.7300\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.6697 - accuracy: 0.7670 - val_loss: 1.6385 - val_accuracy: 0.7400\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6451 - accuracy: 0.7700 - val_loss: 1.6129 - val_accuracy: 0.7450\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6205 - accuracy: 0.7715 - val_loss: 1.5873 - val_accuracy: 0.7450\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 1.5959 - accuracy: 0.7765 - val_loss: 1.5620 - val_accuracy: 0.7450\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.5711 - accuracy: 0.7800 - val_loss: 1.5366 - val_accuracy: 0.7500\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.5464 - accuracy: 0.7825 - val_loss: 1.5112 - val_accuracy: 0.7550\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.5217 - accuracy: 0.7855 - val_loss: 1.4858 - val_accuracy: 0.7550\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4969 - accuracy: 0.7895 - val_loss: 1.4603 - val_accuracy: 0.7550\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4722 - accuracy: 0.7930 - val_loss: 1.4350 - val_accuracy: 0.7600\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4477 - accuracy: 0.7970 - val_loss: 1.4101 - val_accuracy: 0.7600\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4234 - accuracy: 0.7980 - val_loss: 1.3853 - val_accuracy: 0.7700\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3992 - accuracy: 0.8000 - val_loss: 1.3609 - val_accuracy: 0.7850\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3754 - accuracy: 0.7990 - val_loss: 1.3369 - val_accuracy: 0.7800\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3518 - accuracy: 0.8020 - val_loss: 1.3132 - val_accuracy: 0.7750\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3286 - accuracy: 0.8030 - val_loss: 1.2897 - val_accuracy: 0.7800\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3057 - accuracy: 0.8050 - val_loss: 1.2665 - val_accuracy: 0.7850\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.2831 - accuracy: 0.8070 - val_loss: 1.2439 - val_accuracy: 0.7850\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2608 - accuracy: 0.8075 - val_loss: 1.2212 - val_accuracy: 0.7850\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2389 - accuracy: 0.8090 - val_loss: 1.1994 - val_accuracy: 0.7900\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.2172 - accuracy: 0.8100 - val_loss: 1.1777 - val_accuracy: 0.7900\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1961 - accuracy: 0.8130 - val_loss: 1.1567 - val_accuracy: 0.7900\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.1754 - accuracy: 0.8155 - val_loss: 1.1361 - val_accuracy: 0.7900\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 1.1550 - accuracy: 0.8150 - val_loss: 1.1157 - val_accuracy: 0.7900\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1351 - accuracy: 0.8210 - val_loss: 1.0964 - val_accuracy: 0.7950\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.1156 - accuracy: 0.8245 - val_loss: 1.0773 - val_accuracy: 0.8050\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0965 - accuracy: 0.8245 - val_loss: 1.0586 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0778 - accuracy: 0.8275 - val_loss: 1.0409 - val_accuracy: 0.8000\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.0596 - accuracy: 0.8300 - val_loss: 1.0231 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0419 - accuracy: 0.8295 - val_loss: 1.0060 - val_accuracy: 0.8000\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.0246 - accuracy: 0.8335 - val_loss: 0.9890 - val_accuracy: 0.8000\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0077 - accuracy: 0.8355 - val_loss: 0.9727 - val_accuracy: 0.8000\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9913 - accuracy: 0.8345 - val_loss: 0.9570 - val_accuracy: 0.8000\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9754 - accuracy: 0.8375 - val_loss: 0.9422 - val_accuracy: 0.8050\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9599 - accuracy: 0.8375 - val_loss: 0.9276 - val_accuracy: 0.8100\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9449 - accuracy: 0.8365 - val_loss: 0.9129 - val_accuracy: 0.8100\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9304 - accuracy: 0.8390 - val_loss: 0.8989 - val_accuracy: 0.8100\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9162 - accuracy: 0.8415 - val_loss: 0.8854 - val_accuracy: 0.8100\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.9025 - accuracy: 0.8420 - val_loss: 0.8728 - val_accuracy: 0.8100\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8891 - accuracy: 0.8440 - val_loss: 0.8605 - val_accuracy: 0.8100\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8760 - accuracy: 0.8475 - val_loss: 0.8488 - val_accuracy: 0.8100\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8633 - accuracy: 0.8475 - val_loss: 0.8369 - val_accuracy: 0.8100\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8509 - accuracy: 0.8505 - val_loss: 0.8255 - val_accuracy: 0.8150\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8390 - accuracy: 0.8520 - val_loss: 0.8145 - val_accuracy: 0.8150\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8273 - accuracy: 0.8510 - val_loss: 0.8040 - val_accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8160 - accuracy: 0.8525 - val_loss: 0.7936 - val_accuracy: 0.8250\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8048 - accuracy: 0.8515 - val_loss: 0.7834 - val_accuracy: 0.8250\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7941 - accuracy: 0.8530 - val_loss: 0.7739 - val_accuracy: 0.8250\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7835 - accuracy: 0.8540 - val_loss: 0.7642 - val_accuracy: 0.8250\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7733 - accuracy: 0.8565 - val_loss: 0.7551 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7635 - accuracy: 0.8565 - val_loss: 0.7457 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7539 - accuracy: 0.8570 - val_loss: 0.7375 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7445 - accuracy: 0.8560 - val_loss: 0.7290 - val_accuracy: 0.8300\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7354 - accuracy: 0.8590 - val_loss: 0.7211 - val_accuracy: 0.8300\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7266 - accuracy: 0.8605 - val_loss: 0.7134 - val_accuracy: 0.8300\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7180 - accuracy: 0.8620 - val_loss: 0.7057 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7097 - accuracy: 0.8620 - val_loss: 0.6980 - val_accuracy: 0.8350\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.7015 - accuracy: 0.8640 - val_loss: 0.6905 - val_accuracy: 0.8350\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6935 - accuracy: 0.8655 - val_loss: 0.6838 - val_accuracy: 0.8350\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6858 - accuracy: 0.8635 - val_loss: 0.6765 - val_accuracy: 0.8350\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6783 - accuracy: 0.8665 - val_loss: 0.6702 - val_accuracy: 0.8350\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6708 - accuracy: 0.8670 - val_loss: 0.6640 - val_accuracy: 0.8400\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6636 - accuracy: 0.8680 - val_loss: 0.6579 - val_accuracy: 0.8450\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6567 - accuracy: 0.8695 - val_loss: 0.6517 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6498 - accuracy: 0.8690 - val_loss: 0.6456 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6431 - accuracy: 0.8695 - val_loss: 0.6402 - val_accuracy: 0.8450\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6366 - accuracy: 0.8710 - val_loss: 0.6344 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6303 - accuracy: 0.8720 - val_loss: 0.6290 - val_accuracy: 0.8500\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.6241 - accuracy: 0.8710 - val_loss: 0.6239 - val_accuracy: 0.8500\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6181 - accuracy: 0.8720 - val_loss: 0.6188 - val_accuracy: 0.8500\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6122 - accuracy: 0.8715 - val_loss: 0.6137 - val_accuracy: 0.8500\n",
            "Largemodel Test loss: 0.6658527851104736\n",
            "Largemodel Test accuracy: 0.8501999974250793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j_-85YFDA6y"
      },
      "source": [
        "##4.2 Dropout\n",
        "Dropout은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 Dropout layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 Dropout model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYHpRcVDX22",
        "outputId": "061f7245-c860-4787-e606-dab82749f78d"
      },
      "source": [
        "dropout_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "dropout_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuxjOGWEKo9"
      },
      "source": [
        "Dropout Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?\n",
        "\n",
        "※Dropout을 적용한 Model은 일정확률로 신경망의 뉴런을 비활성화시키기 때문에, 오버피팅을 방지하는 효과가 있습니다. 하지만 비활성화로 인해서  학습속도가 떨어진다는 단점이 존재합니다. 그 때문에 test accuracy는 같은 학습파라미터 조건에서 기존모델보다 더 낮을 수 있습니다. \n",
        "\n",
        "기존모델과 dropout을 적용한 모델에 대해서, train 데이터와 test데이터에 대한 accuracy차이를 주목해보시면 좋을 것같습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqMK-jsyED_h",
        "outputId": "e8387ae3-d4cb-4979-cf5a-352a88babb01"
      },
      "source": [
        "dropout_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dropout_model_history=dropout_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = dropout_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Dropout model Test loss:', score[0])\n",
        "print('Dropout model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3049 - accuracy: 0.1127 - val_loss: 2.2889 - val_accuracy: 0.1350\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.2835 - accuracy: 0.1429 - val_loss: 2.2687 - val_accuracy: 0.1550\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.2704 - accuracy: 0.1570 - val_loss: 2.2492 - val_accuracy: 0.1850\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.2513 - accuracy: 0.2017 - val_loss: 2.2300 - val_accuracy: 0.2550\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.2371 - accuracy: 0.2170 - val_loss: 2.2112 - val_accuracy: 0.3100\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.2111 - accuracy: 0.2850 - val_loss: 2.1926 - val_accuracy: 0.3550\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1971 - accuracy: 0.3223 - val_loss: 2.1742 - val_accuracy: 0.4000\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1814 - accuracy: 0.3403 - val_loss: 2.1560 - val_accuracy: 0.4650\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1577 - accuracy: 0.3858 - val_loss: 2.1378 - val_accuracy: 0.5100\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1470 - accuracy: 0.4174 - val_loss: 2.1193 - val_accuracy: 0.5300\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1355 - accuracy: 0.4158 - val_loss: 2.1010 - val_accuracy: 0.5750\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.1039 - accuracy: 0.4781 - val_loss: 2.0825 - val_accuracy: 0.5850\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.1008 - accuracy: 0.4837 - val_loss: 2.0640 - val_accuracy: 0.6000\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0729 - accuracy: 0.5236 - val_loss: 2.0451 - val_accuracy: 0.6100\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0563 - accuracy: 0.5452 - val_loss: 2.0262 - val_accuracy: 0.6350\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0443 - accuracy: 0.5498 - val_loss: 2.0072 - val_accuracy: 0.6450\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 2.0272 - accuracy: 0.5707 - val_loss: 1.9878 - val_accuracy: 0.6650\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9983 - accuracy: 0.5912 - val_loss: 1.9683 - val_accuracy: 0.6750\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9782 - accuracy: 0.6032 - val_loss: 1.9485 - val_accuracy: 0.6800\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9743 - accuracy: 0.5996 - val_loss: 1.9284 - val_accuracy: 0.6900\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9510 - accuracy: 0.6067 - val_loss: 1.9082 - val_accuracy: 0.7000\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9289 - accuracy: 0.6175 - val_loss: 1.8875 - val_accuracy: 0.7050\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.9095 - accuracy: 0.6369 - val_loss: 1.8667 - val_accuracy: 0.7100\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8939 - accuracy: 0.6658 - val_loss: 1.8456 - val_accuracy: 0.7100\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.8549 - accuracy: 0.6799 - val_loss: 1.8245 - val_accuracy: 0.7150\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8439 - accuracy: 0.6538 - val_loss: 1.8031 - val_accuracy: 0.7350\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8194 - accuracy: 0.6751 - val_loss: 1.7814 - val_accuracy: 0.7350\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8133 - accuracy: 0.6726 - val_loss: 1.7597 - val_accuracy: 0.7350\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7832 - accuracy: 0.7016 - val_loss: 1.7378 - val_accuracy: 0.7400\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7618 - accuracy: 0.7059 - val_loss: 1.7154 - val_accuracy: 0.7500\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7296 - accuracy: 0.7090 - val_loss: 1.6932 - val_accuracy: 0.7500\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7202 - accuracy: 0.7009 - val_loss: 1.6705 - val_accuracy: 0.7550\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7049 - accuracy: 0.7142 - val_loss: 1.6480 - val_accuracy: 0.7500\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6747 - accuracy: 0.7162 - val_loss: 1.6253 - val_accuracy: 0.7550\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6662 - accuracy: 0.6948 - val_loss: 1.6021 - val_accuracy: 0.7550\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6278 - accuracy: 0.7200 - val_loss: 1.5790 - val_accuracy: 0.7600\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6215 - accuracy: 0.7230 - val_loss: 1.5559 - val_accuracy: 0.7600\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5890 - accuracy: 0.7444 - val_loss: 1.5327 - val_accuracy: 0.7600\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5703 - accuracy: 0.7355 - val_loss: 1.5093 - val_accuracy: 0.7600\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5480 - accuracy: 0.7403 - val_loss: 1.4861 - val_accuracy: 0.7650\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5014 - accuracy: 0.7589 - val_loss: 1.4634 - val_accuracy: 0.7700\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5047 - accuracy: 0.7534 - val_loss: 1.4404 - val_accuracy: 0.7700\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4787 - accuracy: 0.7410 - val_loss: 1.4176 - val_accuracy: 0.7700\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4556 - accuracy: 0.7551 - val_loss: 1.3951 - val_accuracy: 0.7750\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4255 - accuracy: 0.7621 - val_loss: 1.3725 - val_accuracy: 0.7750\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4274 - accuracy: 0.7335 - val_loss: 1.3500 - val_accuracy: 0.7800\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3925 - accuracy: 0.7638 - val_loss: 1.3277 - val_accuracy: 0.7800\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3747 - accuracy: 0.7685 - val_loss: 1.3058 - val_accuracy: 0.7800\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.3350 - accuracy: 0.7853 - val_loss: 1.2843 - val_accuracy: 0.7800\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.3165 - accuracy: 0.7715 - val_loss: 1.2627 - val_accuracy: 0.7850\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3180 - accuracy: 0.7673 - val_loss: 1.2416 - val_accuracy: 0.7900\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3029 - accuracy: 0.7765 - val_loss: 1.2210 - val_accuracy: 0.7950\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2699 - accuracy: 0.7879 - val_loss: 1.2006 - val_accuracy: 0.7950\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2593 - accuracy: 0.7751 - val_loss: 1.1804 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 1.2279 - accuracy: 0.7658 - val_loss: 1.1609 - val_accuracy: 0.8000\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1980 - accuracy: 0.8008 - val_loss: 1.1415 - val_accuracy: 0.8000\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1850 - accuracy: 0.7902 - val_loss: 1.1223 - val_accuracy: 0.8000\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1704 - accuracy: 0.8015 - val_loss: 1.1037 - val_accuracy: 0.8000\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1609 - accuracy: 0.8027 - val_loss: 1.0861 - val_accuracy: 0.8150\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1346 - accuracy: 0.8121 - val_loss: 1.0684 - val_accuracy: 0.8200\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1229 - accuracy: 0.7896 - val_loss: 1.0511 - val_accuracy: 0.8200\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1258 - accuracy: 0.7719 - val_loss: 1.0340 - val_accuracy: 0.8200\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0849 - accuracy: 0.8114 - val_loss: 1.0174 - val_accuracy: 0.8200\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0815 - accuracy: 0.7949 - val_loss: 1.0013 - val_accuracy: 0.8200\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0447 - accuracy: 0.8027 - val_loss: 0.9855 - val_accuracy: 0.8200\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0363 - accuracy: 0.8161 - val_loss: 0.9703 - val_accuracy: 0.8200\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0459 - accuracy: 0.8148 - val_loss: 0.9555 - val_accuracy: 0.8250\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0249 - accuracy: 0.8117 - val_loss: 0.9407 - val_accuracy: 0.8200\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9709 - accuracy: 0.8311 - val_loss: 0.9264 - val_accuracy: 0.8200\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9753 - accuracy: 0.8128 - val_loss: 0.9126 - val_accuracy: 0.8200\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9716 - accuracy: 0.8279 - val_loss: 0.8990 - val_accuracy: 0.8200\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9480 - accuracy: 0.8037 - val_loss: 0.8864 - val_accuracy: 0.8200\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9464 - accuracy: 0.8253 - val_loss: 0.8743 - val_accuracy: 0.8200\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.9456 - accuracy: 0.8122 - val_loss: 0.8623 - val_accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9256 - accuracy: 0.8154 - val_loss: 0.8506 - val_accuracy: 0.8250\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8863 - accuracy: 0.8409 - val_loss: 0.8390 - val_accuracy: 0.8250\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.8863 - accuracy: 0.8323 - val_loss: 0.8276 - val_accuracy: 0.8200\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.8862 - accuracy: 0.8266 - val_loss: 0.8169 - val_accuracy: 0.8250\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.8927 - accuracy: 0.8271 - val_loss: 0.8064 - val_accuracy: 0.8250\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8731 - accuracy: 0.8193 - val_loss: 0.7959 - val_accuracy: 0.8250\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8381 - accuracy: 0.8486 - val_loss: 0.7858 - val_accuracy: 0.8200\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8577 - accuracy: 0.8292 - val_loss: 0.7765 - val_accuracy: 0.8250\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8189 - accuracy: 0.8434 - val_loss: 0.7670 - val_accuracy: 0.8250\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8168 - accuracy: 0.8358 - val_loss: 0.7578 - val_accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8065 - accuracy: 0.8298 - val_loss: 0.7490 - val_accuracy: 0.8250\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8152 - accuracy: 0.8277 - val_loss: 0.7401 - val_accuracy: 0.8250\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7704 - accuracy: 0.8430 - val_loss: 0.7316 - val_accuracy: 0.8250\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8104 - accuracy: 0.8254 - val_loss: 0.7236 - val_accuracy: 0.8250\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7789 - accuracy: 0.8387 - val_loss: 0.7164 - val_accuracy: 0.8250\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7616 - accuracy: 0.8461 - val_loss: 0.7087 - val_accuracy: 0.8250\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7357 - accuracy: 0.8441 - val_loss: 0.7010 - val_accuracy: 0.8250\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7259 - accuracy: 0.8552 - val_loss: 0.6935 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7390 - accuracy: 0.8404 - val_loss: 0.6860 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7324 - accuracy: 0.8440 - val_loss: 0.6796 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7315 - accuracy: 0.8466 - val_loss: 0.6732 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6894 - accuracy: 0.8561 - val_loss: 0.6673 - val_accuracy: 0.8300\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6821 - accuracy: 0.8551 - val_loss: 0.6609 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6947 - accuracy: 0.8599 - val_loss: 0.6542 - val_accuracy: 0.8300\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7040 - accuracy: 0.8483 - val_loss: 0.6485 - val_accuracy: 0.8300\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6714 - accuracy: 0.8559 - val_loss: 0.6431 - val_accuracy: 0.8300\n",
            "Dropout model Test loss: 0.7159370183944702\n",
            "Dropout model Test accuracy: 0.8421000242233276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEyhMagnGq8F"
      },
      "source": [
        "##4.3 BatchNormalization\n",
        "BatchNormalization(BN)은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 BN layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 BN model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIBMssiBJaB",
        "outputId": "7dcc3d6e-e836-407d-c502-c45cfe565393"
      },
      "source": [
        "bn_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "bn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQBrVW6KYr3"
      },
      "source": [
        "BN Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2oakRaKZ4y",
        "outputId": "38c4f3fc-19e9-4e5c-9b16-293284b05960"
      },
      "source": [
        "bn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "bn_model_history=bn_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = bn_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('BN model Test loss:', score[0])\n",
        "print('BN model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 37ms/step - loss: 2.5394 - accuracy: 0.1298 - val_loss: 2.2691 - val_accuracy: 0.1650\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.3138 - accuracy: 0.2056 - val_loss: 2.1844 - val_accuracy: 0.2550\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.1731 - accuracy: 0.2358 - val_loss: 2.0627 - val_accuracy: 0.3400\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.0301 - accuracy: 0.3091 - val_loss: 1.9262 - val_accuracy: 0.3950\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.8793 - accuracy: 0.4023 - val_loss: 1.7999 - val_accuracy: 0.4400\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.7715 - accuracy: 0.4224 - val_loss: 1.6854 - val_accuracy: 0.4650\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6718 - accuracy: 0.4560 - val_loss: 1.5809 - val_accuracy: 0.4850\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5696 - accuracy: 0.5091 - val_loss: 1.4859 - val_accuracy: 0.5150\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4862 - accuracy: 0.5528 - val_loss: 1.3961 - val_accuracy: 0.5300\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4146 - accuracy: 0.5808 - val_loss: 1.3169 - val_accuracy: 0.5750\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3383 - accuracy: 0.6011 - val_loss: 1.2452 - val_accuracy: 0.6350\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2626 - accuracy: 0.6416 - val_loss: 1.1818 - val_accuracy: 0.6700\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2088 - accuracy: 0.6610 - val_loss: 1.1220 - val_accuracy: 0.6650\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1893 - accuracy: 0.6557 - val_loss: 1.0691 - val_accuracy: 0.6850\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0899 - accuracy: 0.7198 - val_loss: 1.0208 - val_accuracy: 0.7100\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0499 - accuracy: 0.7204 - val_loss: 0.9784 - val_accuracy: 0.7400\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0009 - accuracy: 0.7309 - val_loss: 0.9392 - val_accuracy: 0.7400\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9432 - accuracy: 0.7528 - val_loss: 0.9037 - val_accuracy: 0.7500\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9126 - accuracy: 0.7722 - val_loss: 0.8687 - val_accuracy: 0.7600\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8853 - accuracy: 0.7522 - val_loss: 0.8396 - val_accuracy: 0.7850\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8505 - accuracy: 0.7902 - val_loss: 0.8117 - val_accuracy: 0.7900\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7724 - accuracy: 0.8235 - val_loss: 0.7871 - val_accuracy: 0.8000\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7921 - accuracy: 0.8133 - val_loss: 0.7627 - val_accuracy: 0.8150\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7692 - accuracy: 0.8064 - val_loss: 0.7405 - val_accuracy: 0.8200\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7662 - accuracy: 0.8165 - val_loss: 0.7207 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7427 - accuracy: 0.8363 - val_loss: 0.7024 - val_accuracy: 0.8250\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6771 - accuracy: 0.8391 - val_loss: 0.6848 - val_accuracy: 0.8250\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6597 - accuracy: 0.8430 - val_loss: 0.6683 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6352 - accuracy: 0.8543 - val_loss: 0.6526 - val_accuracy: 0.8400\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6260 - accuracy: 0.8641 - val_loss: 0.6392 - val_accuracy: 0.8400\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6459 - accuracy: 0.8594 - val_loss: 0.6260 - val_accuracy: 0.8450\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5809 - accuracy: 0.8656 - val_loss: 0.6115 - val_accuracy: 0.8500\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5964 - accuracy: 0.8634 - val_loss: 0.5993 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5987 - accuracy: 0.8612 - val_loss: 0.5881 - val_accuracy: 0.8550\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6051 - accuracy: 0.8662 - val_loss: 0.5773 - val_accuracy: 0.8550\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5604 - accuracy: 0.8673 - val_loss: 0.5677 - val_accuracy: 0.8550\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5332 - accuracy: 0.8889 - val_loss: 0.5587 - val_accuracy: 0.8550\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5393 - accuracy: 0.8750 - val_loss: 0.5495 - val_accuracy: 0.8550\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5117 - accuracy: 0.8887 - val_loss: 0.5416 - val_accuracy: 0.8550\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5303 - accuracy: 0.8779 - val_loss: 0.5330 - val_accuracy: 0.8550\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5020 - accuracy: 0.8781 - val_loss: 0.5264 - val_accuracy: 0.8650\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4918 - accuracy: 0.8836 - val_loss: 0.5195 - val_accuracy: 0.8650\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4678 - accuracy: 0.8952 - val_loss: 0.5116 - val_accuracy: 0.8650\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4728 - accuracy: 0.8917 - val_loss: 0.5051 - val_accuracy: 0.8650\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4374 - accuracy: 0.9070 - val_loss: 0.4981 - val_accuracy: 0.8650\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4388 - accuracy: 0.9025 - val_loss: 0.4915 - val_accuracy: 0.8750\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4527 - accuracy: 0.8954 - val_loss: 0.4865 - val_accuracy: 0.8750\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4255 - accuracy: 0.9170 - val_loss: 0.4812 - val_accuracy: 0.8750\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4215 - accuracy: 0.9008 - val_loss: 0.4768 - val_accuracy: 0.8750\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4148 - accuracy: 0.9124 - val_loss: 0.4712 - val_accuracy: 0.8800\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4125 - accuracy: 0.9052 - val_loss: 0.4659 - val_accuracy: 0.8850\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4067 - accuracy: 0.9098 - val_loss: 0.4613 - val_accuracy: 0.8850\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3851 - accuracy: 0.9227 - val_loss: 0.4568 - val_accuracy: 0.8850\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4050 - accuracy: 0.9126 - val_loss: 0.4516 - val_accuracy: 0.8850\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3674 - accuracy: 0.9206 - val_loss: 0.4470 - val_accuracy: 0.8900\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3660 - accuracy: 0.9227 - val_loss: 0.4438 - val_accuracy: 0.8900\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3706 - accuracy: 0.9143 - val_loss: 0.4394 - val_accuracy: 0.8900\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3792 - accuracy: 0.9176 - val_loss: 0.4342 - val_accuracy: 0.8900\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3359 - accuracy: 0.9376 - val_loss: 0.4301 - val_accuracy: 0.8900\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3692 - accuracy: 0.9167 - val_loss: 0.4269 - val_accuracy: 0.8900\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3465 - accuracy: 0.9342 - val_loss: 0.4239 - val_accuracy: 0.8900\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3202 - accuracy: 0.9429 - val_loss: 0.4200 - val_accuracy: 0.8900\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3219 - accuracy: 0.9453 - val_loss: 0.4165 - val_accuracy: 0.8900\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3386 - accuracy: 0.9217 - val_loss: 0.4137 - val_accuracy: 0.8900\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3147 - accuracy: 0.9281 - val_loss: 0.4106 - val_accuracy: 0.8900\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3175 - accuracy: 0.9456 - val_loss: 0.4080 - val_accuracy: 0.8900\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3403 - accuracy: 0.9259 - val_loss: 0.4048 - val_accuracy: 0.8950\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3167 - accuracy: 0.9287 - val_loss: 0.4011 - val_accuracy: 0.8950\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3016 - accuracy: 0.9420 - val_loss: 0.3982 - val_accuracy: 0.8950\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3354 - accuracy: 0.9256 - val_loss: 0.3952 - val_accuracy: 0.8950\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2953 - accuracy: 0.9427 - val_loss: 0.3925 - val_accuracy: 0.8950\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2891 - accuracy: 0.9460 - val_loss: 0.3906 - val_accuracy: 0.8950\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3023 - accuracy: 0.9331 - val_loss: 0.3880 - val_accuracy: 0.9000\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2857 - accuracy: 0.9483 - val_loss: 0.3856 - val_accuracy: 0.9000\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.2708 - accuracy: 0.9538 - val_loss: 0.3829 - val_accuracy: 0.9000\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.2856 - accuracy: 0.9459 - val_loss: 0.3797 - val_accuracy: 0.9000\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2743 - accuracy: 0.9500 - val_loss: 0.3772 - val_accuracy: 0.9050\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2825 - accuracy: 0.9487 - val_loss: 0.3755 - val_accuracy: 0.9050\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2798 - accuracy: 0.9435 - val_loss: 0.3742 - val_accuracy: 0.9100\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3014 - accuracy: 0.9401 - val_loss: 0.3722 - val_accuracy: 0.9100\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2624 - accuracy: 0.9475 - val_loss: 0.3699 - val_accuracy: 0.9100\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2389 - accuracy: 0.9604 - val_loss: 0.3678 - val_accuracy: 0.9100\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2492 - accuracy: 0.9516 - val_loss: 0.3657 - val_accuracy: 0.9100\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2568 - accuracy: 0.9532 - val_loss: 0.3640 - val_accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2462 - accuracy: 0.9530 - val_loss: 0.3607 - val_accuracy: 0.9100\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2310 - accuracy: 0.9638 - val_loss: 0.3582 - val_accuracy: 0.9100\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2592 - accuracy: 0.9479 - val_loss: 0.3574 - val_accuracy: 0.9100\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2389 - accuracy: 0.9543 - val_loss: 0.3553 - val_accuracy: 0.9100\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2425 - accuracy: 0.9489 - val_loss: 0.3541 - val_accuracy: 0.9100\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2462 - accuracy: 0.9479 - val_loss: 0.3520 - val_accuracy: 0.9100\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2490 - accuracy: 0.9564 - val_loss: 0.3510 - val_accuracy: 0.9100\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2284 - accuracy: 0.9599 - val_loss: 0.3501 - val_accuracy: 0.9100\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2276 - accuracy: 0.9647 - val_loss: 0.3467 - val_accuracy: 0.9100\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2434 - accuracy: 0.9532 - val_loss: 0.3456 - val_accuracy: 0.9100\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2147 - accuracy: 0.9645 - val_loss: 0.3441 - val_accuracy: 0.9100\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2235 - accuracy: 0.9607 - val_loss: 0.3429 - val_accuracy: 0.9100\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 0.2256 - accuracy: 0.9584 - val_loss: 0.3417 - val_accuracy: 0.9150\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2322 - accuracy: 0.9550 - val_loss: 0.3395 - val_accuracy: 0.9150\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.2114 - accuracy: 0.9665 - val_loss: 0.3383 - val_accuracy: 0.9150\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.2270 - accuracy: 0.9608 - val_loss: 0.3368 - val_accuracy: 0.9150\n",
            "BN model Test loss: 0.3762774169445038\n",
            "BN model Test accuracy: 0.8919000029563904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fDPwaFsJ47I"
      },
      "source": [
        "##4.4 Final Model\n",
        "지금까지 썼던 Regularization 기법들을 종합선물세트로 적용해 봅시다. 다음과 같은 Final model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ppnPcELADo",
        "outputId": "63dfb2b2-8abe-484b-f8e5-df8270d5981b"
      },
      "source": [
        "final_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "final_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocok8VnoLcb4"
      },
      "source": [
        "Final Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bFRiHtLdVe",
        "outputId": "bd8dd073-3471-463f-f482-66b1d99d7fc9"
      },
      "source": [
        "final_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "final_model_history=final_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = final_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Final model Test loss:', score[0])\n",
        "print('Final model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 37ms/step - loss: 2.8955 - accuracy: 0.1204 - val_loss: 2.2754 - val_accuracy: 0.1200\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.7084 - accuracy: 0.1313 - val_loss: 2.2247 - val_accuracy: 0.1550\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.5755 - accuracy: 0.1531 - val_loss: 2.1182 - val_accuracy: 0.2250\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.4187 - accuracy: 0.1860 - val_loss: 1.9820 - val_accuracy: 0.2850\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.2413 - accuracy: 0.2345 - val_loss: 1.8511 - val_accuracy: 0.3250\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.1545 - accuracy: 0.2505 - val_loss: 1.7301 - val_accuracy: 0.3950\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9716 - accuracy: 0.3018 - val_loss: 1.6215 - val_accuracy: 0.4500\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.9178 - accuracy: 0.3356 - val_loss: 1.5235 - val_accuracy: 0.4900\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.8989 - accuracy: 0.3492 - val_loss: 1.4347 - val_accuracy: 0.5400\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.7533 - accuracy: 0.4191 - val_loss: 1.3557 - val_accuracy: 0.5850\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6817 - accuracy: 0.4232 - val_loss: 1.2844 - val_accuracy: 0.6200\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5772 - accuracy: 0.4709 - val_loss: 1.2204 - val_accuracy: 0.6250\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5230 - accuracy: 0.5010 - val_loss: 1.1627 - val_accuracy: 0.6350\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4977 - accuracy: 0.5170 - val_loss: 1.1104 - val_accuracy: 0.6500\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3753 - accuracy: 0.5492 - val_loss: 1.0639 - val_accuracy: 0.6700\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3597 - accuracy: 0.5349 - val_loss: 1.0189 - val_accuracy: 0.6800\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2687 - accuracy: 0.5849 - val_loss: 0.9781 - val_accuracy: 0.6950\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2412 - accuracy: 0.6174 - val_loss: 0.9422 - val_accuracy: 0.7050\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1827 - accuracy: 0.6330 - val_loss: 0.9086 - val_accuracy: 0.7200\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1396 - accuracy: 0.6373 - val_loss: 0.8766 - val_accuracy: 0.7400\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0607 - accuracy: 0.6774 - val_loss: 0.8481 - val_accuracy: 0.7450\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0773 - accuracy: 0.6601 - val_loss: 0.8215 - val_accuracy: 0.7600\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.0076 - accuracy: 0.6867 - val_loss: 0.7981 - val_accuracy: 0.7650\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9889 - accuracy: 0.6993 - val_loss: 0.7746 - val_accuracy: 0.7650\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.9996 - accuracy: 0.7114 - val_loss: 0.7518 - val_accuracy: 0.7700\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.9798 - accuracy: 0.6907 - val_loss: 0.7321 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9486 - accuracy: 0.7138 - val_loss: 0.7138 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9006 - accuracy: 0.7315 - val_loss: 0.6961 - val_accuracy: 0.7950\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8979 - accuracy: 0.7407 - val_loss: 0.6796 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8871 - accuracy: 0.7445 - val_loss: 0.6638 - val_accuracy: 0.8000\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8305 - accuracy: 0.7377 - val_loss: 0.6491 - val_accuracy: 0.8050\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7884 - accuracy: 0.7834 - val_loss: 0.6363 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.7846 - accuracy: 0.7750 - val_loss: 0.6237 - val_accuracy: 0.8050\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7536 - accuracy: 0.7882 - val_loss: 0.6115 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7700 - accuracy: 0.7828 - val_loss: 0.6000 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7654 - accuracy: 0.7855 - val_loss: 0.5895 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7250 - accuracy: 0.7999 - val_loss: 0.5783 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7272 - accuracy: 0.7999 - val_loss: 0.5687 - val_accuracy: 0.8150\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6656 - accuracy: 0.8158 - val_loss: 0.5601 - val_accuracy: 0.8150\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7133 - accuracy: 0.7959 - val_loss: 0.5495 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6347 - accuracy: 0.8242 - val_loss: 0.5415 - val_accuracy: 0.8300\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6525 - accuracy: 0.8170 - val_loss: 0.5329 - val_accuracy: 0.8300\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6563 - accuracy: 0.8301 - val_loss: 0.5257 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6426 - accuracy: 0.8127 - val_loss: 0.5186 - val_accuracy: 0.8350\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.5976 - accuracy: 0.8266 - val_loss: 0.5115 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6115 - accuracy: 0.8175 - val_loss: 0.5047 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5733 - accuracy: 0.8451 - val_loss: 0.4992 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5723 - accuracy: 0.8420 - val_loss: 0.4925 - val_accuracy: 0.8500\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5931 - accuracy: 0.8281 - val_loss: 0.4877 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5519 - accuracy: 0.8520 - val_loss: 0.4801 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5268 - accuracy: 0.8701 - val_loss: 0.4742 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5048 - accuracy: 0.8688 - val_loss: 0.4692 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5378 - accuracy: 0.8435 - val_loss: 0.4637 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5296 - accuracy: 0.8631 - val_loss: 0.4586 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5190 - accuracy: 0.8684 - val_loss: 0.4543 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5320 - accuracy: 0.8594 - val_loss: 0.4490 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5172 - accuracy: 0.8616 - val_loss: 0.4440 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4764 - accuracy: 0.8794 - val_loss: 0.4387 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4782 - accuracy: 0.8740 - val_loss: 0.4347 - val_accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4772 - accuracy: 0.8855 - val_loss: 0.4307 - val_accuracy: 0.8750\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4768 - accuracy: 0.8674 - val_loss: 0.4268 - val_accuracy: 0.8750\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4575 - accuracy: 0.8864 - val_loss: 0.4218 - val_accuracy: 0.8750\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4883 - accuracy: 0.8652 - val_loss: 0.4185 - val_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4487 - accuracy: 0.8893 - val_loss: 0.4154 - val_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4712 - accuracy: 0.8754 - val_loss: 0.4113 - val_accuracy: 0.8850\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4624 - accuracy: 0.8785 - val_loss: 0.4084 - val_accuracy: 0.8850\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4669 - accuracy: 0.8638 - val_loss: 0.4045 - val_accuracy: 0.8850\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4214 - accuracy: 0.8941 - val_loss: 0.4010 - val_accuracy: 0.8850\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4255 - accuracy: 0.8846 - val_loss: 0.3979 - val_accuracy: 0.8850\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4336 - accuracy: 0.8867 - val_loss: 0.3946 - val_accuracy: 0.8850\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4197 - accuracy: 0.8927 - val_loss: 0.3918 - val_accuracy: 0.8900\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4063 - accuracy: 0.8913 - val_loss: 0.3889 - val_accuracy: 0.8900\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4231 - accuracy: 0.8798 - val_loss: 0.3858 - val_accuracy: 0.8900\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4333 - accuracy: 0.8809 - val_loss: 0.3827 - val_accuracy: 0.8900\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3862 - accuracy: 0.9058 - val_loss: 0.3804 - val_accuracy: 0.8900\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4084 - accuracy: 0.8904 - val_loss: 0.3775 - val_accuracy: 0.8900\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3977 - accuracy: 0.8965 - val_loss: 0.3746 - val_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3792 - accuracy: 0.9075 - val_loss: 0.3726 - val_accuracy: 0.8950\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3759 - accuracy: 0.8924 - val_loss: 0.3700 - val_accuracy: 0.9000\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3773 - accuracy: 0.9133 - val_loss: 0.3669 - val_accuracy: 0.9000\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3921 - accuracy: 0.9013 - val_loss: 0.3652 - val_accuracy: 0.9050\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3697 - accuracy: 0.8968 - val_loss: 0.3631 - val_accuracy: 0.9100\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3796 - accuracy: 0.9040 - val_loss: 0.3604 - val_accuracy: 0.9100\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3772 - accuracy: 0.9062 - val_loss: 0.3592 - val_accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3560 - accuracy: 0.9111 - val_loss: 0.3571 - val_accuracy: 0.9150\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3549 - accuracy: 0.9118 - val_loss: 0.3548 - val_accuracy: 0.9150\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3338 - accuracy: 0.9120 - val_loss: 0.3523 - val_accuracy: 0.9150\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3345 - accuracy: 0.9207 - val_loss: 0.3494 - val_accuracy: 0.9150\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3715 - accuracy: 0.9015 - val_loss: 0.3475 - val_accuracy: 0.9150\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3096 - accuracy: 0.9216 - val_loss: 0.3455 - val_accuracy: 0.9200\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3411 - accuracy: 0.9052 - val_loss: 0.3437 - val_accuracy: 0.9200\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3062 - accuracy: 0.9260 - val_loss: 0.3417 - val_accuracy: 0.9200\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3406 - accuracy: 0.9109 - val_loss: 0.3410 - val_accuracy: 0.9150\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3273 - accuracy: 0.9271 - val_loss: 0.3386 - val_accuracy: 0.9200\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2994 - accuracy: 0.9339 - val_loss: 0.3365 - val_accuracy: 0.9200\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3277 - accuracy: 0.9223 - val_loss: 0.3357 - val_accuracy: 0.9200\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2972 - accuracy: 0.9256 - val_loss: 0.3340 - val_accuracy: 0.9200\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3239 - accuracy: 0.9189 - val_loss: 0.3322 - val_accuracy: 0.9200\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3220 - accuracy: 0.9212 - val_loss: 0.3289 - val_accuracy: 0.9200\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3189 - accuracy: 0.9155 - val_loss: 0.3285 - val_accuracy: 0.9250\n",
            "Final model Test loss: 0.39608311653137207\n",
            "Final model Test accuracy: 0.8842999935150146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPR3nEMAL4Yd"
      },
      "source": [
        "(optional) Training data가 늘어나면 regularization 효과가 나는 것을 보였습니다. 쉽게 training data를 늘릴 수 있는 방법은 data augmentation 입니다. 이 방법은 기존 training data를 적절히 rotating, flipping, scaling, shifting 하여 training data 수를 늘리는 것입니다. data augmentation의 regularization 효과를 테스트해 보세요. 또한 뉴럴 네트워크의 노드 개수나 층수를 바꿔서 성능을 올려보는 것도 테스트해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn11ZtM7GmS-"
      },
      "source": [
        ""
      ]
    }
  ]
}