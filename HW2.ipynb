{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woonghee97/deep_learning/blob/DeepLearning/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OA0f9oxYGYb"
      },
      "source": [
        "# HW#6 Regularization\n",
        "\n",
        "안녕하세요, 광운대학교 로봇학부의 오정현 교수입니다. 본 자료는 딥러닝 실습 수업을 위해 제작된 것입니다.\n",
        "\n",
        "파이썬 문법\n",
        "- 점프투파이썬(https://wikidocs.net/book/1) 참고\n",
        "\n",
        "이번 과제는 딥러닝의 일반화 성능을 높이기 위한 Regularization을 해보는 것입니다.이미지 분류에 여러 가지 Regularization 기법을 적용해 보도록 하겠습니다. 대표적인 Regularization 기법으로 Dropout, Data augmentation, Batch Normalization 등이 있습니다.\n",
        "\n",
        "이번 과제는 (https://www.tensorflow.org/tutorials/keras/classification?hl=ko)를 참고하면 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9KBzwzd7Bub"
      },
      "source": [
        "#1. Data Generation\n",
        "Data는 mnist dataset을 이용하도록 하겠습니다. mnist dataset은 원래 60000개의 training set이 주어져 있지만 overfitting을 유도하기 위하여 1000개의 data만 이용하려고 합니다. 1000개의 data로 이루어진 x_train과 y_train을 만들어 보세요. 그리고 2000개로 이루어진 large_x_train, large_y_train을 만들어보세요. 그리고 training data의 다른 범위에서 200개로 이루어진 x_validation과 y_validation도 만들어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3K3NqD_wTyh",
        "outputId": "a266b8de-4865-4b67-836f-51752920aea1"
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot\n",
        "from keras.utils import np_utils\n",
        "\n",
        "batch_size = 28\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, Y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "### START CODE HERE ###\n",
        "x_validation = X_train[3000:3200]\n",
        "y_validation = Y_train[3000:3200]\n",
        "large_x_train = X_train[0:2000]\n",
        "large_y_train = Y_train[0:2000]\n",
        "x_train = X_train[2000:3000]\n",
        "y_train = Y_train[2000:3000]\n",
        "### END CODE HERE ###\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    large_x_train = large_x_train.reshape(large_x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_validation = x_validation.reshape(x_validation.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "large_x_train = large_x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "large_x_train /= 255\n",
        "x_validation /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "large_y_train = keras.utils.np_utils.to_categorical(large_y_train, num_classes)\n",
        "y_validation = keras.utils.np_utils.to_categorical(y_validation, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "assert large_x_train.shape[0]==2000\n",
        "assert large_y_train.shape[0]==2000\n",
        "assert x_train.shape[0]==1000\n",
        "assert y_train.shape[0]==1000\n",
        "assert x_validation.shape[0]==200\n",
        "assert y_validation.shape[0]==200\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"large_x_train shape:\", large_x_train.shape)\n",
        "print(\"large_y_train shape:\", large_y_train.shape)\n",
        "print(\"x_validation shape:\", x_validation.shape)\n",
        "print(\"y_validation shape:\", y_validation.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_validation shape: (200, 28, 28)\n",
            "y_validation shape: (200,)\n",
            "x_train shape: (1000, 28, 28, 1)\n",
            "y_train shape: (1000, 10)\n",
            "large_x_train shape: (2000, 28, 28, 1)\n",
            "large_y_train shape: (2000, 10)\n",
            "x_validation shape: (200, 28, 28, 1)\n",
            "y_validation shape: (200, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlNTST2K_IRE"
      },
      "source": [
        "#2. 모델 생성\n",
        "복습 차원에서 MLP 분류모델을 만들어 보도록 하겠습니다. 모델의 마지막 레이어에는 활성화 함수로 10개의 출력을 가지는 softmax를 달겠습니다. 이를 통해서 모델은 이미지안의 숫자가 0부터 9까지의 숫자중에 어디에 가까운지를 확률적으로 나타냅니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNI8KWo8PyC"
      },
      "source": [
        "다음과 같은 MLP 모델을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560L-utPx8z1",
        "outputId": "0598847c-87ed-4734-df07-670af78b4899"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "### START CODE HERE ###\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Dense(1024))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhIfmZeBZm8"
      },
      "source": [
        "#3. Learning MLP\n",
        "기본 MLP 분류모델을 학습해 보겠습니다. Overfitting은 Training data에 맞추어 과도하게 학습이 이루어져 Test data에서 높은 성능이 나지 않는 현상, 즉 Generalization 성능이 높지 않게 나타나는 현상을 의미합니다. 따라서 Overfitting이 발생하면 Training accuracy는 높지만 Test accuracy는 높지 않게 나타납니다. \n",
        "\n",
        "현재 모델이 overfitting이 발생하는지 체크해 보세요. 일부러 overfitting이 발생하도록 유도하였기 때문에 overfitting 현상이 나타나야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1VoI2bCyBy-",
        "outputId": "ab9c6d05-bbd2-4fce-80f7-76e1c4c074bc"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "weights = model.get_weights()\n",
        "\n",
        "history=model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_input'), name='flatten_input', description=\"created by layer 'flatten_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_input'), name='flatten_input', description=\"created by layer 'flatten_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "35/36 [============================>.] - ETA: 0s - loss: 2.3961 - accuracy: 0.0592WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_input'), name='flatten_input', description=\"created by layer 'flatten_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "36/36 [==============================] - 2s 40ms/step - loss: 2.3950 - accuracy: 0.0600 - val_loss: 2.3678 - val_accuracy: 0.0650\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.2975 - accuracy: 0.0990 - val_loss: 2.2908 - val_accuracy: 0.1200\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.2083 - accuracy: 0.1520 - val_loss: 2.2191 - val_accuracy: 0.1650\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 2.1254 - accuracy: 0.2500 - val_loss: 2.1518 - val_accuracy: 0.2400\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 2.0473 - accuracy: 0.3610 - val_loss: 2.0883 - val_accuracy: 0.3050\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.9735 - accuracy: 0.4420 - val_loss: 2.0276 - val_accuracy: 0.3700\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.9042 - accuracy: 0.5090 - val_loss: 1.9703 - val_accuracy: 0.4450\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.8383 - accuracy: 0.5600 - val_loss: 1.9156 - val_accuracy: 0.5150\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 1.7761 - accuracy: 0.6000 - val_loss: 1.8644 - val_accuracy: 0.5600\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.7167 - accuracy: 0.6310 - val_loss: 1.8152 - val_accuracy: 0.5950\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.6600 - accuracy: 0.6650 - val_loss: 1.7673 - val_accuracy: 0.5950\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.6064 - accuracy: 0.6820 - val_loss: 1.7211 - val_accuracy: 0.6300\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.5547 - accuracy: 0.7030 - val_loss: 1.6771 - val_accuracy: 0.6450\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.5058 - accuracy: 0.7150 - val_loss: 1.6346 - val_accuracy: 0.6500\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - 1s 39ms/step - loss: 1.4593 - accuracy: 0.7210 - val_loss: 1.5943 - val_accuracy: 0.6550\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.4147 - accuracy: 0.7340 - val_loss: 1.5559 - val_accuracy: 0.6700\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3725 - accuracy: 0.7440 - val_loss: 1.5189 - val_accuracy: 0.6850\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 1.3324 - accuracy: 0.7550 - val_loss: 1.4835 - val_accuracy: 0.6850\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2940 - accuracy: 0.7670 - val_loss: 1.4493 - val_accuracy: 0.7100\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2576 - accuracy: 0.7720 - val_loss: 1.4166 - val_accuracy: 0.7200\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.2229 - accuracy: 0.7760 - val_loss: 1.3853 - val_accuracy: 0.7250\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.1899 - accuracy: 0.7820 - val_loss: 1.3555 - val_accuracy: 0.7300\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.1583 - accuracy: 0.7840 - val_loss: 1.3266 - val_accuracy: 0.7400\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 1.1282 - accuracy: 0.7870 - val_loss: 1.2992 - val_accuracy: 0.7450\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0996 - accuracy: 0.7910 - val_loss: 1.2729 - val_accuracy: 0.7450\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0725 - accuracy: 0.7950 - val_loss: 1.2479 - val_accuracy: 0.7550\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0465 - accuracy: 0.8010 - val_loss: 1.2237 - val_accuracy: 0.7600\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 1.0218 - accuracy: 0.8020 - val_loss: 1.2004 - val_accuracy: 0.7600\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.9982 - accuracy: 0.8030 - val_loss: 1.1777 - val_accuracy: 0.7700\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.9756 - accuracy: 0.8080 - val_loss: 1.1562 - val_accuracy: 0.7700\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.9541 - accuracy: 0.8090 - val_loss: 1.1362 - val_accuracy: 0.7800\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.9336 - accuracy: 0.8170 - val_loss: 1.1162 - val_accuracy: 0.7850\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.9139 - accuracy: 0.8210 - val_loss: 1.0972 - val_accuracy: 0.7900\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8948 - accuracy: 0.8250 - val_loss: 1.0790 - val_accuracy: 0.7900\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8767 - accuracy: 0.8290 - val_loss: 1.0612 - val_accuracy: 0.7950\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.8593 - accuracy: 0.8280 - val_loss: 1.0444 - val_accuracy: 0.8000\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.8427 - accuracy: 0.8310 - val_loss: 1.0281 - val_accuracy: 0.8000\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - 1s 38ms/step - loss: 0.8266 - accuracy: 0.8330 - val_loss: 1.0126 - val_accuracy: 0.8000\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - 1s 37ms/step - loss: 0.8112 - accuracy: 0.8340 - val_loss: 0.9970 - val_accuracy: 0.8000\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - 1s 38ms/step - loss: 0.7964 - accuracy: 0.8390 - val_loss: 0.9822 - val_accuracy: 0.8000\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7824 - accuracy: 0.8410 - val_loss: 0.9677 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7687 - accuracy: 0.8440 - val_loss: 0.9540 - val_accuracy: 0.8000\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - 1s 37ms/step - loss: 0.7556 - accuracy: 0.8470 - val_loss: 0.9412 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7430 - accuracy: 0.8480 - val_loss: 0.9282 - val_accuracy: 0.8000\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - 1s 37ms/step - loss: 0.7309 - accuracy: 0.8490 - val_loss: 0.9162 - val_accuracy: 0.8000\n",
            "Epoch 46/100\n",
            "36/36 [==============================] - 1s 36ms/step - loss: 0.7194 - accuracy: 0.8530 - val_loss: 0.9046 - val_accuracy: 0.8050\n",
            "Epoch 47/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.7082 - accuracy: 0.8600 - val_loss: 0.8932 - val_accuracy: 0.8050\n",
            "Epoch 48/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6975 - accuracy: 0.8600 - val_loss: 0.8828 - val_accuracy: 0.8100\n",
            "Epoch 49/100\n",
            "36/36 [==============================] - 2s 47ms/step - loss: 0.6870 - accuracy: 0.8590 - val_loss: 0.8724 - val_accuracy: 0.8100\n",
            "Epoch 50/100\n",
            "36/36 [==============================] - 1s 40ms/step - loss: 0.6770 - accuracy: 0.8610 - val_loss: 0.8626 - val_accuracy: 0.8100\n",
            "Epoch 51/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6675 - accuracy: 0.8590 - val_loss: 0.8524 - val_accuracy: 0.8150\n",
            "Epoch 52/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6582 - accuracy: 0.8620 - val_loss: 0.8428 - val_accuracy: 0.8150\n",
            "Epoch 53/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6491 - accuracy: 0.8620 - val_loss: 0.8343 - val_accuracy: 0.8150\n",
            "Epoch 54/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6404 - accuracy: 0.8630 - val_loss: 0.8253 - val_accuracy: 0.8150\n",
            "Epoch 55/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6320 - accuracy: 0.8650 - val_loss: 0.8165 - val_accuracy: 0.8150\n",
            "Epoch 56/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6239 - accuracy: 0.8660 - val_loss: 0.8082 - val_accuracy: 0.8150\n",
            "Epoch 57/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6160 - accuracy: 0.8650 - val_loss: 0.8000 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.6083 - accuracy: 0.8710 - val_loss: 0.7923 - val_accuracy: 0.8200\n",
            "Epoch 59/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.6009 - accuracy: 0.8710 - val_loss: 0.7846 - val_accuracy: 0.8200\n",
            "Epoch 60/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5937 - accuracy: 0.8720 - val_loss: 0.7769 - val_accuracy: 0.8200\n",
            "Epoch 61/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5866 - accuracy: 0.8760 - val_loss: 0.7696 - val_accuracy: 0.8200\n",
            "Epoch 62/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5797 - accuracy: 0.8750 - val_loss: 0.7624 - val_accuracy: 0.8250\n",
            "Epoch 63/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5731 - accuracy: 0.8780 - val_loss: 0.7561 - val_accuracy: 0.8250\n",
            "Epoch 64/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5666 - accuracy: 0.8800 - val_loss: 0.7494 - val_accuracy: 0.8300\n",
            "Epoch 65/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5604 - accuracy: 0.8800 - val_loss: 0.7427 - val_accuracy: 0.8300\n",
            "Epoch 66/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.5542 - accuracy: 0.8810 - val_loss: 0.7363 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5483 - accuracy: 0.8840 - val_loss: 0.7301 - val_accuracy: 0.8300\n",
            "Epoch 68/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5425 - accuracy: 0.8820 - val_loss: 0.7244 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5368 - accuracy: 0.8840 - val_loss: 0.7186 - val_accuracy: 0.8300\n",
            "Epoch 70/100\n",
            "36/36 [==============================] - 2s 45ms/step - loss: 0.5313 - accuracy: 0.8870 - val_loss: 0.7132 - val_accuracy: 0.8300\n",
            "Epoch 71/100\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 0.5259 - accuracy: 0.8880 - val_loss: 0.7080 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5207 - accuracy: 0.8920 - val_loss: 0.7025 - val_accuracy: 0.8300\n",
            "Epoch 73/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5155 - accuracy: 0.8920 - val_loss: 0.6970 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5105 - accuracy: 0.8920 - val_loss: 0.6921 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.5057 - accuracy: 0.8930 - val_loss: 0.6874 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.5009 - accuracy: 0.8950 - val_loss: 0.6825 - val_accuracy: 0.8400\n",
            "Epoch 77/100\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.4962 - accuracy: 0.8960 - val_loss: 0.6779 - val_accuracy: 0.8400\n",
            "Epoch 78/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4917 - accuracy: 0.8960 - val_loss: 0.6737 - val_accuracy: 0.8400\n",
            "Epoch 79/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4873 - accuracy: 0.8980 - val_loss: 0.6694 - val_accuracy: 0.8400\n",
            "Epoch 80/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4830 - accuracy: 0.8990 - val_loss: 0.6648 - val_accuracy: 0.8400\n",
            "Epoch 81/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4787 - accuracy: 0.8980 - val_loss: 0.6607 - val_accuracy: 0.8400\n",
            "Epoch 82/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4746 - accuracy: 0.9000 - val_loss: 0.6566 - val_accuracy: 0.8400\n",
            "Epoch 83/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4707 - accuracy: 0.9000 - val_loss: 0.6524 - val_accuracy: 0.8450\n",
            "Epoch 84/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4665 - accuracy: 0.8990 - val_loss: 0.6487 - val_accuracy: 0.8450\n",
            "Epoch 85/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4627 - accuracy: 0.8990 - val_loss: 0.6449 - val_accuracy: 0.8450\n",
            "Epoch 86/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4591 - accuracy: 0.9010 - val_loss: 0.6408 - val_accuracy: 0.8450\n",
            "Epoch 87/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4551 - accuracy: 0.9010 - val_loss: 0.6369 - val_accuracy: 0.8450\n",
            "Epoch 88/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4515 - accuracy: 0.9000 - val_loss: 0.6330 - val_accuracy: 0.8450\n",
            "Epoch 89/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4479 - accuracy: 0.9020 - val_loss: 0.6293 - val_accuracy: 0.8450\n",
            "Epoch 90/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4444 - accuracy: 0.9000 - val_loss: 0.6259 - val_accuracy: 0.8450\n",
            "Epoch 91/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4409 - accuracy: 0.8990 - val_loss: 0.6226 - val_accuracy: 0.8450\n",
            "Epoch 92/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4376 - accuracy: 0.9020 - val_loss: 0.6193 - val_accuracy: 0.8450\n",
            "Epoch 93/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4342 - accuracy: 0.9010 - val_loss: 0.6158 - val_accuracy: 0.8450\n",
            "Epoch 94/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4309 - accuracy: 0.9040 - val_loss: 0.6130 - val_accuracy: 0.8450\n",
            "Epoch 95/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4277 - accuracy: 0.9030 - val_loss: 0.6095 - val_accuracy: 0.8450\n",
            "Epoch 96/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4245 - accuracy: 0.9020 - val_loss: 0.6066 - val_accuracy: 0.8450\n",
            "Epoch 97/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4214 - accuracy: 0.9020 - val_loss: 0.6036 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4184 - accuracy: 0.9020 - val_loss: 0.6005 - val_accuracy: 0.8450\n",
            "Epoch 99/100\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.4154 - accuracy: 0.9010 - val_loss: 0.5976 - val_accuracy: 0.8450\n",
            "Epoch 100/100\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4124 - accuracy: 0.9020 - val_loss: 0.5947 - val_accuracy: 0.8450\n",
            "Test loss: 0.5809028744697571\n",
            "Test accuracy: 0.8439000248908997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jebNKzFBJGE"
      },
      "source": [
        "#4. Regularization\n",
        "Overfitting이 발생한 모델에 다양한 Regularization 기법을 이용해 보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxZKsjCFvhT"
      },
      "source": [
        "4.1 Large Dataset\n",
        "\n",
        "Training data가 충분하다면 overfitting 현상이 발생할 가능성이 줄어듭니다. 기본 MLP 분류 모델에서 large_x_train과 large_y_train을 이용하면 성능이 올라갈 것입니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5b1fp96FuEy",
        "outputId": "b8b7fa87-5735-4750-bad5-9bd8df1d041f"
      },
      "source": [
        "#initialize weights\n",
        "model.set_weights(weights)\n",
        "\n",
        "large_model_history=model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Largemodel Test loss:', score[0])\n",
        "print('Largemodel Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.3104 - accuracy: 0.1080 - val_loss: 2.2287 - val_accuracy: 0.1700\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.0933 - accuracy: 0.3130 - val_loss: 2.0610 - val_accuracy: 0.3550\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9249 - accuracy: 0.4865 - val_loss: 1.9243 - val_accuracy: 0.4950\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.7858 - accuracy: 0.5935 - val_loss: 1.8085 - val_accuracy: 0.5700\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6675 - accuracy: 0.6465 - val_loss: 1.7081 - val_accuracy: 0.6350\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5657 - accuracy: 0.6820 - val_loss: 1.6196 - val_accuracy: 0.6450\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.4759 - accuracy: 0.7050 - val_loss: 1.5408 - val_accuracy: 0.6550\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3967 - accuracy: 0.7120 - val_loss: 1.4710 - val_accuracy: 0.6800\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3263 - accuracy: 0.7305 - val_loss: 1.4064 - val_accuracy: 0.6900\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2630 - accuracy: 0.7455 - val_loss: 1.3481 - val_accuracy: 0.7050\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2060 - accuracy: 0.7575 - val_loss: 1.2949 - val_accuracy: 0.7200\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1541 - accuracy: 0.7660 - val_loss: 1.2468 - val_accuracy: 0.7250\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1069 - accuracy: 0.7750 - val_loss: 1.2013 - val_accuracy: 0.7350\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0636 - accuracy: 0.7815 - val_loss: 1.1599 - val_accuracy: 0.7450\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.0238 - accuracy: 0.7920 - val_loss: 1.1208 - val_accuracy: 0.7500\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9872 - accuracy: 0.7995 - val_loss: 1.0852 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.9536 - accuracy: 0.8050 - val_loss: 1.0520 - val_accuracy: 0.7550\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9226 - accuracy: 0.8115 - val_loss: 1.0218 - val_accuracy: 0.7550\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8940 - accuracy: 0.8105 - val_loss: 0.9929 - val_accuracy: 0.7550\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8675 - accuracy: 0.8130 - val_loss: 0.9659 - val_accuracy: 0.7550\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 3s 40ms/step - loss: 0.8431 - accuracy: 0.8185 - val_loss: 0.9414 - val_accuracy: 0.7650\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8204 - accuracy: 0.8195 - val_loss: 0.9187 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7993 - accuracy: 0.8240 - val_loss: 0.8966 - val_accuracy: 0.7700\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7796 - accuracy: 0.8290 - val_loss: 0.8762 - val_accuracy: 0.7750\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7612 - accuracy: 0.8330 - val_loss: 0.8580 - val_accuracy: 0.7750\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7440 - accuracy: 0.8350 - val_loss: 0.8406 - val_accuracy: 0.7800\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7279 - accuracy: 0.8375 - val_loss: 0.8230 - val_accuracy: 0.7800\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.7128 - accuracy: 0.8410 - val_loss: 0.8079 - val_accuracy: 0.7850\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6984 - accuracy: 0.8415 - val_loss: 0.7931 - val_accuracy: 0.7950\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6850 - accuracy: 0.8445 - val_loss: 0.7785 - val_accuracy: 0.8000\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.6723 - accuracy: 0.8480 - val_loss: 0.7653 - val_accuracy: 0.8050\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6602 - accuracy: 0.8490 - val_loss: 0.7527 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6488 - accuracy: 0.8500 - val_loss: 0.7408 - val_accuracy: 0.8050\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6379 - accuracy: 0.8550 - val_loss: 0.7291 - val_accuracy: 0.8100\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6275 - accuracy: 0.8565 - val_loss: 0.7184 - val_accuracy: 0.8100\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6178 - accuracy: 0.8575 - val_loss: 0.7085 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6085 - accuracy: 0.8610 - val_loss: 0.6975 - val_accuracy: 0.8200\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5995 - accuracy: 0.8595 - val_loss: 0.6886 - val_accuracy: 0.8250\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5910 - accuracy: 0.8620 - val_loss: 0.6798 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5829 - accuracy: 0.8650 - val_loss: 0.6709 - val_accuracy: 0.8300\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5750 - accuracy: 0.8665 - val_loss: 0.6625 - val_accuracy: 0.8450\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5674 - accuracy: 0.8675 - val_loss: 0.6555 - val_accuracy: 0.8400\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5603 - accuracy: 0.8680 - val_loss: 0.6473 - val_accuracy: 0.8450\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5533 - accuracy: 0.8710 - val_loss: 0.6408 - val_accuracy: 0.8450\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5466 - accuracy: 0.8725 - val_loss: 0.6336 - val_accuracy: 0.8500\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5403 - accuracy: 0.8725 - val_loss: 0.6265 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5341 - accuracy: 0.8755 - val_loss: 0.6194 - val_accuracy: 0.8600\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5282 - accuracy: 0.8760 - val_loss: 0.6135 - val_accuracy: 0.8600\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5224 - accuracy: 0.8785 - val_loss: 0.6081 - val_accuracy: 0.8600\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5169 - accuracy: 0.8780 - val_loss: 0.6018 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5116 - accuracy: 0.8800 - val_loss: 0.5959 - val_accuracy: 0.8600\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5066 - accuracy: 0.8820 - val_loss: 0.5916 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5016 - accuracy: 0.8825 - val_loss: 0.5863 - val_accuracy: 0.8650\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4969 - accuracy: 0.8830 - val_loss: 0.5805 - val_accuracy: 0.8700\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4922 - accuracy: 0.8820 - val_loss: 0.5761 - val_accuracy: 0.8700\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4877 - accuracy: 0.8850 - val_loss: 0.5713 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4834 - accuracy: 0.8875 - val_loss: 0.5673 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4792 - accuracy: 0.8850 - val_loss: 0.5634 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4751 - accuracy: 0.8870 - val_loss: 0.5587 - val_accuracy: 0.8700\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4710 - accuracy: 0.8895 - val_loss: 0.5552 - val_accuracy: 0.8700\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4672 - accuracy: 0.8890 - val_loss: 0.5514 - val_accuracy: 0.8700\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4634 - accuracy: 0.8895 - val_loss: 0.5474 - val_accuracy: 0.8700\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4597 - accuracy: 0.8900 - val_loss: 0.5427 - val_accuracy: 0.8700\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4561 - accuracy: 0.8920 - val_loss: 0.5395 - val_accuracy: 0.8700\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 32ms/step - loss: 0.4526 - accuracy: 0.8935 - val_loss: 0.5355 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4492 - accuracy: 0.8935 - val_loss: 0.5328 - val_accuracy: 0.8700\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4459 - accuracy: 0.8950 - val_loss: 0.5300 - val_accuracy: 0.8700\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4426 - accuracy: 0.8940 - val_loss: 0.5268 - val_accuracy: 0.8700\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4395 - accuracy: 0.8955 - val_loss: 0.5225 - val_accuracy: 0.8700\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4365 - accuracy: 0.8955 - val_loss: 0.5198 - val_accuracy: 0.8700\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4335 - accuracy: 0.8945 - val_loss: 0.5169 - val_accuracy: 0.8700\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4305 - accuracy: 0.8960 - val_loss: 0.5140 - val_accuracy: 0.8700\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4276 - accuracy: 0.8975 - val_loss: 0.5112 - val_accuracy: 0.8700\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4248 - accuracy: 0.8975 - val_loss: 0.5080 - val_accuracy: 0.8700\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4220 - accuracy: 0.8990 - val_loss: 0.5061 - val_accuracy: 0.8700\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4193 - accuracy: 0.8975 - val_loss: 0.5033 - val_accuracy: 0.8700\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4166 - accuracy: 0.8995 - val_loss: 0.5015 - val_accuracy: 0.8700\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4140 - accuracy: 0.8995 - val_loss: 0.4992 - val_accuracy: 0.8700\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4115 - accuracy: 0.8980 - val_loss: 0.4967 - val_accuracy: 0.8700\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4089 - accuracy: 0.8990 - val_loss: 0.4939 - val_accuracy: 0.8700\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4065 - accuracy: 0.8995 - val_loss: 0.4918 - val_accuracy: 0.8700\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4042 - accuracy: 0.8995 - val_loss: 0.4898 - val_accuracy: 0.8700\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4018 - accuracy: 0.9000 - val_loss: 0.4873 - val_accuracy: 0.8700\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3995 - accuracy: 0.8995 - val_loss: 0.4843 - val_accuracy: 0.8700\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3973 - accuracy: 0.9015 - val_loss: 0.4829 - val_accuracy: 0.8700\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3951 - accuracy: 0.9015 - val_loss: 0.4820 - val_accuracy: 0.8700\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3930 - accuracy: 0.9025 - val_loss: 0.4794 - val_accuracy: 0.8700\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3909 - accuracy: 0.9025 - val_loss: 0.4772 - val_accuracy: 0.8700\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3887 - accuracy: 0.9025 - val_loss: 0.4752 - val_accuracy: 0.8700\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3867 - accuracy: 0.9025 - val_loss: 0.4732 - val_accuracy: 0.8700\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3847 - accuracy: 0.9030 - val_loss: 0.4716 - val_accuracy: 0.8700\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3827 - accuracy: 0.9025 - val_loss: 0.4705 - val_accuracy: 0.8700\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3808 - accuracy: 0.9035 - val_loss: 0.4683 - val_accuracy: 0.8750\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3788 - accuracy: 0.9045 - val_loss: 0.4661 - val_accuracy: 0.8750\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3769 - accuracy: 0.9030 - val_loss: 0.4645 - val_accuracy: 0.8750\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3750 - accuracy: 0.9050 - val_loss: 0.4634 - val_accuracy: 0.8750\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3732 - accuracy: 0.9020 - val_loss: 0.4623 - val_accuracy: 0.8750\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3714 - accuracy: 0.9035 - val_loss: 0.4604 - val_accuracy: 0.8750\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3696 - accuracy: 0.9045 - val_loss: 0.4589 - val_accuracy: 0.8750\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.3678 - accuracy: 0.9050 - val_loss: 0.4581 - val_accuracy: 0.8750\n",
            "Largemodel Test loss: 0.45339247584342957\n",
            "Largemodel Test accuracy: 0.8704000115394592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j_-85YFDA6y"
      },
      "source": [
        "##4.2 Dropout\n",
        "Dropout은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 Dropout layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 Dropout model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYHpRcVDX22",
        "outputId": "49cce7b4-4579-4293-b992-0b4b8d943336"
      },
      "source": [
        "dropout_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dense(1024))\n",
        "dropout_model.add(Dropout(0.2))\n",
        "dropout_model.add(Flatten())\n",
        "dropout_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "dropout_model.build(input_shape=(None, 28, 28, 1))\n",
        "### END CODE HERE ###\n",
        "\n",
        "# ### START CODE HERE ###\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1024))\n",
        "# model.add(Dense(1024))\n",
        "# model.add(Dense(1024))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "# model.build(input_shape=(None,784))\n",
        "# ### END CODE HERE ###\n",
        "\n",
        "dropout_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,913,290\n",
            "Trainable params: 2,913,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuxjOGWEKo9"
      },
      "source": [
        "Dropout Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?\n",
        "\n",
        "※Dropout을 적용한 Model은 일정확률로 신경망의 뉴런을 비활성화시키기 때문에, 오버피팅을 방지하는 효과가 있습니다. 하지만 비활성화로 인해서  학습속도가 떨어진다는 단점이 존재합니다. 그 때문에 test accuracy는 같은 학습파라미터 조건에서 기존모델보다 더 낮을 수 있습니다. \n",
        "\n",
        "기존모델과 dropout을 적용한 모델에 대해서, train 데이터와 test데이터에 대한 accuracy차이를 주목해보시면 좋을 것같습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqMK-jsyED_h",
        "outputId": "5456b707-d173-4bb5-eb89-a3a55242a31d"
      },
      "source": [
        "dropout_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "dropout_model_history=dropout_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = dropout_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Dropout model Test loss:', score[0])\n",
        "print('Dropout model Test accuracy:', score[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_8_input'), name='flatten_8_input', description=\"created by layer 'flatten_8_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_8_input'), name='flatten_8_input', description=\"created by layer 'flatten_8_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "71/72 [============================>.] - ETA: 0s - loss: 2.2521 - accuracy: 0.1937WARNING:tensorflow:Model was constructed with shape (None, 784) for input KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='flatten_8_input'), name='flatten_8_input', description=\"created by layer 'flatten_8_input'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.2521 - accuracy: 0.1930 - val_loss: 2.1332 - val_accuracy: 0.2650\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.1105 - accuracy: 0.2630 - val_loss: 2.0125 - val_accuracy: 0.3400\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9908 - accuracy: 0.3560 - val_loss: 1.9073 - val_accuracy: 0.4000\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.8789 - accuracy: 0.4305 - val_loss: 1.8135 - val_accuracy: 0.4800\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.7828 - accuracy: 0.5025 - val_loss: 1.7286 - val_accuracy: 0.5450\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6902 - accuracy: 0.5465 - val_loss: 1.6500 - val_accuracy: 0.6000\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.6060 - accuracy: 0.5945 - val_loss: 1.5790 - val_accuracy: 0.6450\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.5249 - accuracy: 0.6365 - val_loss: 1.5125 - val_accuracy: 0.6650\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4503 - accuracy: 0.6645 - val_loss: 1.4508 - val_accuracy: 0.6700\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.3824 - accuracy: 0.6920 - val_loss: 1.3942 - val_accuracy: 0.6800\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3281 - accuracy: 0.6955 - val_loss: 1.3412 - val_accuracy: 0.6900\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2654 - accuracy: 0.7225 - val_loss: 1.2923 - val_accuracy: 0.7100\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2230 - accuracy: 0.7325 - val_loss: 1.2456 - val_accuracy: 0.7200\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1741 - accuracy: 0.7590 - val_loss: 1.2027 - val_accuracy: 0.7350\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1265 - accuracy: 0.7670 - val_loss: 1.1639 - val_accuracy: 0.7450\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 3s 43ms/step - loss: 1.0868 - accuracy: 0.7650 - val_loss: 1.1275 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0549 - accuracy: 0.7755 - val_loss: 1.0935 - val_accuracy: 0.7650\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0099 - accuracy: 0.7945 - val_loss: 1.0618 - val_accuracy: 0.7800\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9806 - accuracy: 0.7960 - val_loss: 1.0320 - val_accuracy: 0.7900\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9472 - accuracy: 0.8000 - val_loss: 1.0034 - val_accuracy: 0.7950\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9203 - accuracy: 0.8030 - val_loss: 0.9784 - val_accuracy: 0.8150\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8962 - accuracy: 0.8020 - val_loss: 0.9542 - val_accuracy: 0.8100\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8741 - accuracy: 0.8160 - val_loss: 0.9321 - val_accuracy: 0.8100\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8520 - accuracy: 0.8180 - val_loss: 0.9112 - val_accuracy: 0.8150\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.8264 - accuracy: 0.8235 - val_loss: 0.8913 - val_accuracy: 0.8200\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8014 - accuracy: 0.8245 - val_loss: 0.8727 - val_accuracy: 0.8250\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 3s 41ms/step - loss: 0.7908 - accuracy: 0.8320 - val_loss: 0.8546 - val_accuracy: 0.8350\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7659 - accuracy: 0.8265 - val_loss: 0.8376 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7494 - accuracy: 0.8340 - val_loss: 0.8221 - val_accuracy: 0.8350\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7363 - accuracy: 0.8350 - val_loss: 0.8068 - val_accuracy: 0.8450\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7243 - accuracy: 0.8375 - val_loss: 0.7924 - val_accuracy: 0.8450\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7107 - accuracy: 0.8435 - val_loss: 0.7792 - val_accuracy: 0.8400\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6997 - accuracy: 0.8435 - val_loss: 0.7670 - val_accuracy: 0.8400\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6865 - accuracy: 0.8410 - val_loss: 0.7553 - val_accuracy: 0.8400\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6719 - accuracy: 0.8435 - val_loss: 0.7443 - val_accuracy: 0.8450\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6622 - accuracy: 0.8535 - val_loss: 0.7323 - val_accuracy: 0.8450\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6450 - accuracy: 0.8540 - val_loss: 0.7224 - val_accuracy: 0.8500\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6434 - accuracy: 0.8485 - val_loss: 0.7122 - val_accuracy: 0.8550\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6337 - accuracy: 0.8575 - val_loss: 0.7028 - val_accuracy: 0.8550\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6217 - accuracy: 0.8585 - val_loss: 0.6939 - val_accuracy: 0.8550\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6120 - accuracy: 0.8555 - val_loss: 0.6849 - val_accuracy: 0.8550\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.6036 - accuracy: 0.8650 - val_loss: 0.6771 - val_accuracy: 0.8600\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5999 - accuracy: 0.8570 - val_loss: 0.6691 - val_accuracy: 0.8600\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5892 - accuracy: 0.8595 - val_loss: 0.6609 - val_accuracy: 0.8600\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5843 - accuracy: 0.8635 - val_loss: 0.6542 - val_accuracy: 0.8600\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5726 - accuracy: 0.8675 - val_loss: 0.6470 - val_accuracy: 0.8600\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5674 - accuracy: 0.8645 - val_loss: 0.6400 - val_accuracy: 0.8550\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5625 - accuracy: 0.8635 - val_loss: 0.6336 - val_accuracy: 0.8550\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5552 - accuracy: 0.8695 - val_loss: 0.6275 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5450 - accuracy: 0.8705 - val_loss: 0.6210 - val_accuracy: 0.8550\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5420 - accuracy: 0.8730 - val_loss: 0.6149 - val_accuracy: 0.8550\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5328 - accuracy: 0.8735 - val_loss: 0.6097 - val_accuracy: 0.8550\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5293 - accuracy: 0.8745 - val_loss: 0.6036 - val_accuracy: 0.8550\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5226 - accuracy: 0.8725 - val_loss: 0.5985 - val_accuracy: 0.8600\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5188 - accuracy: 0.8735 - val_loss: 0.5939 - val_accuracy: 0.8600\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5142 - accuracy: 0.8770 - val_loss: 0.5884 - val_accuracy: 0.8600\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.5117 - accuracy: 0.8755 - val_loss: 0.5837 - val_accuracy: 0.8600\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5072 - accuracy: 0.8720 - val_loss: 0.5794 - val_accuracy: 0.8600\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5006 - accuracy: 0.8825 - val_loss: 0.5746 - val_accuracy: 0.8600\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4960 - accuracy: 0.8810 - val_loss: 0.5706 - val_accuracy: 0.8600\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4928 - accuracy: 0.8800 - val_loss: 0.5664 - val_accuracy: 0.8600\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4887 - accuracy: 0.8840 - val_loss: 0.5615 - val_accuracy: 0.8600\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4825 - accuracy: 0.8820 - val_loss: 0.5584 - val_accuracy: 0.8600\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4802 - accuracy: 0.8815 - val_loss: 0.5551 - val_accuracy: 0.8600\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4768 - accuracy: 0.8825 - val_loss: 0.5515 - val_accuracy: 0.8600\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4733 - accuracy: 0.8845 - val_loss: 0.5474 - val_accuracy: 0.8600\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4695 - accuracy: 0.8820 - val_loss: 0.5432 - val_accuracy: 0.8650\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4664 - accuracy: 0.8890 - val_loss: 0.5400 - val_accuracy: 0.8650\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4623 - accuracy: 0.8860 - val_loss: 0.5366 - val_accuracy: 0.8650\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4593 - accuracy: 0.8870 - val_loss: 0.5343 - val_accuracy: 0.8650\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4522 - accuracy: 0.8935 - val_loss: 0.5312 - val_accuracy: 0.8650\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 0.4495 - accuracy: 0.8825 - val_loss: 0.5277 - val_accuracy: 0.8650\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4455 - accuracy: 0.8885 - val_loss: 0.5246 - val_accuracy: 0.8650\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4468 - accuracy: 0.8880 - val_loss: 0.5220 - val_accuracy: 0.8650\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4446 - accuracy: 0.8865 - val_loss: 0.5190 - val_accuracy: 0.8650\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4396 - accuracy: 0.8880 - val_loss: 0.5160 - val_accuracy: 0.8650\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4358 - accuracy: 0.8920 - val_loss: 0.5140 - val_accuracy: 0.8650\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4377 - accuracy: 0.8880 - val_loss: 0.5114 - val_accuracy: 0.8650\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4349 - accuracy: 0.8920 - val_loss: 0.5095 - val_accuracy: 0.8650\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4318 - accuracy: 0.8915 - val_loss: 0.5075 - val_accuracy: 0.8650\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4231 - accuracy: 0.8940 - val_loss: 0.5045 - val_accuracy: 0.8650\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4251 - accuracy: 0.8945 - val_loss: 0.5019 - val_accuracy: 0.8650\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4229 - accuracy: 0.8925 - val_loss: 0.5003 - val_accuracy: 0.8650\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4183 - accuracy: 0.8980 - val_loss: 0.4991 - val_accuracy: 0.8650\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4177 - accuracy: 0.8960 - val_loss: 0.4963 - val_accuracy: 0.8650\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4120 - accuracy: 0.8935 - val_loss: 0.4945 - val_accuracy: 0.8650\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4127 - accuracy: 0.8945 - val_loss: 0.4928 - val_accuracy: 0.8650\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4066 - accuracy: 0.8950 - val_loss: 0.4905 - val_accuracy: 0.8650\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4051 - accuracy: 0.8995 - val_loss: 0.4882 - val_accuracy: 0.8750\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4075 - accuracy: 0.8960 - val_loss: 0.4859 - val_accuracy: 0.8700\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 49ms/step - loss: 0.4067 - accuracy: 0.8990 - val_loss: 0.4836 - val_accuracy: 0.8650\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 5s 74ms/step - loss: 0.4003 - accuracy: 0.9015 - val_loss: 0.4817 - val_accuracy: 0.8650\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.4021 - accuracy: 0.8960 - val_loss: 0.4803 - val_accuracy: 0.8650\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 4s 59ms/step - loss: 0.3946 - accuracy: 0.9005 - val_loss: 0.4778 - val_accuracy: 0.8650\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 4s 55ms/step - loss: 0.3966 - accuracy: 0.8980 - val_loss: 0.4767 - val_accuracy: 0.8650\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 42ms/step - loss: 0.3949 - accuracy: 0.9000 - val_loss: 0.4749 - val_accuracy: 0.8700\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 4s 50ms/step - loss: 0.3883 - accuracy: 0.9000 - val_loss: 0.4737 - val_accuracy: 0.8700\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 44ms/step - loss: 0.3931 - accuracy: 0.8985 - val_loss: 0.4717 - val_accuracy: 0.8700\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3869 - accuracy: 0.9010 - val_loss: 0.4693 - val_accuracy: 0.8700\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3894 - accuracy: 0.9025 - val_loss: 0.4685 - val_accuracy: 0.8700\n",
            "Dropout model Test loss: 0.4530337452888489\n",
            "Dropout model Test accuracy: 0.8720999956130981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEyhMagnGq8F"
      },
      "source": [
        "##4.3 BatchNormalization\n",
        "BatchNormalization(BN)은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 BN layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 BN model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIBMssiBJaB",
        "outputId": "8a3a98cf-ec18-473f-9bfd-8e4af435f1d2"
      },
      "source": [
        "bn_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('softmax'))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('softmax'))\n",
        "bn_model.add(Dense(1024))\n",
        "bn_model.add(BatchNormalization())\n",
        "bn_model.add(Activation('softmax'))\n",
        "bn_model.add(Flatten())\n",
        "bn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "bn_model.build(input_shape=(None, 28, 28, 1))\n",
        "\n",
        "### END CODE HERE ###\n",
        "bn_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_12 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1024)              803840    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izQBrVW6KYr3"
      },
      "source": [
        "BN Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2oakRaKZ4y",
        "outputId": "b841bd04-94d2-484a-d643-60792e1fcad8"
      },
      "source": [
        "bn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "bn_model_history=bn_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = bn_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('BN model Test loss:', score[0])\n",
        "print('BN model Test accuracy:', score[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 41ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 3s 49ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 3s 46ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 3s 40ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 3s 43ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 5s 68ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 3s 38ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 3s 40ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3026 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.0900 - val_loss: 2.3027 - val_accuracy: 0.0700\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.0890 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1105 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.3025 - accuracy: 0.1100 - val_loss: 2.3027 - val_accuracy: 0.0950\n",
            "BN model Test loss: 2.302563190460205\n",
            "BN model Test accuracy: 0.11349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fDPwaFsJ47I"
      },
      "source": [
        "##4.4 Final Model\n",
        "지금까지 썼던 Regularization 기법들을 종합선물세트로 적용해 봅시다. 다음과 같은 Final model을 만들어 보세요.\n",
        "\n",
        "| Layer (type) | Output Shape | Param # |\n",
        "|------|------|------|\n",
        "| Flatten | (None, 784) | 0 |\n",
        "| Dense | (None, 1024) | 803840 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dense | (None, 1024) | 1049600 |\n",
        "| BatchNormalization | (None, 1024) | 4096 |\n",
        "| Activation | (None, 1024) | 0 |\n",
        "| Dropout | (None, 1024) | 0 |\n",
        "| Flatten | (None, 1024) | 0 |\n",
        "| Dense | (None, 10) | 10250 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ppnPcELADo",
        "outputId": "63dfb2b2-8abe-484b-f8e5-df8270d5981b"
      },
      "source": [
        "final_model = Sequential()\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###\n",
        "final_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,925,578\n",
            "Trainable params: 2,919,434\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocok8VnoLcb4"
      },
      "source": [
        "Final Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bFRiHtLdVe",
        "outputId": "bd8dd073-3471-463f-f482-66b1d99d7fc9"
      },
      "source": [
        "final_model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "final_model_history=final_model.fit(large_x_train, large_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_validation, y_validation))\n",
        "\n",
        "score = final_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Final model Test loss:', score[0])\n",
        "print('Final model Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "72/72 [==============================] - 4s 37ms/step - loss: 2.8955 - accuracy: 0.1204 - val_loss: 2.2754 - val_accuracy: 0.1200\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.7084 - accuracy: 0.1313 - val_loss: 2.2247 - val_accuracy: 0.1550\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.5755 - accuracy: 0.1531 - val_loss: 2.1182 - val_accuracy: 0.2250\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.4187 - accuracy: 0.1860 - val_loss: 1.9820 - val_accuracy: 0.2850\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.2413 - accuracy: 0.2345 - val_loss: 1.8511 - val_accuracy: 0.3250\n",
            "Epoch 6/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 2.1545 - accuracy: 0.2505 - val_loss: 1.7301 - val_accuracy: 0.3950\n",
            "Epoch 7/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.9716 - accuracy: 0.3018 - val_loss: 1.6215 - val_accuracy: 0.4500\n",
            "Epoch 8/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.9178 - accuracy: 0.3356 - val_loss: 1.5235 - val_accuracy: 0.4900\n",
            "Epoch 9/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.8989 - accuracy: 0.3492 - val_loss: 1.4347 - val_accuracy: 0.5400\n",
            "Epoch 10/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.7533 - accuracy: 0.4191 - val_loss: 1.3557 - val_accuracy: 0.5850\n",
            "Epoch 11/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.6817 - accuracy: 0.4232 - val_loss: 1.2844 - val_accuracy: 0.6200\n",
            "Epoch 12/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5772 - accuracy: 0.4709 - val_loss: 1.2204 - val_accuracy: 0.6250\n",
            "Epoch 13/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.5230 - accuracy: 0.5010 - val_loss: 1.1627 - val_accuracy: 0.6350\n",
            "Epoch 14/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.4977 - accuracy: 0.5170 - val_loss: 1.1104 - val_accuracy: 0.6500\n",
            "Epoch 15/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3753 - accuracy: 0.5492 - val_loss: 1.0639 - val_accuracy: 0.6700\n",
            "Epoch 16/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.3597 - accuracy: 0.5349 - val_loss: 1.0189 - val_accuracy: 0.6800\n",
            "Epoch 17/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2687 - accuracy: 0.5849 - val_loss: 0.9781 - val_accuracy: 0.6950\n",
            "Epoch 18/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.2412 - accuracy: 0.6174 - val_loss: 0.9422 - val_accuracy: 0.7050\n",
            "Epoch 19/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1827 - accuracy: 0.6330 - val_loss: 0.9086 - val_accuracy: 0.7200\n",
            "Epoch 20/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.1396 - accuracy: 0.6373 - val_loss: 0.8766 - val_accuracy: 0.7400\n",
            "Epoch 21/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0607 - accuracy: 0.6774 - val_loss: 0.8481 - val_accuracy: 0.7450\n",
            "Epoch 22/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 1.0773 - accuracy: 0.6601 - val_loss: 0.8215 - val_accuracy: 0.7600\n",
            "Epoch 23/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 1.0076 - accuracy: 0.6867 - val_loss: 0.7981 - val_accuracy: 0.7650\n",
            "Epoch 24/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9889 - accuracy: 0.6993 - val_loss: 0.7746 - val_accuracy: 0.7650\n",
            "Epoch 25/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.9996 - accuracy: 0.7114 - val_loss: 0.7518 - val_accuracy: 0.7700\n",
            "Epoch 26/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.9798 - accuracy: 0.6907 - val_loss: 0.7321 - val_accuracy: 0.7750\n",
            "Epoch 27/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9486 - accuracy: 0.7138 - val_loss: 0.7138 - val_accuracy: 0.7850\n",
            "Epoch 28/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.9006 - accuracy: 0.7315 - val_loss: 0.6961 - val_accuracy: 0.7950\n",
            "Epoch 29/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.8979 - accuracy: 0.7407 - val_loss: 0.6796 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8871 - accuracy: 0.7445 - val_loss: 0.6638 - val_accuracy: 0.8000\n",
            "Epoch 31/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.8305 - accuracy: 0.7377 - val_loss: 0.6491 - val_accuracy: 0.8050\n",
            "Epoch 32/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7884 - accuracy: 0.7834 - val_loss: 0.6363 - val_accuracy: 0.8050\n",
            "Epoch 33/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.7846 - accuracy: 0.7750 - val_loss: 0.6237 - val_accuracy: 0.8050\n",
            "Epoch 34/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7536 - accuracy: 0.7882 - val_loss: 0.6115 - val_accuracy: 0.8050\n",
            "Epoch 35/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7700 - accuracy: 0.7828 - val_loss: 0.6000 - val_accuracy: 0.8050\n",
            "Epoch 36/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7654 - accuracy: 0.7855 - val_loss: 0.5895 - val_accuracy: 0.8100\n",
            "Epoch 37/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7250 - accuracy: 0.7999 - val_loss: 0.5783 - val_accuracy: 0.8100\n",
            "Epoch 38/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7272 - accuracy: 0.7999 - val_loss: 0.5687 - val_accuracy: 0.8150\n",
            "Epoch 39/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6656 - accuracy: 0.8158 - val_loss: 0.5601 - val_accuracy: 0.8150\n",
            "Epoch 40/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.7133 - accuracy: 0.7959 - val_loss: 0.5495 - val_accuracy: 0.8200\n",
            "Epoch 41/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6347 - accuracy: 0.8242 - val_loss: 0.5415 - val_accuracy: 0.8300\n",
            "Epoch 42/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6525 - accuracy: 0.8170 - val_loss: 0.5329 - val_accuracy: 0.8300\n",
            "Epoch 43/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6563 - accuracy: 0.8301 - val_loss: 0.5257 - val_accuracy: 0.8350\n",
            "Epoch 44/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.6426 - accuracy: 0.8127 - val_loss: 0.5186 - val_accuracy: 0.8350\n",
            "Epoch 45/100\n",
            "72/72 [==============================] - 3s 37ms/step - loss: 0.5976 - accuracy: 0.8266 - val_loss: 0.5115 - val_accuracy: 0.8400\n",
            "Epoch 46/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.6115 - accuracy: 0.8175 - val_loss: 0.5047 - val_accuracy: 0.8450\n",
            "Epoch 47/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.5733 - accuracy: 0.8451 - val_loss: 0.4992 - val_accuracy: 0.8500\n",
            "Epoch 48/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5723 - accuracy: 0.8420 - val_loss: 0.4925 - val_accuracy: 0.8500\n",
            "Epoch 49/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5931 - accuracy: 0.8281 - val_loss: 0.4877 - val_accuracy: 0.8550\n",
            "Epoch 50/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5519 - accuracy: 0.8520 - val_loss: 0.4801 - val_accuracy: 0.8600\n",
            "Epoch 51/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5268 - accuracy: 0.8701 - val_loss: 0.4742 - val_accuracy: 0.8700\n",
            "Epoch 52/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5048 - accuracy: 0.8688 - val_loss: 0.4692 - val_accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5378 - accuracy: 0.8435 - val_loss: 0.4637 - val_accuracy: 0.8750\n",
            "Epoch 54/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5296 - accuracy: 0.8631 - val_loss: 0.4586 - val_accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5190 - accuracy: 0.8684 - val_loss: 0.4543 - val_accuracy: 0.8750\n",
            "Epoch 56/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.5320 - accuracy: 0.8594 - val_loss: 0.4490 - val_accuracy: 0.8750\n",
            "Epoch 57/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.5172 - accuracy: 0.8616 - val_loss: 0.4440 - val_accuracy: 0.8750\n",
            "Epoch 58/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4764 - accuracy: 0.8794 - val_loss: 0.4387 - val_accuracy: 0.8750\n",
            "Epoch 59/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4782 - accuracy: 0.8740 - val_loss: 0.4347 - val_accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4772 - accuracy: 0.8855 - val_loss: 0.4307 - val_accuracy: 0.8750\n",
            "Epoch 61/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4768 - accuracy: 0.8674 - val_loss: 0.4268 - val_accuracy: 0.8750\n",
            "Epoch 62/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4575 - accuracy: 0.8864 - val_loss: 0.4218 - val_accuracy: 0.8750\n",
            "Epoch 63/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4883 - accuracy: 0.8652 - val_loss: 0.4185 - val_accuracy: 0.8800\n",
            "Epoch 64/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4487 - accuracy: 0.8893 - val_loss: 0.4154 - val_accuracy: 0.8800\n",
            "Epoch 65/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4712 - accuracy: 0.8754 - val_loss: 0.4113 - val_accuracy: 0.8850\n",
            "Epoch 66/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4624 - accuracy: 0.8785 - val_loss: 0.4084 - val_accuracy: 0.8850\n",
            "Epoch 67/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4669 - accuracy: 0.8638 - val_loss: 0.4045 - val_accuracy: 0.8850\n",
            "Epoch 68/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4214 - accuracy: 0.8941 - val_loss: 0.4010 - val_accuracy: 0.8850\n",
            "Epoch 69/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4255 - accuracy: 0.8846 - val_loss: 0.3979 - val_accuracy: 0.8850\n",
            "Epoch 70/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4336 - accuracy: 0.8867 - val_loss: 0.3946 - val_accuracy: 0.8850\n",
            "Epoch 71/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.4197 - accuracy: 0.8927 - val_loss: 0.3918 - val_accuracy: 0.8900\n",
            "Epoch 72/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.4063 - accuracy: 0.8913 - val_loss: 0.3889 - val_accuracy: 0.8900\n",
            "Epoch 73/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4231 - accuracy: 0.8798 - val_loss: 0.3858 - val_accuracy: 0.8900\n",
            "Epoch 74/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4333 - accuracy: 0.8809 - val_loss: 0.3827 - val_accuracy: 0.8900\n",
            "Epoch 75/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3862 - accuracy: 0.9058 - val_loss: 0.3804 - val_accuracy: 0.8900\n",
            "Epoch 76/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.4084 - accuracy: 0.8904 - val_loss: 0.3775 - val_accuracy: 0.8900\n",
            "Epoch 77/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3977 - accuracy: 0.8965 - val_loss: 0.3746 - val_accuracy: 0.8900\n",
            "Epoch 78/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3792 - accuracy: 0.9075 - val_loss: 0.3726 - val_accuracy: 0.8950\n",
            "Epoch 79/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3759 - accuracy: 0.8924 - val_loss: 0.3700 - val_accuracy: 0.9000\n",
            "Epoch 80/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3773 - accuracy: 0.9133 - val_loss: 0.3669 - val_accuracy: 0.9000\n",
            "Epoch 81/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3921 - accuracy: 0.9013 - val_loss: 0.3652 - val_accuracy: 0.9050\n",
            "Epoch 82/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3697 - accuracy: 0.8968 - val_loss: 0.3631 - val_accuracy: 0.9100\n",
            "Epoch 83/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3796 - accuracy: 0.9040 - val_loss: 0.3604 - val_accuracy: 0.9100\n",
            "Epoch 84/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3772 - accuracy: 0.9062 - val_loss: 0.3592 - val_accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3560 - accuracy: 0.9111 - val_loss: 0.3571 - val_accuracy: 0.9150\n",
            "Epoch 86/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3549 - accuracy: 0.9118 - val_loss: 0.3548 - val_accuracy: 0.9150\n",
            "Epoch 87/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3338 - accuracy: 0.9120 - val_loss: 0.3523 - val_accuracy: 0.9150\n",
            "Epoch 88/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3345 - accuracy: 0.9207 - val_loss: 0.3494 - val_accuracy: 0.9150\n",
            "Epoch 89/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3715 - accuracy: 0.9015 - val_loss: 0.3475 - val_accuracy: 0.9150\n",
            "Epoch 90/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3096 - accuracy: 0.9216 - val_loss: 0.3455 - val_accuracy: 0.9200\n",
            "Epoch 91/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3411 - accuracy: 0.9052 - val_loss: 0.3437 - val_accuracy: 0.9200\n",
            "Epoch 92/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3062 - accuracy: 0.9260 - val_loss: 0.3417 - val_accuracy: 0.9200\n",
            "Epoch 93/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3406 - accuracy: 0.9109 - val_loss: 0.3410 - val_accuracy: 0.9150\n",
            "Epoch 94/100\n",
            "72/72 [==============================] - 2s 34ms/step - loss: 0.3273 - accuracy: 0.9271 - val_loss: 0.3386 - val_accuracy: 0.9200\n",
            "Epoch 95/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2994 - accuracy: 0.9339 - val_loss: 0.3365 - val_accuracy: 0.9200\n",
            "Epoch 96/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3277 - accuracy: 0.9223 - val_loss: 0.3357 - val_accuracy: 0.9200\n",
            "Epoch 97/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.2972 - accuracy: 0.9256 - val_loss: 0.3340 - val_accuracy: 0.9200\n",
            "Epoch 98/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3239 - accuracy: 0.9189 - val_loss: 0.3322 - val_accuracy: 0.9200\n",
            "Epoch 99/100\n",
            "72/72 [==============================] - 3s 35ms/step - loss: 0.3220 - accuracy: 0.9212 - val_loss: 0.3289 - val_accuracy: 0.9200\n",
            "Epoch 100/100\n",
            "72/72 [==============================] - 2s 35ms/step - loss: 0.3189 - accuracy: 0.9155 - val_loss: 0.3285 - val_accuracy: 0.9250\n",
            "Final model Test loss: 0.39608311653137207\n",
            "Final model Test accuracy: 0.8842999935150146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPR3nEMAL4Yd"
      },
      "source": [
        "(optional) Training data가 늘어나면 regularization 효과가 나는 것을 보였습니다. 쉽게 training data를 늘릴 수 있는 방법은 data augmentation 입니다. 이 방법은 기존 training data를 적절히 rotating, flipping, scaling, shifting 하여 training data 수를 늘리는 것입니다. data augmentation의 regularization 효과를 테스트해 보세요. 또한 뉴럴 네트워크의 노드 개수나 층수를 바꿔서 성능을 올려보는 것도 테스트해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn11ZtM7GmS-"
      },
      "source": [
        ""
      ]
    }
  ]
}